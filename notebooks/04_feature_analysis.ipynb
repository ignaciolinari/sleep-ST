{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad de modelos ML (SHAP)\n",
    "\n",
    "Notebook para analizar importancias y explicaciones locales de modelos tradicionales (RandomForest / XGBoost) entrenados con features PSG.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json as _json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Agregar src al path para imports\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from src.models.data_preparation import prepare_train_test_split\n",
    "\n",
    "# Suprimir warnings de SHAP que no son criticos\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"shap\")\n",
    "\n",
    "# -------------------------\n",
    "# Config global\n",
    "# -------------------------\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Submuestreo para SHAP (el dataset es grande)\n",
    "MAX_SAMPLES = 8000\n",
    "STRATIFY_SAMPLE = True  # si hay y, intenta muestreo estratificado\n",
    "\n",
    "# Permutation importance\n",
    "PERMUTATION_SCORING = \"balanced_accuracy\"  # alternativa: \"accuracy\"\n",
    "PERMUTATION_REPEATS = 10  # repeticiones para reducir varianza\n",
    "\n",
    "# Configuracion de paths\n",
    "DATA_PATH = \"../data/processed/features_resamp200.parquet\"\n",
    "\n",
    "# Modelos disponibles\n",
    "# NOTA: Usamos xgb_opt_bayes porque tiene un holdout test set que garantiza\n",
    "# validez cientifica del analisis SHAP. Los hiperparametros de este modelo\n",
    "# son los mismos usados en LOSO (los mejores de la optimizacion bayesiana).\n",
    "MODELS = {\n",
    "    \"xgboost_bayes\": \"../models/xgb_opt_bayes/xgboost_model.pkl\",\n",
    "    \"xgboost_loso\": \"../models/xgb_loso_best/xgboost_model.pkl\",\n",
    "    \"random_forest_bayes\": \"../models/rf_opt_bayes_best/random_forest_model.pkl\",\n",
    "}\n",
    "MODEL_NAME = \"xgboost_bayes\"  # Modelo con holdout test set valido\n",
    "MODEL_PATH = MODELS[MODEL_NAME]\n",
    "OUTPUT_DIR = f\"../reports/shap_{MODEL_NAME}\"\n",
    "\n",
    "# Parametros del split (deben coincidir con los usados en entrenamiento)\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Mapeo de etiquetas de sleep staging (AASM standard)\n",
    "STAGE_LABELS = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"REM\"}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Data: {DATA_PATH}\\nModelo: {MODEL_NAME} -> {MODEL_PATH}\\nOutput: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Justificacion del Modelo Seleccionado\n",
    "\n",
    "Este analisis usa el modelo **XGBoost con optimizacion bayesiana** (`xgb_opt_bayes`) en lugar del modelo LOSO por las siguientes razones:\n",
    "\n",
    "1. **Validez cientifica**: El modelo `xgb_opt_bayes` fue entrenado con un split train/val/test **por sujeto**. El test set (20% de sujetos) **nunca fue visto** durante el entrenamiento ni la optimizacion de hiperparametros. Esto garantiza que el analisis SHAP refleja el comportamiento real del modelo sobre datos no vistos.\n",
    "\n",
    "2. **Mismos hiperparametros**: Los hiperparametros optimos encontrados en la optimizacion bayesiana son los mismos que se usaron para entrenar el modelo LOSO. Por lo tanto, las conclusiones sobre importancia de features son transferibles.\n",
    "\n",
    "3. **Problema con LOSO**: En validacion cruzada LOSO, el modelo final se re-entrena con **todos los sujetos**. Esto significa que no existe un test set \"limpio\" - cada sujeto fue usado como test en algun fold, pero el modelo final los vio a todos durante el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cargar datos y modelo, y recrear el split train/test\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Cargar features completo\n",
    "df_full = (\n",
    "    pd.read_parquet(DATA_PATH)\n",
    "    if DATA_PATH.endswith(\".parquet\")\n",
    "    else pd.read_feather(DATA_PATH)\n",
    ")\n",
    "\n",
    "# Ajusta si tu columna de etiqueta se llama distinto\n",
    "target_col = \"stage\"\n",
    "\n",
    "# Cargar modelo\n",
    "model = joblib.load(MODEL_PATH)\n",
    "print(f\"Modelo: {type(model).__name__}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# IMPORTANTE: Recrear el split exacto usado en entrenamiento\n",
    "# ------------------------------------------------------------\n",
    "# Usamos los mismos parametros (test_size, val_size, random_state)\n",
    "# para obtener el MISMO test set que nunca vio el modelo.\n",
    "\n",
    "print(\"\\nRecreando split train/val/test...\")\n",
    "train_df, test_df, val_df = prepare_train_test_split(\n",
    "    df_full,\n",
    "    test_size=TEST_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify_by=\"subject_core\",\n",
    "    stage_stratify=True,\n",
    ")\n",
    "\n",
    "print(f\"  Train: {len(train_df):,} epochs\")\n",
    "print(f\"  Val:   {len(val_df):,} epochs\" if val_df is not None else \"  Val: None\")\n",
    "print(f\"  Test:  {len(test_df):,} epochs <- USAREMOS ESTE PARA SHAP\")\n",
    "\n",
    "# Verificar sujetos unicos por split\n",
    "train_subjects = (\n",
    "    train_df[\"subject_core\"].nunique() if \"subject_core\" in train_df.columns else \"?\"\n",
    ")\n",
    "test_subjects = (\n",
    "    test_df[\"subject_core\"].nunique() if \"subject_core\" in test_df.columns else \"?\"\n",
    ")\n",
    "print(f\"\\nSujetos unicos - Train: {train_subjects}, Test: {test_subjects}\")\n",
    "\n",
    "# Usar SOLO el test set para el analisis (garantiza validez cientifica)\n",
    "df = test_df.copy()\n",
    "print(f\"\\n[OK] Usando TEST SET ({len(df):,} epochs) para analisis SHAP\")\n",
    "\n",
    "# --- Seleccion de columnas ---\n",
    "meta_cols = [\n",
    "    c\n",
    "    for c in [\n",
    "        target_col,\n",
    "        \"subject_core\",\n",
    "        \"subject_id\",\n",
    "        \"epoch_time_start\",\n",
    "        \"epoch_index\",\n",
    "    ]\n",
    "    if c in df.columns\n",
    "]\n",
    "\n",
    "# y (etiquetas)\n",
    "y_raw = df[target_col] if target_col in df.columns else None\n",
    "\n",
    "# Alinear y al espacio de clases del modelo (evita inconsistencias tipo str vs int)\n",
    "y = y_raw\n",
    "MODEL_STAGE_TO_LABEL = None  # stage_str -> model_label (si aplica)\n",
    "MODEL_LABEL_TO_STAGE = None  # model_label -> stage_str (si aplica)\n",
    "\n",
    "if (\n",
    "    y_raw is not None\n",
    "    and hasattr(model, \"classes_\")\n",
    "    and len(getattr(model, \"classes_\", [])) > 0\n",
    "):\n",
    "    model_classes = list(model.classes_)\n",
    "    cls0 = model_classes[0]\n",
    "\n",
    "    stage_order_aasm = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "    stage_order_alpha = sorted(\n",
    "        stage_order_aasm\n",
    "    )  # LabelEncoder típico: orden alfabético\n",
    "    inv_stage = {v: k for k, v in STAGE_LABELS.items()}\n",
    "\n",
    "    # Caso típico: y es str y el modelo usa labels numéricas (XGBoost entrenado con LabelEncoder)\n",
    "    if isinstance(cls0, (int, np.integer)) and y_raw.dtype == object:\n",
    "        uniques = set(pd.Series(y_raw.dropna().unique()).tolist())\n",
    "        if uniques and uniques.issubset(set(stage_order_aasm)):\n",
    "            # No asumimos el orden: puede ser AASM (W,N1,...) o alfabético (N1,N2,N3,REM,W).\n",
    "            # Lo decidimos luego con un chequeo rápido de accuracy sobre el mismo test set.\n",
    "            if len(model_classes) == len(stage_order_aasm):\n",
    "                _map_aasm = dict(zip(stage_order_aasm, model_classes))\n",
    "                _map_alpha = dict(zip(stage_order_alpha, model_classes))\n",
    "                # Inicialmente, mapea con AASM; luego se ajusta si hace falta\n",
    "                y = y_raw.map(_map_aasm)\n",
    "                MODEL_STAGE_TO_LABEL = _map_aasm\n",
    "            else:\n",
    "                # Fallback: ids canonicos 0..4 (solo para evitar crasheos)\n",
    "                y = y_raw.map(inv_stage)\n",
    "                MODEL_STAGE_TO_LABEL = inv_stage\n",
    "                print(\n",
    "                    \"[WARN] model.classes_ no tiene 5 clases; usando mapeo canonico 0..4 como fallback\"\n",
    "                )\n",
    "        else:\n",
    "            # intento simple de casteo\n",
    "            try:\n",
    "                y = y_raw.astype(int)\n",
    "                print(\"[INFO] y se casteo a int para coincidir con el modelo\")\n",
    "            except Exception:\n",
    "                print(\n",
    "                    \"[WARN] y parece no estar alineado con model.classes_. \"\n",
    "                    \"Esto puede invalidar permutation importance / analisis de errores.\"\n",
    "                )\n",
    "\n",
    "    # Caso inverso: y numérico y modelo usa str\n",
    "    elif (\n",
    "        isinstance(cls0, str) and y is not None and not pd.api.types.is_object_dtype(y)\n",
    "    ):\n",
    "        y = y.astype(str)\n",
    "        print(\"[INFO] y se casteo a str para coincidir con el modelo\")\n",
    "\n",
    "# X (features) - primero quita metas; luego filtra no-numericas\n",
    "feature_cols_raw = [c for c in df.columns if c not in meta_cols]\n",
    "X_raw = df[feature_cols_raw]\n",
    "\n",
    "non_numeric = [c for c in X_raw.columns if not pd.api.types.is_numeric_dtype(X_raw[c])]\n",
    "if non_numeric:\n",
    "    print(\n",
    "        f\"[WARN] Se excluyen {len(non_numeric)} columnas no-numericas de X (ejemplos): {non_numeric[:10]}\"\n",
    "        + (\" ...\" if len(non_numeric) > 10 else \"\")\n",
    "    )\n",
    "\n",
    "X = X_raw.drop(columns=non_numeric) if non_numeric else X_raw\n",
    "\n",
    "# Reemplazar infinitos por NaN (diagnostico)\n",
    "n_inf = np.isinf(X.select_dtypes(include=[np.number]).values).sum()\n",
    "if n_inf > 0:\n",
    "    print(f\"[WARN] Se encontraron {n_inf:,} valores infinitos; se reemplazan por NaN\")\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_rows = int(X.isna().any(axis=1).sum())\n",
    "if nan_rows:\n",
    "    print(\n",
    "        f\"[WARN] Hay {nan_rows:,} filas con NaN/inf en X. Para SHAP/permutation importance se filtraran filas con NaN cuando sea necesario.\"\n",
    "    )\n",
    "\n",
    "# Alinear el orden de columnas si el modelo lo guarda (evita desajustes silenciosos)\n",
    "if hasattr(model, \"feature_names_in_\"):\n",
    "    model_feats = list(model.feature_names_in_)\n",
    "    missing = [c for c in model_feats if c not in X.columns]\n",
    "    extra = [c for c in X.columns if c not in model_feats]\n",
    "\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"[WARN] Faltan {len(missing)} features que el modelo espera (ejemplos): {missing[:10]}\"\n",
    "            + (\" ...\" if len(missing) > 10 else \"\")\n",
    "        )\n",
    "    if extra:\n",
    "        print(\n",
    "            f\"[INFO] X tiene {len(extra)} columnas extra no usadas por el modelo (ejemplos): {extra[:10]}\"\n",
    "            + (\" ...\" if len(extra) > 10 else \"\")\n",
    "        )\n",
    "\n",
    "    # Reindexa a las features del modelo (si faltan, quedaran NaN -> se aviso arriba)\n",
    "    X = X.reindex(columns=model_feats)\n",
    "\n",
    "# -------------------------\n",
    "# Resolver encoding correcto (AASM vs LabelEncoder) para y, si aplica\n",
    "# -------------------------\n",
    "if (\n",
    "    y_raw is not None\n",
    "    and hasattr(model, \"classes_\")\n",
    "    and len(getattr(model, \"classes_\", [])) == 5\n",
    "    and isinstance(list(model.classes_)[0], (int, np.integer))\n",
    "    and y_raw.dtype == object\n",
    "    and set(pd.Series(y_raw.dropna().unique()).tolist()).issubset(\n",
    "        {\"W\", \"N1\", \"N2\", \"N3\", \"REM\"}\n",
    "    )\n",
    "    and X is not None\n",
    "    and MODEL_STAGE_TO_LABEL is not None\n",
    "    and \"_map_alpha\" in locals()\n",
    "):\n",
    "\n",
    "    def _pred_labels(_m, _X: pd.DataFrame) -> np.ndarray:\n",
    "        _p = np.asarray(_m.predict(_X))\n",
    "        if _p.ndim == 2 and _p.shape[1] > 1:\n",
    "            if (\n",
    "                hasattr(_m, \"classes_\")\n",
    "                and len(getattr(_m, \"classes_\", [])) == _p.shape[1]\n",
    "            ):\n",
    "                return np.asarray(_m.classes_)[_p.argmax(axis=1)]\n",
    "            return _p.argmax(axis=1)\n",
    "        return _p\n",
    "\n",
    "    _X_ok = X.dropna()\n",
    "    if len(_X_ok) > 0:\n",
    "        _n = min(2000, len(_X_ok))\n",
    "        _rng = np.random.default_rng(RANDOM_STATE)\n",
    "        _idx = _rng.choice(len(_X_ok), size=_n, replace=False)\n",
    "        _X_chk = _X_ok.iloc[_idx]\n",
    "        _y_str_chk = y_raw.loc[_X_chk.index]\n",
    "        _pred = _pred_labels(model, _X_chk)\n",
    "\n",
    "        _y_aasm = _y_str_chk.map(_map_aasm)\n",
    "        _y_alpha = _y_str_chk.map(_map_alpha)\n",
    "        _acc_aasm = (np.asarray(_y_aasm) == _pred).mean()\n",
    "        _acc_alpha = (np.asarray(_y_alpha) == _pred).mean()\n",
    "\n",
    "        if _acc_alpha > _acc_aasm + 1e-6:\n",
    "            y = y_raw.map(_map_alpha)\n",
    "            MODEL_STAGE_TO_LABEL = _map_alpha\n",
    "            print(\n",
    "                \"[INFO] Encoding detectado: LabelEncoder (alfabético). y se remapea a labels del modelo.\"\n",
    "            )\n",
    "            print(f\"       acc(alpha)={_acc_alpha:.3f} vs acc(aasm)={_acc_aasm:.3f}\")\n",
    "        else:\n",
    "            y = y_raw.map(_map_aasm)\n",
    "            MODEL_STAGE_TO_LABEL = _map_aasm\n",
    "            print(\n",
    "                \"[INFO] Encoding detectado: AASM (W,N1,N2,N3,REM). y se mapea a labels del modelo.\"\n",
    "            )\n",
    "            print(f\"       acc(aasm)={_acc_aasm:.3f} vs acc(alpha)={_acc_alpha:.3f}\")\n",
    "\n",
    "# Construir mapping inverso (para imprimir labels correctamente)\n",
    "if isinstance(MODEL_STAGE_TO_LABEL, dict) and MODEL_STAGE_TO_LABEL:\n",
    "    try:\n",
    "        MODEL_LABEL_TO_STAGE = {\n",
    "            int(v): k for k, v in MODEL_STAGE_TO_LABEL.items() if pd.notna(v)\n",
    "        }\n",
    "    except Exception:\n",
    "        MODEL_LABEL_TO_STAGE = None\n",
    "\n",
    "feature_cols = list(X.columns)\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Features: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
    "\n",
    "if y is not None:\n",
    "    class_counts = y.value_counts(dropna=False).sort_index()\n",
    "    print(f\"\\nDistribucion de clases ({y.nunique(dropna=True)} clases):\")\n",
    "    for cls, count in class_counts.items():\n",
    "        # Si el modelo usa ids \"raros\" (LabelEncoder), preferimos mostrar la etiqueta real\n",
    "        if MODEL_LABEL_TO_STAGE is not None:\n",
    "            try:\n",
    "                label = MODEL_LABEL_TO_STAGE.get(int(cls), str(cls))\n",
    "            except Exception:\n",
    "                label = str(cls)\n",
    "            print(f\"  {label} ({cls}): {count:,} ({100*count/len(y):.1f}%)\")\n",
    "        else:\n",
    "            try:\n",
    "                cls_int = int(cls) if pd.notna(cls) else None\n",
    "            except (ValueError, TypeError):\n",
    "                cls_int = None\n",
    "            label = STAGE_LABELS.get(cls_int, str(cls))\n",
    "            print(f\"  {label} ({cls}): {count:,} ({100*count/len(y):.1f}%)\")\n",
    "else:\n",
    "    print(\n",
    "        \"[WARN] No se encontro columna de etiqueta; se omiten secciones dependientes de y\"\n",
    "    )\n",
    "\n",
    "# Mostrar importancia nativa si esta disponible\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    imp_native = pd.DataFrame(\n",
    "        {\"feature\": feature_cols, \"importance\": model.feature_importances_}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "    print(\"\\nTop 10 features (importancia nativa - Gini/Gain):\")\n",
    "    print(imp_native.head(10).to_string(index=False))\n",
    "\n",
    "# --- Muestreo para SHAP/permutation importance ---\n",
    "# NOTA: Ya estamos usando el test set, asi que el analisis es valido\n",
    "\n",
    "\n",
    "def _sample_rows(\n",
    "    X_in: pd.DataFrame,\n",
    "    y_in: pd.Series | None,\n",
    "    n: int,\n",
    "    *,\n",
    "    stratify: bool,\n",
    "    random_state: int = RANDOM_STATE,\n",
    ") -> tuple[pd.DataFrame, pd.Series | None]:\n",
    "    \"\"\"Muestrea filas de X e y, filtrando NaN y opcionalmente estratificando.\"\"\"\n",
    "    if X_in.isna().any().any():\n",
    "        X_ok = X_in.dropna()\n",
    "        if len(X_ok) == 0:\n",
    "            raise ValueError(\n",
    "                \"X tiene NaNs en todas las filas; no se puede muestrear para SHAP\"\n",
    "            )\n",
    "        y_ok = y_in.loc[X_ok.index] if y_in is not None else None\n",
    "        dropped = len(X_in) - len(X_ok)\n",
    "        if dropped:\n",
    "            print(f\"[INFO] Filtrando {dropped:,} filas con NaN/inf para analisis\")\n",
    "    else:\n",
    "        X_ok, y_ok = X_in, y_in\n",
    "\n",
    "    if n >= len(X_ok):\n",
    "        return X_ok.copy(), y_ok.copy() if y_ok is not None else None\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    if stratify and (y_ok is not None) and (y_ok.nunique(dropna=True) > 1):\n",
    "        n_classes = int(y_ok.nunique(dropna=True))\n",
    "        per_class = max(1, n // n_classes)\n",
    "        df_join = X_ok.copy()\n",
    "        df_join[\"__y__\"] = y_ok\n",
    "\n",
    "        parts = []\n",
    "        for _, g in df_join.groupby(\"__y__\"):\n",
    "            sample_n = min(len(g), per_class)\n",
    "            idx = rng.choice(len(g), size=sample_n, replace=False)\n",
    "            parts.append(g.iloc[idx])\n",
    "        sampled = pd.concat(parts, axis=0)\n",
    "\n",
    "        if len(sampled) < n:\n",
    "            remaining = df_join.drop(index=sampled.index)\n",
    "            if len(remaining) > 0:\n",
    "                fill_n = min(n - len(sampled), len(remaining))\n",
    "                fill_idx = rng.choice(len(remaining), size=fill_n, replace=False)\n",
    "                fill = remaining.iloc[fill_idx]\n",
    "                sampled = pd.concat([sampled, fill], axis=0)\n",
    "\n",
    "        sampled = sampled.sample(n=min(n, len(sampled)), random_state=random_state)\n",
    "        y_s = sampled.pop(\"__y__\")\n",
    "        return sampled, y_s\n",
    "\n",
    "    idx = rng.choice(len(X_ok), size=n, replace=False)\n",
    "    X_s = X_ok.iloc[idx]\n",
    "    y_s = y_ok.iloc[idx] if y_ok is not None else None\n",
    "    return X_s, y_s\n",
    "\n",
    "\n",
    "if len(X) > MAX_SAMPLES:\n",
    "    X_sample, y_sample = _sample_rows(X, y, MAX_SAMPLES, stratify=STRATIFY_SAMPLE)\n",
    "    print(\n",
    "        f\"\\nUsando submuestra de {len(X_sample):,} para SHAP (de {len(X):,} total en test set)\"\n",
    "    )\n",
    "else:\n",
    "    X_sample, y_sample = _sample_rows(X, y, len(X), stratify=False)\n",
    "    print(f\"\\nUsando test set completo ({len(X_sample):,} muestras) para SHAP\")\n",
    "\n",
    "print(f\"Sample shape: {X_sample.shape}\")\n",
    "\n",
    "# Flag para indicar si permutation importance fue calculado\n",
    "_permutation_importance_computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Validación de compatibilidad modelo-datos\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"=== VALIDACIÓN DE COMPATIBILIDAD MODELO-DATOS ===\\n\")\n",
    "\n",
    "# Guard: esta celda asume que ya corriste la celda anterior (carga/split/feature matrix)\n",
    "_required = [\"MODEL_PATH\", \"model\", \"df\", \"X\", \"y\"]\n",
    "_missing = [k for k in _required if k not in globals()]\n",
    "if _missing:\n",
    "    print(f\"[ERROR] Faltan variables: {_missing}\")\n",
    "    print(\"Ejecuta primero: Celda 2 (config) y luego Celda 4 (cargar datos/split).\")\n",
    "else:\n",
    "    # Cargar métricas originales del modelo\n",
    "    _metrics_path = MODEL_PATH.replace(\"_model.pkl\", \"_metrics.json\")\n",
    "    _original_accuracy = None\n",
    "    if os.path.exists(_metrics_path):\n",
    "        with open(_metrics_path) as _f:\n",
    "            _metrics = _json.load(_f)\n",
    "        _original_accuracy = _metrics.get(\"metrics\", {}).get(\"accuracy\")\n",
    "        print(\n",
    "            f\"Accuracy reportada en entrenamiento: {_original_accuracy:.3f}\"\n",
    "            if _original_accuracy\n",
    "            else \"N/A\"\n",
    "        )\n",
    "        print(f\"Fecha de entrenamiento: {_metrics.get('timestamp', 'N/A')}\")\n",
    "\n",
    "    # Diagnóstico rápido: clases y tipos\n",
    "    if hasattr(model, \"classes_\"):\n",
    "        print(f\"model.classes_: {list(model.classes_)}\")\n",
    "    print(f\"Test set size (df): {len(df):,}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    if y is not None:\n",
    "        print(f\"y dtype: {y.dtype}\")\n",
    "\n",
    "    # Verificar accuracy actual sobre una muestra pequeña\n",
    "    _X_clean = X.dropna()\n",
    "    _dropped = len(X) - len(_X_clean)\n",
    "    if _dropped:\n",
    "        print(f\"[INFO] Filtrando {_dropped:,} filas con NaN para validación\")\n",
    "    _y_clean = y.loc[_X_clean.index] if y is not None else None\n",
    "\n",
    "    _data_compatible = True\n",
    "    if _y_clean is not None and len(_X_clean) > 0:\n",
    "        _test_n = min(2000, len(_X_clean))\n",
    "        _rng_test = np.random.default_rng(RANDOM_STATE)\n",
    "        _test_idx = _rng_test.choice(len(_X_clean), size=_test_n, replace=False)\n",
    "        _X_test = _X_clean.iloc[_test_idx]\n",
    "        _y_test = _y_clean.iloc[_test_idx]\n",
    "\n",
    "        _y_pred_test = model.predict(_X_test)\n",
    "        _y_pred_arr = np.asarray(_y_pred_test)\n",
    "\n",
    "        # Robustez: algunos wrappers devuelven matriz (n, k) en predict\n",
    "        if _y_pred_arr.ndim == 2 and _y_pred_arr.shape[1] > 1:\n",
    "            if (\n",
    "                hasattr(model, \"classes_\")\n",
    "                and len(getattr(model, \"classes_\", [])) == _y_pred_arr.shape[1]\n",
    "            ):\n",
    "                _y_pred_labels = np.asarray(model.classes_)[_y_pred_arr.argmax(axis=1)]\n",
    "                print(\n",
    "                    \"[WARN] model.predict devolvió matriz; usando argmax y model.classes_ para labels\"\n",
    "                )\n",
    "            else:\n",
    "                _y_pred_labels = _y_pred_arr.argmax(axis=1)\n",
    "                print(\n",
    "                    \"[WARN] model.predict devolvió matriz; usando argmax (sin mapeo a clases)\"\n",
    "                )\n",
    "        else:\n",
    "            _y_pred_labels = _y_pred_arr\n",
    "\n",
    "        _current_accuracy = (np.asarray(_y_pred_labels) == np.asarray(_y_test)).mean()\n",
    "        print(\n",
    "            f\"Accuracy actual sobre muestra aleatoria ({_test_n}): {_current_accuracy:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Evaluar compatibilidad\n",
    "        if _original_accuracy and abs(_current_accuracy - _original_accuracy) > 0.2:\n",
    "            _data_compatible = False\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"[ADVERTENCIA]  ADVERTENCIA: POSIBLE INCOMPATIBILIDAD DE DATOS\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"\"\"\n",
    "La accuracy actual ({_current_accuracy:.1%}) difiere significativamente\n",
    "de la accuracy original ({_original_accuracy:.1%}).\n",
    "\n",
    "Posibles causas:\n",
    "1. El archivo de features fue regenerado con parámetros diferentes\n",
    "2. La versión de las librerías de extracción cambió\n",
    "3. Se modificó el preprocesamiento de señales\n",
    "4. Se desalinearon las clases/encoding (p.ej. 0..4 vs 1..5)\n",
    "\n",
    "El análisis SHAP sigue siendo válido para explicar cómo el modelo\n",
    "interpreta estos datos, pero NO refleja el comportamiento real del\n",
    "modelo entrenado.\n",
    "\n",
    "RECOMENDACIÓN: Regenerar el modelo con los datos actuales O usar el\n",
    "dataset de features original.\n",
    "\"\"\")\n",
    "        elif _current_accuracy > 0.5:\n",
    "            print(\"\\n[OK] Los datos parecen compatibles con el modelo\")\n",
    "        else:\n",
    "            _data_compatible = False\n",
    "            print(\n",
    "                \"\\n[ADVERTENCIA]  Accuracy muy baja - revisar compatibilidad de datos\"\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: explicaciones globales y locales\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Helpers de labels y clases (robusto a clases no-int)\n",
    "\n",
    "\n",
    "def _fmt_stage_label(cls) -> str:\n",
    "    \"\"\"Formatea una etiqueta de clase a su nombre legible.\"\"\"\n",
    "    if pd.isna(cls):\n",
    "        return \"NaN\"\n",
    "    try:\n",
    "        cls_int = int(cls)\n",
    "    except (ValueError, TypeError):\n",
    "        cls_int = None\n",
    "    if cls_int is not None:\n",
    "        # Si el modelo usa encoding tipo LabelEncoder, el notebook define MODEL_LABEL_TO_STAGE\n",
    "        try:\n",
    "            if (\n",
    "                \"MODEL_LABEL_TO_STAGE\" in globals()\n",
    "                and globals()[\"MODEL_LABEL_TO_STAGE\"]\n",
    "            ):\n",
    "                return globals()[\"MODEL_LABEL_TO_STAGE\"].get(cls_int, str(cls))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return STAGE_LABELS.get(cls_int, str(cls))\n",
    "    return str(cls)\n",
    "\n",
    "\n",
    "# TreeExplainer es eficiente para RF / XGBoost; intentar probas para interpretabilidad\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(model, model_output=\"probability\")\n",
    "    print(\"Explainer: TreeExplainer con output=probability\")\n",
    "    print(\n",
    "        \"  (Los SHAP values representan contribuciones a la probabilidad de cada clase)\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"[WARN] TreeExplainer(probability) falló ({e}); usando TreeExplainer por defecto\"\n",
    "    )\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    print(\"  (Los SHAP values representan contribuciones al log-odds o margin)\")\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Detectar formato de shap_values (lista para multi-clase en SHAP antiguo, o array 3D en SHAP nuevo)\n",
    "is_multiclass_list = isinstance(shap_values, list)\n",
    "is_multiclass_3d = isinstance(shap_values, np.ndarray) and shap_values.ndim == 3\n",
    "\n",
    "n_classes = (\n",
    "    len(shap_values)\n",
    "    if is_multiclass_list\n",
    "    else (shap_values.shape[2] if is_multiclass_3d else 1)\n",
    ")\n",
    "print(\n",
    "    f\"SHAP values calculados. Multi-clase: {is_multiclass_list or is_multiclass_3d} ({n_classes} clases)\"\n",
    ")\n",
    "\n",
    "# Determinar clases del modelo (si existen) y su orden\n",
    "model_classes = list(getattr(model, \"classes_\", []))\n",
    "if model_classes and len(model_classes) == n_classes:\n",
    "    class_values = model_classes\n",
    "else:\n",
    "    class_values = list(range(n_classes))\n",
    "\n",
    "class_to_index = {cls: i for i, cls in enumerate(class_values)}\n",
    "\n",
    "print(\n",
    "    \"Clases del modelo (orden): \"\n",
    "    + \", \".join([f\"{_fmt_stage_label(c)} ({c})\" for c in class_values])\n",
    ")\n",
    "\n",
    "\n",
    "def _choose_main_class_label():\n",
    "    \"\"\"Clase principal para plots (moda de y_sample si existe; si no, predicción).\"\"\"\n",
    "    if y_sample is not None and len(y_sample) > 0:\n",
    "        mode_val = y_sample.mode(dropna=True)\n",
    "        if len(mode_val) > 0:\n",
    "            return mode_val.iloc[0]\n",
    "    try:\n",
    "        return model.predict(X_sample.iloc[[0]])[0]\n",
    "    except Exception:\n",
    "        return class_values[0]\n",
    "\n",
    "\n",
    "main_class_label = _choose_main_class_label()\n",
    "main_class_index = class_to_index.get(main_class_label, 0)\n",
    "print(\n",
    "    f\"Clase principal para plots: {_fmt_stage_label(main_class_label)} ({main_class_label}) [idx={main_class_index}]\"\n",
    ")\n",
    "\n",
    "\n",
    "def _mean_abs_shap(shap_vals) -> np.ndarray:\n",
    "    \"\"\"Calcula mean(|SHAP|) agregado sobre todas las clases para ranking global.\"\"\"\n",
    "    if isinstance(shap_vals, list):\n",
    "        # lista de (n_samples, n_features) por clase -> promedio entre clases\n",
    "        return np.mean([np.abs(sv) for sv in shap_vals], axis=0)\n",
    "    elif isinstance(shap_vals, np.ndarray) and shap_vals.ndim == 3:\n",
    "        # (n_samples, n_features, n_classes) -> promedio entre clases\n",
    "        return np.mean(np.abs(shap_vals), axis=2)\n",
    "    return np.abs(shap_vals)\n",
    "\n",
    "\n",
    "shap_abs = _mean_abs_shap(shap_values)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# IMPORTANTE (validez científica): agregación por sujeto\n",
    "# ------------------------------------------------------------\n",
    "# Los epochs dentro de un sujeto están correlacionados; si promediamos por epoch,\n",
    "# sujetos con más epochs dominan el ranking. Para evitarlo, agregamos primero por sujeto\n",
    "# (mean(|SHAP|) por sujeto) y luego promediamos entre sujetos (peso igual por sujeto).\n",
    "\n",
    "mean_abs_epoch = shap_abs.mean(axis=0)  # referencia (ponderado por epoch)\n",
    "mean_abs_subject = None\n",
    "\n",
    "if \"subject_core\" in df.columns:\n",
    "    subj = df.loc[X_sample.index, \"subject_core\"].astype(str)\n",
    "    shap_df = pd.DataFrame(shap_abs, columns=feature_cols)\n",
    "    subj_means = shap_df.groupby(subj.to_numpy()).mean()  # (n_subjects, n_features)\n",
    "    mean_abs_subject = subj_means.mean(axis=0).to_numpy()\n",
    "    mean_abs = mean_abs_subject\n",
    "    print(\n",
    "        f\"[INFO] SHAP global agregado por sujeto: {subj_means.shape[0]} sujetos (peso igual por sujeto)\"\n",
    "    )\n",
    "else:\n",
    "    mean_abs = mean_abs_epoch\n",
    "    print(\n",
    "        \"[WARN] No hay 'subject_core'; usando agregado por epoch (puede sesgar por sujetos con más epochs)\"\n",
    "    )\n",
    "\n",
    "top_idx = np.argsort(mean_abs)[::-1]\n",
    "\n",
    "# Mostrar ranking de features\n",
    "print(\n",
    "    \"\\nTop 15 features por SHAP (mean|SHAP| agregado multi-clase, ponderación por sujeto si es posible):\"\n",
    ")\n",
    "for rank, idx in enumerate(top_idx[:15], 1):\n",
    "    print(f\"  {rank:2d}. {feature_cols[idx]}: {mean_abs[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: Plots globales (bar + beeswarm)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Summary bar (importancias globales)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "\n",
    "# Renombrar leyenda \"Class k\" -> etiqueta de etapa (si aplica)\n",
    "try:\n",
    "    ax = plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if labels and all(lbl.startswith(\"Class \") for lbl in labels):\n",
    "        label_map = {\n",
    "            f\"Class {i}\": _fmt_stage_label(class_values[i])\n",
    "            for i in range(min(len(labels), len(class_values)))\n",
    "        }\n",
    "        ax.legend(handles, [label_map.get(lbl, lbl) for lbl in labels], title=\"Clase\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "plt.title(\"SHAP Feature Importances (global)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_importances_bar.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm - para multi-clase mostramos una clase representativa\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "if is_multiclass_list:\n",
    "    shap.summary_plot(\n",
    "        shap_values[main_class_index], X_sample, show=False, max_display=20\n",
    "    )\n",
    "    plt.title(f\"SHAP Beeswarm - Clase {_fmt_stage_label(main_class_label)}\")\n",
    "elif is_multiclass_3d:\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, :, main_class_index], X_sample, show=False, max_display=20\n",
    "    )\n",
    "    plt.title(f\"SHAP Beeswarm - Clase {_fmt_stage_label(main_class_label)}\")\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_sample, show=False, max_display=20)\n",
    "    plt.title(\"SHAP Beeswarm\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"shap_beeswarm.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plots guardados en {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP por clase: análisis específico para cada etapa del sueño\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Crear beeswarm para cada clase (importante para entender qué features discriminan cada etapa)\n",
    "# Nota: algunas versiones de SHAP pueden crear figuras separadas aunque uses subplots.\n",
    "\n",
    "n_classes_to_plot = min(n_classes, 5)  # limitar si hay muchas clases\n",
    "\n",
    "fig, axes = plt.subplots(1, n_classes_to_plot, figsize=(5 * n_classes_to_plot, 6))\n",
    "if n_classes_to_plot == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for class_index in range(n_classes_to_plot):\n",
    "    plt.sca(axes[class_index])\n",
    "    class_value = (\n",
    "        class_values[class_index] if class_index < len(class_values) else class_index\n",
    "    )\n",
    "    class_label = _fmt_stage_label(class_value)\n",
    "\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_index]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_index]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    shap.summary_plot(sv_class, X_sample, show=False, max_display=10, plot_size=None)\n",
    "    axes[class_index].set_title(f\"Clase: {class_label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_beeswarm_by_class.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Tabla: Top 5 features por clase\n",
    "print(\"\\nTop 5 features más importantes por clase:\")\n",
    "print(\"-\" * 60)\n",
    "for class_index in range(n_classes):\n",
    "    class_value = (\n",
    "        class_values[class_index] if class_index < len(class_values) else class_index\n",
    "    )\n",
    "    class_label = _fmt_stage_label(class_value)\n",
    "\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_index]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_index]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    mean_abs_class = np.abs(sv_class).mean(axis=0)\n",
    "    top5_idx = np.argsort(mean_abs_class)[::-1][:5]\n",
    "    top5_features = [feature_cols[i] for i in top5_idx]\n",
    "    print(f\"{class_label}: {', '.join(top5_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: Dependence plots para top features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "N_TOP = 5\n",
    "\n",
    "# Preparar shap_values para dependence plot (necesita 2D para una clase)\n",
    "if is_multiclass_list:\n",
    "    shap_for_dep = shap_values[main_class_index]\n",
    "elif is_multiclass_3d:\n",
    "    shap_for_dep = shap_values[:, :, main_class_index]\n",
    "else:\n",
    "    shap_for_dep = shap_values\n",
    "\n",
    "fig, axes = plt.subplots(1, N_TOP, figsize=(4 * N_TOP, 4))\n",
    "if N_TOP == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, idx in enumerate(top_idx[:N_TOP]):\n",
    "    fname = feature_cols[idx]\n",
    "    plt.sca(axes[i])\n",
    "    shap.dependence_plot(fname, shap_for_dep, X_sample, show=False, ax=axes[i])\n",
    "    axes[i].set_title(fname)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_dependence_top5.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dependence plots para top {N_TOP} features guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP local: explica casos individuales\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Elige un indice para inspeccionar\n",
    "sample_idx = X_sample.index[0]\n",
    "\n",
    "# Info del ejemplo\n",
    "true_label = y_sample.loc[sample_idx] if y_sample is not None else \"?\"\n",
    "pred_label = model.predict(X_sample.loc[[sample_idx]])[0]\n",
    "pred_class_index = class_to_index.get(pred_label, main_class_index)\n",
    "\n",
    "pred_proba = (\n",
    "    model.predict_proba(X_sample.loc[[sample_idx]])[0]\n",
    "    if hasattr(model, \"predict_proba\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "print(f\"Ejemplo idx={sample_idx}\")\n",
    "print(f\"  Etiqueta real: {_fmt_stage_label(true_label)}\")\n",
    "print(f\"  Predicción:    {_fmt_stage_label(pred_label)}\")\n",
    "\n",
    "if pred_proba is not None:\n",
    "    if hasattr(model, \"classes_\"):\n",
    "        proba_labels = [_fmt_stage_label(c) for c in model.classes_]\n",
    "    else:\n",
    "        proba_labels = [str(i) for i in range(len(pred_proba))]\n",
    "    print(f\"  Probabilidades: {dict(zip(proba_labels, pred_proba.round(3)))}\")\n",
    "\n",
    "# Calcular SHAP del ejemplo\n",
    "a_single = X_sample.loc[[sample_idx]]\n",
    "shap_value_single = explainer.shap_values(a_single)\n",
    "\n",
    "# Extraer valores para la clase predicha (o única)\n",
    "if isinstance(shap_value_single, list):\n",
    "    values = shap_value_single[pred_class_index][0]\n",
    "    base_value = (\n",
    "        explainer.expected_value[pred_class_index]\n",
    "        if isinstance(explainer.expected_value, (list, np.ndarray))\n",
    "        else explainer.expected_value\n",
    "    )\n",
    "elif isinstance(shap_value_single, np.ndarray) and shap_value_single.ndim == 3:\n",
    "    values = shap_value_single[0, :, pred_class_index]\n",
    "    base_value = (\n",
    "        explainer.expected_value[pred_class_index]\n",
    "        if isinstance(explainer.expected_value, (list, np.ndarray))\n",
    "        else explainer.expected_value\n",
    "    )\n",
    "else:\n",
    "    values = shap_value_single[0]\n",
    "    base_value = explainer.expected_value\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=values,\n",
    "        base_values=base_value,\n",
    "        data=X_sample.loc[sample_idx].values,\n",
    "        feature_names=feature_cols,\n",
    "    ),\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"SHAP Waterfall - Predicción: {_fmt_stage_label(pred_label)}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, f\"shap_waterfall_idx{sample_idx}.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGuardado waterfall para idx={sample_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Análisis de errores: explicar casos mal clasificados\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if y_sample is not None:\n",
    "    # Predicciones en la muestra\n",
    "    y_pred = model.predict(X_sample)\n",
    "    y_pred_s = pd.Series(y_pred, index=X_sample.index)\n",
    "\n",
    "    # Debug: verificar alineación de tipos\n",
    "    print(f\"Tipos: y_sample={y_sample.dtype}, y_pred={y_pred_s.dtype}\")\n",
    "    print(f\"Valores únicos y_sample: {sorted(y_sample.unique())}\")\n",
    "    print(f\"Valores únicos y_pred: {sorted(y_pred_s.unique())}\")\n",
    "\n",
    "    # Comparación robusta (manejar posibles diferencias de tipo)\n",
    "    errors_mask = y_pred_s.astype(str) != y_sample.astype(str)\n",
    "    n_errors = int(errors_mask.sum())\n",
    "    accuracy = 1 - n_errors / len(y_sample)\n",
    "\n",
    "    print(\n",
    "        f\"\\nAccuracy en muestra: {accuracy:.3f} ({n_errors:,} errores de {len(y_sample):,})\"\n",
    "    )\n",
    "    print(\n",
    "        \"[ADVERTENCIA]  NOTA: Esta accuracy es sobre la muestra de análisis, no sobre test set.\"\n",
    "    )\n",
    "\n",
    "    if n_errors > 0 and accuracy < 0.5:\n",
    "        # Accuracy muy baja sugiere que la comparación no está funcionando correctamente\n",
    "        print(\"\\n[DEBUG] Verificando primeros 5 ejemplos:\")\n",
    "        for i in range(min(5, len(y_sample))):\n",
    "            idx = y_sample.index[i]\n",
    "            print(\n",
    "                f\"  idx={idx}: y_sample={y_sample.iloc[i]} (type={type(y_sample.iloc[i]).__name__}), \"\n",
    "                f\"y_pred={y_pred_s.iloc[i]} (type={type(y_pred_s.iloc[i]).__name__})\"\n",
    "            )\n",
    "\n",
    "    if n_errors > 0:\n",
    "        error_pairs = list(\n",
    "            zip(\n",
    "                y_sample.loc[errors_mask].astype(str).tolist(),\n",
    "                y_pred_s.loc[errors_mask].astype(str).tolist(),\n",
    "            )\n",
    "        )\n",
    "        error_counts = Counter(error_pairs)\n",
    "\n",
    "        print(\"\\nErrores más frecuentes (real -> predicho):\")\n",
    "        for (true_cls, pred_cls), count in error_counts.most_common(10):\n",
    "            true_lbl = _fmt_stage_label(\n",
    "                int(true_cls) if true_cls.isdigit() else true_cls\n",
    "            )\n",
    "            pred_lbl = _fmt_stage_label(\n",
    "                int(pred_cls) if pred_cls.isdigit() else pred_cls\n",
    "            )\n",
    "            pct = 100 * count / n_errors\n",
    "            print(f\"  {true_lbl} -> {pred_lbl}: {count} ({pct:.1f}% de errores)\")\n",
    "\n",
    "        # Matriz de confusión simplificada para errores\n",
    "        print(\"\\nPatrones de confusión observados:\")\n",
    "        print(\"  - N1 es típicamente la clase más difícil (transición)\")\n",
    "        print(\"  - Confusiones N1<->N2 y N1<->REM son comunes en sleep staging\")\n",
    "\n",
    "        # Analizar SHAP de ejemplos mal clasificados (muestra de errores)\n",
    "        error_indices = X_sample.index[errors_mask.values]\n",
    "        n_error_examples = min(3, len(error_indices))\n",
    "\n",
    "        if n_error_examples > 0:\n",
    "            print(\n",
    "                f\"\\n--- Análisis SHAP de {n_error_examples} ejemplos mal clasificados ---\"\n",
    "            )\n",
    "\n",
    "            fig, axes = plt.subplots(\n",
    "                1, n_error_examples, figsize=(6 * n_error_examples, 5)\n",
    "            )\n",
    "            if n_error_examples == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for i, err_idx in enumerate(error_indices[:n_error_examples]):\n",
    "                true_cls = y_sample.loc[err_idx]\n",
    "                pred_cls = y_pred_s.loc[err_idx]\n",
    "\n",
    "                true_lbl = _fmt_stage_label(true_cls)\n",
    "                pred_lbl = _fmt_stage_label(pred_cls)\n",
    "\n",
    "                row_pos = X_sample.index.get_loc(err_idx)\n",
    "                pred_class_idx = class_to_index.get(pred_cls, main_class_index)\n",
    "\n",
    "                # Obtener SHAP values para la clase predicha (explica por qué predijo eso)\n",
    "                if is_multiclass_list:\n",
    "                    sv_err = shap_values[pred_class_idx][row_pos]\n",
    "                elif is_multiclass_3d:\n",
    "                    sv_err = shap_values[row_pos, :, pred_class_idx]\n",
    "                else:\n",
    "                    sv_err = shap_values[row_pos]\n",
    "\n",
    "                # Top features que más contribuyeron\n",
    "                top_err_idx = np.argsort(np.abs(sv_err))[::-1][:10]\n",
    "\n",
    "                ax = axes[i]\n",
    "                colors = [\n",
    "                    \"#d73027\" if v > 0 else \"#4575b4\" for v in sv_err[top_err_idx]\n",
    "                ]\n",
    "                ax.barh(range(len(top_err_idx)), sv_err[top_err_idx], color=colors)\n",
    "                ax.set_yticks(range(len(top_err_idx)))\n",
    "                ax.set_yticklabels([feature_cols[j] for j in top_err_idx])\n",
    "                ax.set_xlabel(\"SHAP value\")\n",
    "                ax.set_title(f\"Error: {true_lbl} → {pred_lbl}\")\n",
    "                ax.invert_yaxis()\n",
    "                ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                os.path.join(OUTPUT_DIR, \"shap_error_analysis.png\"),\n",
    "                dpi=150,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"[WARN] No se encontró columna de etiqueta; se omite análisis de errores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Leyenda del grafico de errores:**\n",
    "- **Rojo**: Features que empujan HACIA la prediccion (clase incorrecta)\n",
    "- **Azul**: Features que empujan CONTRA la prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Permutation importance (model-agnostic)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Variable global para tracking\n",
    "pi = None  # Inicializar para evitar NameError en celdas posteriores\n",
    "\n",
    "if y_sample is not None:\n",
    "    print(\"Calculando permutation importance (puede tardar)...\")\n",
    "    print(f\"  - Scoring: {PERMUTATION_SCORING}\")\n",
    "    print(f\"  - Repeticiones: {PERMUTATION_REPEATS}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # IMPORTANTE (validez científica): balancear por sujeto\n",
    "    # --------------------------------------------------------\n",
    "    # PI asume muestras i.i.d. y puede quedar dominada por sujetos con más epochs.\n",
    "    # Para mitigar, tomamos una submuestra con máximo N epochs por sujeto.\n",
    "    X_pi, y_pi = X_sample, y_sample\n",
    "    if \"subject_core\" in df.columns:\n",
    "        subj = df.loc[X_sample.index, \"subject_core\"].astype(str)\n",
    "        rng_pi = np.random.default_rng(RANDOM_STATE)\n",
    "        max_per_subject = 50\n",
    "        chosen_idx = []\n",
    "        groups = subj.groupby(subj).groups  # subject -> indices\n",
    "        for _, idxs in groups.items():\n",
    "            idxs = np.array(list(idxs))\n",
    "            take = min(len(idxs), max_per_subject)\n",
    "            sel = rng_pi.choice(idxs, size=take, replace=False)\n",
    "            chosen_idx.extend(sel.tolist())\n",
    "        chosen_idx = pd.Index(chosen_idx)\n",
    "        X_pi = X_sample.loc[chosen_idx]\n",
    "        y_pi = y_sample.loc[chosen_idx]\n",
    "        print(\n",
    "            f\"[INFO] PI sobre muestra balanceada por sujeto: {len(chosen_idx):,} epochs \"\n",
    "            f\"(max {max_per_subject}/sujeto, n_sujetos={len(groups):,})\"\n",
    "        )\n",
    "\n",
    "    pi = permutation_importance(\n",
    "        model,\n",
    "        X_pi,\n",
    "        y_pi,\n",
    "        n_repeats=PERMUTATION_REPEATS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        scoring=PERMUTATION_SCORING,\n",
    "    )\n",
    "\n",
    "    _permutation_importance_computed = True\n",
    "\n",
    "    pi_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_cols,\n",
    "            \"importance_mean\": pi.importances_mean,\n",
    "            \"importance_std\": pi.importances_std,\n",
    "        }\n",
    "    ).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "    # Calcular intervalo de confianza al 95% (aprox. ±1.96*std/sqrt(n))\n",
    "    pi_df[\"ci_95\"] = 1.96 * pi_df[\"importance_std\"] / np.sqrt(PERMUTATION_REPEATS)\n",
    "\n",
    "    pi_df.to_csv(os.path.join(OUTPUT_DIR, \"permutation_importance.csv\"), index=False)\n",
    "\n",
    "    # Visualizar top 20\n",
    "    top_pi = pi_df.head(20)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(\n",
    "        range(len(top_pi)),\n",
    "        top_pi[\"importance_mean\"],\n",
    "        xerr=top_pi[\"importance_std\"],\n",
    "        color=\"steelblue\",\n",
    "        capsize=3,\n",
    "    )\n",
    "    plt.yticks(range(len(top_pi)), top_pi[\"feature\"])\n",
    "    plt.xlabel(f\"Decrease in {PERMUTATION_SCORING.replace('_', ' ')}\")\n",
    "    plt.title(\n",
    "        f\"Permutation Feature Importance (Top 20, n_repeats={PERMUTATION_REPEATS})\"\n",
    "    )\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"permutation_importance.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 features (permutation importance):\")\n",
    "    print(\n",
    "        pi_df[[\"feature\", \"importance_mean\", \"importance_std\", \"ci_95\"]]\n",
    "        .head(10)\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "\n",
    "    # Identificar features con importancia significativamente > 0\n",
    "    significant = pi_df[pi_df[\"importance_mean\"] > 2 * pi_df[\"importance_std\"]]\n",
    "    print(\n",
    "        f\"\\nFeatures con importancia significativa (mean > 2*std): {len(significant)}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[WARN] No se encontró columna de etiqueta; se omite permutation importance\")\n",
    "    _permutation_importance_computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Comparación y correlación entre métodos de importancia\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Crear DataFrame comparativo\n",
    "comparison = pd.DataFrame({\"feature\": feature_cols})\n",
    "\n",
    "# SHAP importance\n",
    "comparison[\"shap_importance\"] = mean_abs\n",
    "comparison[\"shap_rank\"] = (\n",
    "    comparison[\"shap_importance\"].rank(ascending=False, method=\"min\").astype(int)\n",
    ")\n",
    "\n",
    "# Native importance (si disponible)\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    comparison[\"native_importance\"] = model.feature_importances_\n",
    "    comparison[\"native_rank\"] = (\n",
    "        comparison[\"native_importance\"].rank(ascending=False, method=\"min\").astype(int)\n",
    "    )\n",
    "\n",
    "# Permutation importance (si calculado)\n",
    "if _permutation_importance_computed and pi is not None:\n",
    "    comparison[\"perm_importance\"] = pi.importances_mean\n",
    "    comparison[\"perm_rank\"] = (\n",
    "        comparison[\"perm_importance\"].rank(ascending=False, method=\"min\").astype(int)\n",
    "    )\n",
    "\n",
    "# Ordenar por SHAP\n",
    "comparison = comparison.sort_values(\"shap_rank\")\n",
    "\n",
    "# Mostrar top 15\n",
    "print(\"Comparación de rankings de importancia (Top 15):\")\n",
    "rank_cols = [c for c in comparison.columns if c.endswith(\"_rank\")]\n",
    "display_cols = [\"feature\"] + rank_cols\n",
    "print(comparison[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "# --- Correlaciones entre métodos ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORRELACIÓN ENTRE MÉTODOS DE IMPORTANCIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "importance_cols = [c for c in comparison.columns if c.endswith(\"_importance\")]\n",
    "\n",
    "\n",
    "def _safe_corr(a: pd.Series, b: pd.Series) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Calcula correlaciones Spearman y Kendall de forma segura.\"\"\"\n",
    "    dfc = pd.concat([a, b], axis=1).dropna()\n",
    "    if dfc.shape[0] < 3:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    if dfc.iloc[:, 0].nunique() <= 1 or dfc.iloc[:, 1].nunique() <= 1:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    spearman_corr, spearman_p = spearmanr(dfc.iloc[:, 0], dfc.iloc[:, 1])\n",
    "    kendall_corr, kendall_p = kendalltau(dfc.iloc[:, 0], dfc.iloc[:, 1])\n",
    "    return spearman_corr, spearman_p, kendall_corr, kendall_p\n",
    "\n",
    "\n",
    "if len(importance_cols) >= 2:\n",
    "    corr_results = []\n",
    "    for i, col1 in enumerate(importance_cols):\n",
    "        for col2 in importance_cols[i + 1 :]:\n",
    "            spearman_corr, spearman_p, kendall_corr, kendall_p = _safe_corr(\n",
    "                comparison[col1], comparison[col2]\n",
    "            )\n",
    "            corr_results.append(\n",
    "                {\n",
    "                    \"Método 1\": col1.replace(\"_importance\", \"\"),\n",
    "                    \"Método 2\": col2.replace(\"_importance\", \"\"),\n",
    "                    \"Spearman ρ\": f\"{spearman_corr:.3f}\"\n",
    "                    if np.isfinite(spearman_corr)\n",
    "                    else \"nan\",\n",
    "                    \"p-value\": f\"{spearman_p:.2e}\"\n",
    "                    if np.isfinite(spearman_p)\n",
    "                    else \"nan\",\n",
    "                    \"Kendall τ\": f\"{kendall_corr:.3f}\"\n",
    "                    if np.isfinite(kendall_corr)\n",
    "                    else \"nan\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_results)\n",
    "    print(\"\\nCorrelaciones (sobre importancias):\")\n",
    "    print(corr_df.to_string(index=False))\n",
    "\n",
    "    # Interpretación (regla práctica basada en literatura)\n",
    "    print(\"\\nInterpretación (regla práctica, Mukaka 2012):\")\n",
    "    print(\"  ρ/τ > 0.7: Alta concordancia entre métodos (más robusto)\")\n",
    "    print(\"  ρ/τ 0.5-0.7: Concordancia moderada\")\n",
    "    print(\"  ρ/τ 0.3-0.5: Concordancia baja\")\n",
    "    print(\"  ρ/τ < 0.3: Concordancia muy baja (investigar diferencias)\")\n",
    "else:\n",
    "    print(\"No hay suficientes métodos de importancia para correlacionar\")\n",
    "    corr_results = []\n",
    "\n",
    "# Scatter plot de importancias\n",
    "if len(importance_cols) >= 2:\n",
    "    n_pairs = len(importance_cols) * (len(importance_cols) - 1) // 2\n",
    "    fig, axes = plt.subplots(1, n_pairs, figsize=(6 * n_pairs, 5))\n",
    "    if n_pairs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    pair_idx = 0\n",
    "    for i, col1 in enumerate(importance_cols):\n",
    "        for col2 in importance_cols[i + 1 :]:\n",
    "            ax = axes[pair_idx]\n",
    "            dfc = comparison[[col1, col2]].dropna()\n",
    "\n",
    "            ax.scatter(dfc[col1], dfc[col2], alpha=0.5, s=20)\n",
    "            ax.set_xlabel(col1.replace(\"_importance\", \"\").upper())\n",
    "            ax.set_ylabel(col2.replace(\"_importance\", \"\").upper())\n",
    "\n",
    "            # Línea de tendencia (si hay datos suficientes)\n",
    "            if len(dfc) >= 3 and dfc[col1].nunique() > 1:\n",
    "                z = np.polyfit(dfc[col1], dfc[col2], 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(dfc[col1].min(), dfc[col1].max(), 100)\n",
    "                ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, label=\"Tendencia lineal\")\n",
    "\n",
    "            r, _, _, _ = _safe_corr(comparison[col1], comparison[col2])\n",
    "            ax.set_title(\n",
    "                f\"Spearman ρ = {r:.3f}\" if np.isfinite(r) else \"Spearman ρ = nan\"\n",
    "            )\n",
    "            pair_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"importance_correlation_scatter.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Guardar comparación completa\n",
    "comparison.to_csv(os.path.join(OUTPUT_DIR, \"importance_comparison.csv\"), index=False)\n",
    "print(f\"\\nComparación completa guardada en {OUTPUT_DIR}/importance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota sobre discrepancias entre métodos\n",
    "\n",
    "Es común observar discrepancias entre SHAP, importancia nativa (Gini/Gain) y Permutation Importance:\n",
    "\n",
    "1. **SHAP vs Native (Gini/Gain)**: Generalmente correlacionados (ρ > 0.7) porque ambos miden contribución al modelo\n",
    "2. **SHAP/Native vs Permutation**: Pueden diferir significativamente porque:\n",
    "   - **Permutation Importance** mide el impacto en *performance* al permutar features\n",
    "   - Si features están correlacionadas, permutar una no afecta mucho porque otra compensa\n",
    "   - Features con alta importancia SHAP pero baja PI sugieren redundancia/correlación\n",
    "\n",
    "**Interpretación práctica:**\n",
    "- Features con alta importancia en TODOS los métodos → evidencia robusta\n",
    "- Features solo importantes en SHAP/Native → posible redundancia con otras features\n",
    "- Features solo importantes en Permutation → su efecto es único y no compensable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Importancia por grupos de features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "def categorize_feature(fname: str) -> str:\n",
    "    \"\"\"\n",
    "    Categoriza features PSG basándose en convenciones de nomenclatura estándar.\n",
    "\n",
    "    Categorías basadas en:\n",
    "    - Canales: EEG (Fpz-Cz, Pz-Oz), EOG, EMG\n",
    "    - Tipo de feature: espectral (bandas), temporal (stats), entropía, Hjorth, etc.\n",
    "\n",
    "    Ajustar patrones según los nombres específicos de features en el dataset.\n",
    "    \"\"\"\n",
    "    fname_lower = fname.lower()\n",
    "\n",
    "    # Primero detectar por canal (más específico)\n",
    "    if \"eog\" in fname_lower or \"eye\" in fname_lower:\n",
    "        return \"EOG\"\n",
    "    elif \"emg\" in fname_lower or \"chin\" in fname_lower:\n",
    "        return \"EMG\"\n",
    "\n",
    "    # Luego por tipo de feature (para EEG y general)\n",
    "    if any(\n",
    "        band in fname_lower\n",
    "        for band in [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\", \"sigma\"]\n",
    "    ):\n",
    "        if \"ratio\" in fname_lower:\n",
    "            return \"Spectral_Ratio\"\n",
    "        return \"Spectral_Power\"\n",
    "    elif \"spindle\" in fname_lower:\n",
    "        return \"Spindles\"\n",
    "    elif \"slow_wave\" in fname_lower or \"sw_\" in fname_lower:\n",
    "        return \"Slow_Waves\"\n",
    "    elif any(\n",
    "        temp in fname_lower for temp in [\"hjorth\", \"mobility\", \"complexity\", \"activity\"]\n",
    "    ):\n",
    "        return \"Hjorth\"\n",
    "    elif any(\n",
    "        ent in fname_lower\n",
    "        for ent in [\"entropy\", \"perm_ent\", \"sample_ent\", \"approx_ent\", \"spectral_ent\"]\n",
    "    ):\n",
    "        return \"Entropy\"\n",
    "    elif any(\n",
    "        stat in fname_lower\n",
    "        for stat in [\n",
    "            \"_mean\",\n",
    "            \"_std\",\n",
    "            \"_var\",\n",
    "            \"_skew\",\n",
    "            \"_kurt\",\n",
    "            \"_rms\",\n",
    "            \"_range\",\n",
    "            \"_min\",\n",
    "            \"_max\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Statistical\"\n",
    "    elif \"zcr\" in fname_lower or \"zero_cross\" in fname_lower:\n",
    "        return \"Zero_Crossing\"\n",
    "    elif \"dominant_freq\" in fname_lower:\n",
    "        return \"Dominant_Freq\"\n",
    "    elif any(\n",
    "        conn in fname_lower\n",
    "        for conn in [\"coherence\", \"correlation\", \"plv\", \"pli\", \"corr_\"]\n",
    "    ):\n",
    "        return \"Connectivity\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "# Asignar categorías\n",
    "feature_groups = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_cols,\n",
    "        \"group\": [categorize_feature(f) for f in feature_cols],\n",
    "        \"shap_importance\": mean_abs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Importancia agregada por grupo\n",
    "group_importance = (\n",
    "    feature_groups.groupby(\"group\")[\"shap_importance\"]\n",
    "    .agg([\"sum\", \"mean\", \"std\", \"count\"])\n",
    "    .rename(columns={\"std\": \"std_within_group\"})\n",
    ")\n",
    "group_importance = group_importance.sort_values(\"sum\", ascending=False)\n",
    "\n",
    "print(\"Importancia SHAP por grupo de features:\")\n",
    "print(group_importance.round(4).to_string())\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Importancia total por grupo\n",
    "ax1 = axes[0]\n",
    "n_groups = len(group_importance)\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_groups))\n",
    "bars1 = ax1.barh(range(n_groups), group_importance[\"sum\"], color=colors)\n",
    "ax1.set_yticks(range(n_groups))\n",
    "ax1.set_yticklabels(group_importance.index)\n",
    "ax1.set_xlabel(\"SHAP importance (sum)\")\n",
    "ax1.set_title(\"Importancia total por grupo\")\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Añadir count como anotación\n",
    "for i, (_, row) in enumerate(group_importance.iterrows()):\n",
    "    ax1.annotate(\n",
    "        f\"n={int(row['count'])}\",\n",
    "        xy=(row[\"sum\"], i),\n",
    "        xytext=(5, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        va=\"center\",\n",
    "        fontsize=8,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "# Importancia media por grupo (más justo para comparar grupos de distinto tamaño)\n",
    "ax2 = axes[1]\n",
    "ax2.barh(range(n_groups), group_importance[\"mean\"], color=colors)\n",
    "ax2.set_yticks(range(n_groups))\n",
    "ax2.set_yticklabels(group_importance.index)\n",
    "ax2.set_xlabel(\"SHAP importance (mean per feature)\")\n",
    "ax2.set_title(\"Importancia media por grupo\")\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_by_feature_group.png\"), dpi=150, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Distribución de grupos\n",
    "print(\"\\nDistribución de features por grupo:\")\n",
    "group_counts = feature_groups[\"group\"].value_counts()\n",
    "print(group_counts.to_string())\n",
    "\n",
    "# Top features por grupo\n",
    "print(\"\\nTop 3 features por grupo:\")\n",
    "for group in group_importance.index:\n",
    "    group_feats = feature_groups[feature_groups[\"group\"] == group].nlargest(\n",
    "        3, \"shap_importance\"\n",
    "    )\n",
    "    feat_list = \", \".join(group_feats[\"feature\"].tolist())\n",
    "    print(f\"  {group}: {feat_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis avanzado\n",
    "\n",
    "Las siguientes celdas son computacionalmente mas costosas (interacciones entre features y analisis de estabilidad bootstrap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP Interaction Values\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "MAX_FEATS_INTERACTION = 200\n",
    "if len(feature_cols) > MAX_FEATS_INTERACTION:\n",
    "    print(\n",
    "        f\"Saltando SHAP interaction values: {len(feature_cols)} features (> {MAX_FEATS_INTERACTION}). Ajusta MAX_FEATS_INTERACTION si realmente lo necesitas.\"\n",
    "    )\n",
    "else:\n",
    "    # Usar submuestra mas pequena para interactions\n",
    "    N_INTERACTION = min(500, len(X_sample))\n",
    "    X_interaction = X_sample.sample(N_INTERACTION, random_state=RANDOM_STATE)\n",
    "\n",
    "    print(f\"Calculando SHAP interaction values para {N_INTERACTION} muestras...\")\n",
    "    print(\"(Esto puede tardar varios minutos)\")\n",
    "\n",
    "    shap_interaction = explainer.shap_interaction_values(X_interaction)\n",
    "\n",
    "    # Para multi-clase, tomar la clase principal\n",
    "    if isinstance(shap_interaction, list):\n",
    "        shap_inter_class = shap_interaction[main_class_index]\n",
    "    elif isinstance(shap_interaction, np.ndarray) and shap_interaction.ndim == 4:\n",
    "        shap_inter_class = shap_interaction[:, :, :, main_class_index]\n",
    "    else:\n",
    "        shap_inter_class = shap_interaction\n",
    "\n",
    "    # Matriz de interacciones promedio\n",
    "    interaction_matrix = np.abs(shap_inter_class).mean(axis=0)\n",
    "\n",
    "    # Encontrar top interacciones (excluyendo diagonal = main effects)\n",
    "    n_feat = len(feature_cols)\n",
    "    interaction_pairs = []\n",
    "    for i in range(n_feat):\n",
    "        for j in range(i + 1, n_feat):\n",
    "            interaction_pairs.append(\n",
    "                {\n",
    "                    \"feature_1\": feature_cols[i],\n",
    "                    \"feature_2\": feature_cols[j],\n",
    "                    \"interaction_strength\": interaction_matrix[i, j],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    interaction_df = pd.DataFrame(interaction_pairs).sort_values(\n",
    "        \"interaction_strength\", ascending=False\n",
    "    )\n",
    "    print(\"\\nTop 10 interacciones entre features:\")\n",
    "    print(interaction_df.head(10).to_string(index=False))\n",
    "\n",
    "    # Heatmap de top features\n",
    "    N_TOP_HEAT = 15\n",
    "    top_feat_idx = top_idx[:N_TOP_HEAT]\n",
    "    interaction_sub = interaction_matrix[np.ix_(top_feat_idx, top_feat_idx)]\n",
    "    top_feat_names = [feature_cols[i] for i in top_feat_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(interaction_sub, cmap=\"YlOrRd\")\n",
    "    plt.xticks(range(N_TOP_HEAT), top_feat_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(N_TOP_HEAT), top_feat_names)\n",
    "    plt.colorbar(label=\"Mean |SHAP interaction|\")\n",
    "    plt.title(\"SHAP Interaction Matrix (Top 15 features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"shap_interaction_heatmap.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    interaction_df.to_csv(\n",
    "        os.path.join(OUTPUT_DIR, \"shap_interactions.csv\"), index=False\n",
    "    )\n",
    "    print(f\"Guardado en {OUTPUT_DIR}/shap_interactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Analisis de estabilidad de rankings (bootstrap)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# NOTA (validez científica):\n",
    "# Los epochs dentro de un sujeto no son i.i.d. Para evaluar estabilidad sin que sujetos con más epochs\n",
    "# dominen el resultado, usamos *cluster bootstrap* por subject_core (muestreo de sujetos con reemplazo).\n",
    "\n",
    "N_BOOTSTRAP = 5  # numero de repeticiones bootstrap\n",
    "BOOTSTRAP_SIZE = min(2000, len(X_sample))\n",
    "MAX_BOOTSTRAP_FEATS = 300\n",
    "\n",
    "# Ajustar bootstrap según número de features (costo computacional)\n",
    "if len(feature_cols) > MAX_BOOTSTRAP_FEATS:\n",
    "    print(\n",
    "        f\"[INFO] Reduciendo N_BOOTSTRAP a 2 porque hay {len(feature_cols)} features (> {MAX_BOOTSTRAP_FEATS})\"\n",
    "    )\n",
    "    N_BOOTSTRAP = 2\n",
    "\n",
    "print(\n",
    "    f\"Análisis de estabilidad: {N_BOOTSTRAP} repeticiones bootstrap (cluster por sujeto), objetivo ~{BOOTSTRAP_SIZE} muestras\"\n",
    ")\n",
    "print(\"(Esto evalúa qué tan robustos son los rankings de features)\")\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "# Preparar agrupación por sujeto si existe\n",
    "use_cluster_bootstrap = \"subject_core\" in df.columns\n",
    "subj_groups: dict[str, list] = {}\n",
    "subjects: list[str] = []\n",
    "subjects_arr = np.array([], dtype=object)\n",
    "n_subjects_boot = 0\n",
    "epochs_per_subject = 0\n",
    "\n",
    "if use_cluster_bootstrap:\n",
    "    subj = df.loc[X_sample.index, \"subject_core\"].astype(str)\n",
    "    subj_groups = {\n",
    "        str(k): list(v) for k, v in subj.groupby(subj).groups.items()\n",
    "    }  # subject -> indices\n",
    "    subjects = list(subj_groups.keys())\n",
    "    subjects_arr = np.array(subjects, dtype=object)\n",
    "    n_subjects = len(subjects)\n",
    "    # Elegimos número de sujetos y epochs por sujeto para aproximar BOOTSTRAP_SIZE\n",
    "    n_subjects_boot = min(n_subjects, max(5, int(np.ceil(BOOTSTRAP_SIZE / 30))))\n",
    "    epochs_per_subject = int(np.ceil(BOOTSTRAP_SIZE / n_subjects_boot))\n",
    "    epochs_per_subject = max(1, min(50, epochs_per_subject))\n",
    "    print(\n",
    "        f\"[INFO] Cluster bootstrap activo: n_sujetos={n_subjects:,}, \"\n",
    "        f\"sujetos/boot={n_subjects_boot}, epochs/sujeto≈{epochs_per_subject}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[WARN] No hay 'subject_core'; se usa bootstrap por epoch (puede sobreestimar estabilidad)\"\n",
    "    )\n",
    "\n",
    "# Almacenar rankings por repetición\n",
    "bootstrap_ranks: dict[str, list[int]] = defaultdict(list)\n",
    "\n",
    "for b in range(N_BOOTSTRAP):\n",
    "    if use_cluster_bootstrap:\n",
    "        # Muestreo de sujetos con reemplazo, luego muestreo de epochs dentro de cada sujeto\n",
    "        sampled_subjects = rng.choice(subjects_arr, size=n_subjects_boot, replace=True)\n",
    "        boot_idx = []\n",
    "        for s in sampled_subjects:\n",
    "            idxs = np.array(subj_groups[str(s)], dtype=object)\n",
    "            # muestreo dentro de sujeto con reemplazo para tamaño estable\n",
    "            sel = rng.choice(\n",
    "                idxs, size=min(epochs_per_subject, len(idxs)), replace=True\n",
    "            )\n",
    "            boot_idx.extend(sel.tolist())\n",
    "        X_boot = X_sample.loc[pd.Index(boot_idx)]\n",
    "    else:\n",
    "        # Fallback: bootstrap clásico por filas (epochs)\n",
    "        idx_boot = rng.choice(len(X_sample), size=BOOTSTRAP_SIZE, replace=True)\n",
    "        X_boot = X_sample.iloc[idx_boot]\n",
    "\n",
    "    # Calcular SHAP\n",
    "    sv_boot = explainer.shap_values(X_boot)\n",
    "\n",
    "    # Mean absolute SHAP (agregado multi-clase)\n",
    "    if isinstance(sv_boot, list):\n",
    "        abs_boot = np.mean([np.abs(sv) for sv in sv_boot], axis=0).mean(axis=0)\n",
    "    elif isinstance(sv_boot, np.ndarray) and sv_boot.ndim == 3:\n",
    "        abs_boot = np.mean(np.abs(sv_boot), axis=2).mean(axis=0)\n",
    "    else:\n",
    "        abs_boot = np.abs(sv_boot).mean(axis=0)\n",
    "\n",
    "    # Rankings (1-indexed, método min para empates)\n",
    "    ranks_boot = np.argsort(np.argsort(-abs_boot)) + 1\n",
    "\n",
    "    for i, fname in enumerate(feature_cols):\n",
    "        bootstrap_ranks[fname].append(int(ranks_boot[i]))\n",
    "\n",
    "    print(f\"  Bootstrap {b+1}/{N_BOOTSTRAP} completado\")\n",
    "\n",
    "# Calcular estabilidad\n",
    "stability_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_cols,\n",
    "        \"rank_mean\": [float(np.mean(bootstrap_ranks[f])) for f in feature_cols],\n",
    "        \"rank_std\": [\n",
    "            float(\n",
    "                np.std(bootstrap_ranks[f], ddof=1)\n",
    "                if len(bootstrap_ranks[f]) > 1\n",
    "                else 0.0\n",
    "            )\n",
    "            for f in feature_cols\n",
    "        ],\n",
    "        \"rank_min\": [int(np.min(bootstrap_ranks[f])) for f in feature_cols],\n",
    "        \"rank_max\": [int(np.max(bootstrap_ranks[f])) for f in feature_cols],\n",
    "    }\n",
    ")\n",
    "stability_df[\"rank_range\"] = stability_df[\"rank_max\"] - stability_df[\"rank_min\"]\n",
    "\n",
    "# Coeficiente de variación del ranking (normalizado)\n",
    "stability_df[\"rank_cv\"] = stability_df[\"rank_std\"] / (stability_df[\"rank_mean\"] + 1e-10)\n",
    "\n",
    "stability_df = stability_df.sort_values(\"rank_mean\")\n",
    "\n",
    "print(\"\\nEstabilidad de rankings (Top 20 features):\")\n",
    "print(stability_df.head(20).to_string(index=False))\n",
    "\n",
    "# Identificar features con rankings inestables\n",
    "# Umbral: std > 5 posiciones O cv > 0.3 para features en top 50\n",
    "top50_mask = stability_df[\"rank_mean\"] <= 50\n",
    "unstable = stability_df[top50_mask & (stability_df[\"rank_std\"] > 5)]\n",
    "\n",
    "if len(unstable) > 0:\n",
    "    print(\n",
    "        f\"\\n[ADVERTENCIA]  ATENCIÓN: {len(unstable)} features en Top 50 con ranking inestable (std > 5):\"\n",
    "    )\n",
    "    print(\n",
    "        unstable[[\"feature\", \"rank_mean\", \"rank_std\", \"rank_range\"]]\n",
    "        .head(10)\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n[OK] Todas las features del Top 50 tienen rankings estables (std ≤ 5)\")\n",
    "\n",
    "# Visualizar estabilidad de top 20\n",
    "top20_stability = stability_df.head(20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(\n",
    "    range(len(top20_stability)),\n",
    "    top20_stability[\"rank_mean\"],\n",
    "    yerr=top20_stability[\"rank_std\"],\n",
    "    fmt=\"o\",\n",
    "    capsize=3,\n",
    "    capthick=1,\n",
    "    color=\"steelblue\",\n",
    "    markersize=6,\n",
    ")\n",
    "plt.xticks(\n",
    "    range(len(top20_stability)),\n",
    "    top20_stability[\"feature\"],\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    ")\n",
    "plt.ylabel(\"Rank (mean ± std)\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.title(\n",
    "    f\"Estabilidad de rankings SHAP ({N_BOOTSTRAP} bootstrap, objetivo n≈{BOOTSTRAP_SIZE})\"\n",
    ")\n",
    "plt.gca().invert_yaxis()  # Rank 1 arriba\n",
    "plt.axhline(y=10, color=\"gray\", linestyle=\"--\", alpha=0.3, label=\"Top 10\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_stability_bootstrap.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "stability_df.to_csv(os.path.join(OUTPUT_DIR, \"shap_stability.csv\"), index=False)\n",
    "print(f\"\\nGuardado en {OUTPUT_DIR}/shap_stability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Resumen del análisis y metadatos\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUMEN DEL ANÁLISIS DE INTERPRETABILIDAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Metadatos\n",
    "print(f\"\\nFecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Modelo: {type(model).__name__}\")\n",
    "print(f\"Dataset: {DATA_PATH}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"N muestras para SHAP: {len(X_sample):,}\")\n",
    "print(f\"N features: {len(feature_cols)}\")\n",
    "print(f\"N clases: {n_classes}\")\n",
    "\n",
    "# Versiones (para reproducibilidad)\n",
    "try:\n",
    "    import sklearn\n",
    "\n",
    "    sklearn_ver = sklearn.__version__\n",
    "except Exception:\n",
    "    sklearn_ver = \"?\"\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "\n",
    "    xgb_ver = xgboost.__version__\n",
    "except Exception:\n",
    "    xgb_ver = \"N/A\"\n",
    "\n",
    "print(\"\\n--- VERSIONES ---\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"shap: {shap.__version__}\")\n",
    "print(f\"sklearn: {sklearn_ver}\")\n",
    "print(f\"xgboost: {xgb_ver}\")\n",
    "\n",
    "# Clases\n",
    "print(\"\\n--- CLASES (orden) ---\")\n",
    "print(\", \".join([f\"{_fmt_stage_label(c)} ({c})\" for c in class_values]))\n",
    "\n",
    "# Top features global\n",
    "print(\"\\n--- TOP 10 FEATURES (SHAP global agregado) ---\")\n",
    "for rank, idx in enumerate(top_idx[:10], 1):\n",
    "    print(f\"  {rank:2d}. {feature_cols[idx]} (importance: {mean_abs[idx]:.4f})\")\n",
    "\n",
    "# Features más importantes por clase\n",
    "print(\"\\n--- FEATURES CLAVE POR CLASE (Top 3) ---\")\n",
    "for class_index in range(n_classes):\n",
    "    class_value = (\n",
    "        class_values[class_index] if class_index < len(class_values) else class_index\n",
    "    )\n",
    "    class_label = _fmt_stage_label(class_value)\n",
    "\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_index]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_index]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    mean_abs_class = np.abs(sv_class).mean(axis=0)\n",
    "    top3_idx = np.argsort(mean_abs_class)[::-1][:3]\n",
    "    top3_features = [feature_cols[i] for i in top3_idx]\n",
    "    print(f\"  {class_label}: {', '.join(top3_features)}\")\n",
    "\n",
    "# Archivos generados\n",
    "print(\"\\n--- ARCHIVOS GENERADOS ---\")\n",
    "output_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"*\")))\n",
    "for f in output_files:\n",
    "    print(f\"  {os.path.basename(f)}\")\n",
    "\n",
    "# Limitaciones - ver celda markdown siguiente\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Análisis completado. Revisa los plots y CSVs en:\", OUTPUT_DIR)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Limitaciones y Consideraciones\n",
    "\n",
    "1. **Validez del test set**: Los valores SHAP se calcularon sobre el holdout test set (20% de sujetos), garantizando que el modelo nunca vio estos datos durante el entrenamiento.\n",
    "\n",
    "2. **Dependencia intra-sujeto (epochs no i.i.d.)**: Los epochs dentro de un mismo sujeto están correlacionados. Para mejorar la validez del ranking global:\n",
    "   - La importancia global SHAP se agrega **por sujeto** (promedio por sujeto y luego promedio entre sujetos).\n",
    "   - La estabilidad por bootstrap se calcula con **cluster bootstrap por sujeto** (muestreo de sujetos con reemplazo).\n",
    "\n",
    "3. **Transferibilidad a LOSO**: Los hiperparametros de este modelo (xgb_opt_bayes) son identicos a los usados en la validacion LOSO. Por lo tanto, las importancias de features identificadas aqui son representativas del comportamiento del modelo LOSO (misma familia/hiperparámetros), pero el set de entrenamiento final difiere.\n",
    "\n",
    "4. **Importancia nativa (Gini/Gain) vs SHAP**: Pueden diferir porque:\n",
    "   - SHAP considera interacciones entre features\n",
    "   - Gini/Gain puede sobre-representar features con alta cardinalidad\n",
    "\n",
    "5. **Permutation importance**: Puede subestimar features correlacionadas (cuando se permuta una, otra puede compensar). Aquí se usa una submuestra **balanceada por sujeto** para mitigar sesgos por cantidad de epochs.\n",
    "\n",
    "6. **Granularidad por sujeto**: El split respeta sujetos completos - todos los epochs de un sujeto estan en train O en test, nunca mezclados. Esto evita data leakage temporal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
