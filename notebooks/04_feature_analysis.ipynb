{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidad de modelos ML (SHAP)\n",
    "\n",
    "Notebook para analizar importancias y explicaciones locales de modelos tradicionales (RandomForest / XGBoost) entrenados con features PSG.\n",
    "\n",
    "> Ajusta las rutas de modelo y dataset según tus artefactos guardados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Configuracion de paths\n",
    "DATA_PATH = \"../data/processed/features_resamp200.parquet\"\n",
    "\n",
    "# Modelos disponibles (cambiar MODEL_NAME para analizar otro modelo)\n",
    "MODELS = {\n",
    "    \"xgboost_loso\": \"../models/xgb_loso_best/xgboost_model.pkl\",\n",
    "    \"random_forest_bayes\": \"../models/rf_opt_bayes_best/random_forest_model.pkl\",\n",
    "}\n",
    "MODEL_NAME = \"xgboost_loso\"  # Cambiar a 'random_forest_bayes' para RF\n",
    "MODEL_PATH = MODELS[MODEL_NAME]\n",
    "OUTPUT_DIR = f\"../reports/shap_{MODEL_NAME}\"\n",
    "\n",
    "# Mapeo de etiquetas de sleep staging\n",
    "STAGE_LABELS = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"REM\"}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Data: {DATA_PATH}\\nModelo: {MODEL_NAME} -> {MODEL_PATH}\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cargar datos y modelo\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Cargar features (usa columnas de features, evita metadata no numerica)\n",
    "df = (\n",
    "    pd.read_parquet(DATA_PATH)\n",
    "    if DATA_PATH.endswith(\".parquet\")\n",
    "    else pd.read_feather(DATA_PATH)\n",
    ")\n",
    "\n",
    "# Ajusta si tu columna de etiqueta se llama distinto\n",
    "target_col = \"stage\"\n",
    "meta_cols = [\n",
    "    c\n",
    "    for c in [\n",
    "        target_col,\n",
    "        \"subject_core\",\n",
    "        \"subject_id\",\n",
    "        \"epoch_time_start\",\n",
    "        \"epoch_index\",\n",
    "    ]\n",
    "    if c in df.columns\n",
    "]\n",
    "feature_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col] if target_col in df else None\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Features: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
    "if y is not None:\n",
    "    class_counts = y.value_counts().sort_index()\n",
    "    print(f\"\\nDistribución de clases ({y.nunique()} clases):\")\n",
    "    for cls, count in class_counts.items():\n",
    "        label = STAGE_LABELS.get(cls, str(cls))\n",
    "        print(f\"  {label} ({cls}): {count:,} ({100*count/len(y):.1f}%)\")\n",
    "\n",
    "# Cargar modelo\n",
    "model = joblib.load(MODEL_PATH)\n",
    "print(f\"\\nModelo: {type(model).__name__}\")\n",
    "\n",
    "# Mostrar importancia nativa si está disponible\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    imp_native = pd.DataFrame(\n",
    "        {\"feature\": feature_cols, \"importance\": model.feature_importances_}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "    print(\"\\nTop 10 features (importancia nativa):\")\n",
    "    print(imp_native.head(10).to_string(index=False))\n",
    "\n",
    "# Opcional: submuestreo para acelerar SHAP si el dataset es grande\n",
    "MAX_SAMPLES = 8000\n",
    "if len(X) > MAX_SAMPLES:\n",
    "    X_sample = X.sample(MAX_SAMPLES, random_state=42)\n",
    "    y_sample = y.loc[X_sample.index] if y is not None else None\n",
    "    print(f\"\\nUsando submuestra de {len(X_sample):,} para SHAP\")\n",
    "else:\n",
    "    X_sample = X\n",
    "    y_sample = y\n",
    "    print(f\"\\nUsando dataset completo ({len(X_sample):,} muestras) para SHAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: explicaciones globales y locales\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# TreeExplainer es eficiente para RF / XGBoost; usar probas para interpretabilidad\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(model, model_output=\"probability\")\n",
    "    print(\"Explainer: TreeExplainer con output=probability\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"TreeExplainer con probability falló ({e}); usando TreeExplainer por defecto\"\n",
    "    )\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Detectar formato de shap_values (lista para multi-clase en SHAP antiguo, o array 3D en SHAP nuevo)\n",
    "is_multiclass_list = isinstance(shap_values, list)\n",
    "is_multiclass_3d = isinstance(shap_values, np.ndarray) and shap_values.ndim == 3\n",
    "\n",
    "n_classes = (\n",
    "    len(shap_values)\n",
    "    if is_multiclass_list\n",
    "    else (shap_values.shape[2] if is_multiclass_3d else 1)\n",
    ")\n",
    "print(\n",
    "    f\"SHAP values calculados. Multi-clase: {is_multiclass_list or is_multiclass_3d} ({n_classes} clases)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Clase principal para plots (modo de y_sample si existe; si no, predicción más probable)\n",
    "def _choose_main_class():\n",
    "    if y_sample is not None and len(y_sample) > 0:\n",
    "        return int(y_sample.mode().iloc[0])\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return int(np.argmax(model.predict_proba(X_sample.iloc[[0]])[0]))\n",
    "    return 0\n",
    "\n",
    "\n",
    "main_class = _choose_main_class()\n",
    "print(\n",
    "    f\"Clase principal para plots: {STAGE_LABELS.get(main_class, main_class)} ({main_class})\"\n",
    ")\n",
    "\n",
    "\n",
    "# Para modelos multi-clase, calculamos media de |shap| para ranking global\n",
    "def _mean_abs_shap(shap_vals):\n",
    "    if isinstance(shap_vals, list):\n",
    "        return np.mean([np.abs(sv) for sv in shap_vals], axis=0)\n",
    "    elif isinstance(shap_vals, np.ndarray) and shap_vals.ndim == 3:\n",
    "        return np.mean(np.abs(shap_vals), axis=2)\n",
    "    return np.abs(shap_vals)\n",
    "\n",
    "\n",
    "shap_abs = _mean_abs_shap(shap_values)\n",
    "mean_abs = shap_abs.mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[::-1]\n",
    "\n",
    "# Mostrar ranking de features\n",
    "print(\"\\nTop 15 features por SHAP (|mean|):\")\n",
    "for rank, idx in enumerate(top_idx[:15], 1):\n",
    "    print(f\"  {rank:2d}. {feature_cols[idx]}: {mean_abs[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: Plots globales (bar + beeswarm)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Summary bar (importancias globales)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title(\"SHAP Feature Importances (global)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_importances_bar.png\"), dpi=150, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm - para multi-clase mostramos una clase representativa o usamos valores agregados\n",
    "plt.figure(figsize=(10, 8))\n",
    "if \"main_class\" not in locals():\n",
    "    main_class = _choose_main_class() if \"_choose_main_class\" in locals() else 0\n",
    "\n",
    "if is_multiclass_list:\n",
    "    shap.summary_plot(shap_values[main_class], X_sample, show=False, max_display=20)\n",
    "    plt.title(f\"SHAP Beeswarm - Clase {STAGE_LABELS.get(main_class, main_class)}\")\n",
    "elif is_multiclass_3d:\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, :, main_class], X_sample, show=False, max_display=20\n",
    "    )\n",
    "    plt.title(f\"SHAP Beeswarm - Clase {STAGE_LABELS.get(main_class, main_class)}\")\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_sample, show=False, max_display=20)\n",
    "    plt.title(\"SHAP Beeswarm\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"shap_beeswarm.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plots guardados en {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP por clase: análisis específico para cada etapa del sueño\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Crear beeswarm para cada clase (importante para entender qué features discriminan cada etapa)\n",
    "n_classes_to_plot = min(n_classes, 5)  # limitar si hay muchas clases\n",
    "\n",
    "fig, axes = plt.subplots(1, n_classes_to_plot, figsize=(5 * n_classes_to_plot, 6))\n",
    "if n_classes_to_plot == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for class_i in range(n_classes_to_plot):\n",
    "    plt.sca(axes[class_i])\n",
    "    class_label = STAGE_LABELS.get(class_i, str(class_i))\n",
    "\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_i]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_i]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    shap.summary_plot(sv_class, X_sample, show=False, max_display=10, plot_size=None)\n",
    "    axes[class_i].set_title(f\"Clase: {class_label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_beeswarm_by_class.png\"), dpi=150, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Tabla: Top 5 features por clase\n",
    "print(\"\\nTop 5 features más importantes por clase:\")\n",
    "print(\"-\" * 60)\n",
    "for class_i in range(n_classes):\n",
    "    class_label = STAGE_LABELS.get(class_i, str(class_i))\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_i]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_i]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    mean_abs_class = np.abs(sv_class).mean(axis=0)\n",
    "    top5_idx = np.argsort(mean_abs_class)[::-1][:5]\n",
    "    top5_features = [feature_cols[i] for i in top5_idx]\n",
    "    print(f\"{class_label}: {', '.join(top5_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP: Dependence plots para top features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "N_TOP = 5\n",
    "\n",
    "# Preparar shap_values para dependence plot (necesita 2D para una clase)\n",
    "if \"main_class\" not in locals():\n",
    "    main_class = _choose_main_class() if \"_choose_main_class\" in locals() else 0\n",
    "\n",
    "if is_multiclass_list:\n",
    "    shap_for_dep = shap_values[main_class]\n",
    "elif is_multiclass_3d:\n",
    "    shap_for_dep = shap_values[:, :, main_class]\n",
    "else:\n",
    "    shap_for_dep = shap_values\n",
    "\n",
    "fig, axes = plt.subplots(1, N_TOP, figsize=(4 * N_TOP, 4))\n",
    "for i, idx in enumerate(top_idx[:N_TOP]):\n",
    "    fname = feature_cols[idx]\n",
    "    plt.sca(axes[i])\n",
    "    shap.dependence_plot(fname, shap_for_dep, X_sample, show=False, ax=axes[i])\n",
    "    axes[i].set_title(fname)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_dependence_top5.png\"), dpi=150, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dependence plots para top {N_TOP} features guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP local: explica casos individuales\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Elige un indice para inspeccionar (ej.: primer ejemplo del test o un FN)\n",
    "sample_idx = X_sample.index[0]\n",
    "shap_value_single = explainer.shap_values(X_sample.loc[[sample_idx]])\n",
    "\n",
    "# Info del ejemplo\n",
    "true_label = y_sample.loc[sample_idx] if y_sample is not None else \"?\"\n",
    "pred_label = model.predict(X_sample.loc[[sample_idx]])[0]\n",
    "pred_proba = (\n",
    "    model.predict_proba(X_sample.loc[[sample_idx]])[0]\n",
    "    if hasattr(model, \"predict_proba\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "print(f\"Ejemplo idx={sample_idx}\")\n",
    "print(f\"  Etiqueta real: {STAGE_LABELS.get(true_label, true_label)}\")\n",
    "print(f\"  Predicción:    {STAGE_LABELS.get(pred_label, pred_label)}\")\n",
    "if pred_proba is not None:\n",
    "    print(\n",
    "        f\"  Probabilidades: {dict(zip([STAGE_LABELS.get(i, i) for i in range(len(pred_proba))], pred_proba.round(3)))}\"\n",
    "    )\n",
    "\n",
    "# Para multi-clase, escoge la clase predicha\n",
    "if isinstance(shap_value_single, list):\n",
    "    class_idx = int(pred_label)\n",
    "    values = shap_value_single[class_idx][0]\n",
    "    base_value = explainer.expected_value[class_idx]\n",
    "elif isinstance(shap_value_single, np.ndarray) and shap_value_single.ndim == 3:\n",
    "    class_idx = int(pred_label)\n",
    "    values = shap_value_single[0, :, class_idx]\n",
    "    base_value = explainer.expected_value[class_idx]\n",
    "else:\n",
    "    values = shap_value_single[0]\n",
    "    base_value = explainer.expected_value\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=values,\n",
    "        base_values=base_value,\n",
    "        data=X_sample.loc[sample_idx].values,\n",
    "        feature_names=feature_cols,\n",
    "    ),\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"SHAP Waterfall - Predicción: {STAGE_LABELS.get(pred_label, pred_label)}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, f\"shap_waterfall_idx{sample_idx}.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGuardado waterfall para idx={sample_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Análisis de errores: explicar casos mal clasificados\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if y_sample is not None:\n",
    "    # Predicciones en la muestra\n",
    "    y_pred = model.predict(X_sample)\n",
    "    errors_mask = y_sample.values != y_pred\n",
    "    n_errors = errors_mask.sum()\n",
    "    accuracy = 1 - n_errors / len(y_sample)\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy en muestra: {accuracy:.3f} ({n_errors} errores de {len(y_sample)})\"\n",
    "    )\n",
    "\n",
    "    if n_errors > 0:\n",
    "        # Matriz de confusión de errores\n",
    "        from collections import Counter\n",
    "\n",
    "        error_pairs = [\n",
    "            (y_sample.iloc[i], y_pred[i])\n",
    "            for i in range(len(y_sample))\n",
    "            if errors_mask[i]\n",
    "        ]\n",
    "        error_counts = Counter(error_pairs)\n",
    "\n",
    "        print(\"\\nErrores más frecuentes (real -> predicho):\")\n",
    "        for (true_cls, pred_cls), count in error_counts.most_common(10):\n",
    "            true_label = STAGE_LABELS.get(true_cls, str(true_cls))\n",
    "            pred_label = STAGE_LABELS.get(pred_cls, str(pred_cls))\n",
    "            print(f\"  {true_label} -> {pred_label}: {count}\")\n",
    "\n",
    "        # Analizar SHAP de ejemplos mal clasificados (muestra de errores)\n",
    "        error_indices = X_sample.index[errors_mask]\n",
    "        n_error_examples = min(3, len(error_indices))\n",
    "\n",
    "        print(\n",
    "            f\"\\n--- Análisis SHAP de {n_error_examples} ejemplos mal clasificados ---\"\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, n_error_examples, figsize=(6 * n_error_examples, 5))\n",
    "        if n_error_examples == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, err_idx in enumerate(error_indices[:n_error_examples]):\n",
    "            true_cls = y_sample.loc[err_idx]\n",
    "            pred_cls = y_pred[X_sample.index.get_loc(err_idx)]\n",
    "\n",
    "            true_label = STAGE_LABELS.get(true_cls, str(true_cls))\n",
    "            pred_label = STAGE_LABELS.get(pred_cls, str(pred_cls))\n",
    "\n",
    "            # Obtener SHAP values para la clase predicha (explica por qué predijo eso)\n",
    "            if is_multiclass_list:\n",
    "                sv_err = shap_values[int(pred_cls)][X_sample.index.get_loc(err_idx)]\n",
    "            elif is_multiclass_3d:\n",
    "                sv_err = shap_values[X_sample.index.get_loc(err_idx), :, int(pred_cls)]\n",
    "            else:\n",
    "                sv_err = shap_values[X_sample.index.get_loc(err_idx)]\n",
    "\n",
    "            # Top features que causaron el error\n",
    "            top_err_idx = np.argsort(np.abs(sv_err))[::-1][:10]\n",
    "\n",
    "            plt.sca(axes[i])\n",
    "            colors = [\"red\" if v > 0 else \"blue\" for v in sv_err[top_err_idx]]\n",
    "            plt.barh(range(len(top_err_idx)), sv_err[top_err_idx], color=colors)\n",
    "            plt.yticks(range(len(top_err_idx)), [feature_cols[j] for j in top_err_idx])\n",
    "            plt.xlabel(\"SHAP value\")\n",
    "            plt.title(f\"Error: {true_label} -> {pred_label}\")\n",
    "            plt.gca().invert_yaxis()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(OUTPUT_DIR, \"shap_error_analysis.png\"),\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Rojo = empuja hacia predicción errónea, Azul = empuja contra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Permutation importance (model-agnostic)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if y_sample is not None:\n",
    "    print(\"Calculando permutation importance (puede tardar)...\")\n",
    "    pi = permutation_importance(\n",
    "        model,\n",
    "        X_sample,\n",
    "        y_sample,\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "    pi_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_cols,\n",
    "            \"importance_mean\": pi.importances_mean,\n",
    "            \"importance_std\": pi.importances_std,\n",
    "        }\n",
    "    ).sort_values(\"importance_mean\", ascending=False)\n",
    "    pi_df.to_csv(os.path.join(OUTPUT_DIR, \"permutation_importance.csv\"), index=False)\n",
    "\n",
    "    # Visualizar top 20\n",
    "    top_pi = pi_df.head(20)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(\n",
    "        range(len(top_pi)),\n",
    "        top_pi[\"importance_mean\"],\n",
    "        xerr=top_pi[\"importance_std\"],\n",
    "        color=\"steelblue\",\n",
    "    )\n",
    "    plt.yticks(range(len(top_pi)), top_pi[\"feature\"])\n",
    "    plt.xlabel(\"Decrease in accuracy\")\n",
    "    plt.title(\"Permutation Feature Importance (Top 20)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"permutation_importance.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 features (permutation importance):\")\n",
    "    print(pi_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No se encontró columna de etiqueta; se omite permutation importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Comparación y correlación entre métodos de importancia\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Crear DataFrame comparativo\n",
    "comparison = pd.DataFrame({\"feature\": feature_cols})\n",
    "\n",
    "# SHAP importance\n",
    "comparison[\"shap_importance\"] = mean_abs\n",
    "comparison[\"shap_rank\"] = (\n",
    "    comparison[\"shap_importance\"].rank(ascending=False).astype(int)\n",
    ")\n",
    "\n",
    "# Native importance (si disponible)\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    comparison[\"native_importance\"] = model.feature_importances_\n",
    "    comparison[\"native_rank\"] = (\n",
    "        comparison[\"native_importance\"].rank(ascending=False).astype(int)\n",
    "    )\n",
    "\n",
    "# Permutation importance (si calculado)\n",
    "if y_sample is not None and \"pi\" in dir():\n",
    "    comparison[\"perm_importance\"] = pi.importances_mean\n",
    "    comparison[\"perm_rank\"] = (\n",
    "        comparison[\"perm_importance\"].rank(ascending=False).astype(int)\n",
    "    )\n",
    "\n",
    "# Ordenar por SHAP\n",
    "comparison = comparison.sort_values(\"shap_rank\")\n",
    "\n",
    "# Mostrar top 15\n",
    "print(\"Comparación de rankings de importancia (Top 15):\")\n",
    "rank_cols = [c for c in comparison.columns if c.endswith(\"_rank\")]\n",
    "display_cols = [\"feature\"] + rank_cols\n",
    "print(comparison[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "# --- Correlaciones entre métodos ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORRELACIÓN ENTRE MÉTODOS DE IMPORTANCIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "importance_cols = [c for c in comparison.columns if c.endswith(\"_importance\")]\n",
    "if len(importance_cols) >= 2:\n",
    "    corr_results = []\n",
    "    for i, col1 in enumerate(importance_cols):\n",
    "        for col2 in importance_cols[i + 1 :]:\n",
    "            spearman_corr, spearman_p = spearmanr(comparison[col1], comparison[col2])\n",
    "            kendall_corr, kendall_p = kendalltau(comparison[col1], comparison[col2])\n",
    "            corr_results.append(\n",
    "                {\n",
    "                    \"Método 1\": col1.replace(\"_importance\", \"\"),\n",
    "                    \"Método 2\": col2.replace(\"_importance\", \"\"),\n",
    "                    \"Spearman ρ\": f\"{spearman_corr:.3f}\",\n",
    "                    \"p-value\": f\"{spearman_p:.2e}\",\n",
    "                    \"Kendall τ\": f\"{kendall_corr:.3f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_results)\n",
    "    print(\"\\nCorrelaciones de ranking:\")\n",
    "    print(corr_df.to_string(index=False))\n",
    "\n",
    "    # Interpretación\n",
    "    print(\"\\nInterpretación:\")\n",
    "    print(\"  ρ/τ > 0.7: Alta concordancia entre métodos (robusto)\")\n",
    "    print(\"  ρ/τ 0.4-0.7: Concordancia moderada\")\n",
    "    print(\"  ρ/τ < 0.4: Baja concordancia (investigar diferencias)\")\n",
    "\n",
    "# Scatter plot de importancias\n",
    "if len(importance_cols) >= 2:\n",
    "    n_pairs = len(importance_cols) * (len(importance_cols) - 1) // 2\n",
    "    fig, axes = plt.subplots(1, n_pairs, figsize=(6 * n_pairs, 5))\n",
    "    if n_pairs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    pair_idx = 0\n",
    "    for i, col1 in enumerate(importance_cols):\n",
    "        for col2 in importance_cols[i + 1 :]:\n",
    "            ax = axes[pair_idx]\n",
    "            ax.scatter(comparison[col1], comparison[col2], alpha=0.5, s=20)\n",
    "            ax.set_xlabel(col1.replace(\"_importance\", \"\").upper())\n",
    "            ax.set_ylabel(col2.replace(\"_importance\", \"\").upper())\n",
    "\n",
    "            # Línea de tendencia\n",
    "            z = np.polyfit(comparison[col1], comparison[col2], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(comparison[col1].min(), comparison[col1].max(), 100)\n",
    "            ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, label=\"Tendencia\")\n",
    "\n",
    "            # Correlación en título\n",
    "            r, _ = spearmanr(comparison[col1], comparison[col2])\n",
    "            ax.set_title(f\"Spearman ρ = {r:.3f}\")\n",
    "            pair_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"importance_correlation_scatter.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Guardar comparación completa\n",
    "comparison.to_csv(os.path.join(OUTPUT_DIR, \"importance_comparison.csv\"), index=False)\n",
    "print(f\"\\nComparación completa guardada en {OUTPUT_DIR}/importance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Importancia por grupos de features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Definir grupos de features basados en nombres típicos de PSG\n",
    "# Ajusta estos patrones según tus nombres de features\n",
    "def categorize_feature(fname):\n",
    "    fname_lower = fname.lower()\n",
    "    if any(\n",
    "        band in fname_lower\n",
    "        for band in [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\", \"sigma\"]\n",
    "    ):\n",
    "        return \"EEG_bands\"\n",
    "    elif \"eog\" in fname_lower or \"eye\" in fname_lower:\n",
    "        return \"EOG\"\n",
    "    elif \"emg\" in fname_lower or \"chin\" in fname_lower:\n",
    "        return \"EMG\"\n",
    "    elif any(\n",
    "        stat in fname_lower for stat in [\"mean\", \"std\", \"var\", \"skew\", \"kurt\", \"rms\"]\n",
    "    ):\n",
    "        return \"Statistical\"\n",
    "    elif any(temp in fname_lower for temp in [\"hjorth\", \"mobility\", \"complexity\"]):\n",
    "        return \"Hjorth\"\n",
    "    elif any(ent in fname_lower for ent in [\"entropy\", \"perm\", \"sample\", \"approx\"]):\n",
    "        return \"Entropy\"\n",
    "    elif any(\n",
    "        conn in fname_lower for conn in [\"coherence\", \"correlation\", \"plv\", \"pli\"]\n",
    "    ):\n",
    "        return \"Connectivity\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "# Asignar categorías\n",
    "feature_groups = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_cols,\n",
    "        \"group\": [categorize_feature(f) for f in feature_cols],\n",
    "        \"shap_importance\": mean_abs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Importancia agregada por grupo\n",
    "group_importance = feature_groups.groupby(\"group\")[\"shap_importance\"].agg(\n",
    "    [\"sum\", \"mean\", \"count\"]\n",
    ")\n",
    "group_importance = group_importance.sort_values(\"sum\", ascending=False)\n",
    "\n",
    "print(\"Importancia SHAP por grupo de features:\")\n",
    "print(group_importance.round(4).to_string())\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Importancia total por grupo\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(group_importance)))\n",
    "ax1.barh(range(len(group_importance)), group_importance[\"sum\"], color=colors)\n",
    "ax1.set_yticks(range(len(group_importance)))\n",
    "ax1.set_yticklabels(group_importance.index)\n",
    "ax1.set_xlabel(\"SHAP importance (sum)\")\n",
    "ax1.set_title(\"Importancia total por grupo\")\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Importancia media por grupo\n",
    "ax2 = axes[1]\n",
    "ax2.barh(range(len(group_importance)), group_importance[\"mean\"], color=colors)\n",
    "ax2.set_yticks(range(len(group_importance)))\n",
    "ax2.set_yticklabels(group_importance.index)\n",
    "ax2.set_xlabel(\"SHAP importance (mean)\")\n",
    "ax2.set_title(\"Importancia media por grupo\")\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_by_feature_group.png\"), dpi=150, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Distribución de grupos\n",
    "print(\"\\nDistribución de features por grupo:\")\n",
    "print(feature_groups[\"group\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis avanzado\n",
    "\n",
    "Las siguientes celdas son computacionalmente mas costosas (interacciones entre features y analisis de estabilidad bootstrap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SHAP Interaction Values\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "MAX_FEATS_INTERACTION = 200\n",
    "if len(feature_cols) > MAX_FEATS_INTERACTION:\n",
    "    print(\n",
    "        f\"Saltando SHAP interaction values: {len(feature_cols)} features (> {MAX_FEATS_INTERACTION}). Ajusta MAX_FEATS_INTERACTION si realmente lo necesitas.\"\n",
    "    )\n",
    "else:\n",
    "    # Usar submuestra mas pequena para interactions\n",
    "    N_INTERACTION = min(500, len(X_sample))\n",
    "    X_interaction = X_sample.sample(N_INTERACTION, random_state=42)\n",
    "\n",
    "    print(f\"Calculando SHAP interaction values para {N_INTERACTION} muestras...\")\n",
    "    print(\"(Esto puede tardar varios minutos)\")\n",
    "\n",
    "    if \"main_class\" not in locals():\n",
    "        main_class = _choose_main_class() if \"_choose_main_class\" in locals() else 0\n",
    "\n",
    "    shap_interaction = explainer.shap_interaction_values(X_interaction)\n",
    "\n",
    "    # Para multi-clase, tomar una clase\n",
    "    if isinstance(shap_interaction, list):\n",
    "        shap_inter_class = shap_interaction[main_class]\n",
    "    elif shap_interaction.ndim == 4:\n",
    "        shap_inter_class = shap_interaction[:, :, :, main_class]\n",
    "    else:\n",
    "        shap_inter_class = shap_interaction\n",
    "\n",
    "    # Matriz de interacciones promedio\n",
    "    interaction_matrix = np.abs(shap_inter_class).mean(axis=0)\n",
    "\n",
    "    # Encontrar top interacciones (excluyendo diagonal = main effects)\n",
    "    n_feat = len(feature_cols)\n",
    "    interaction_pairs = []\n",
    "    for i in range(n_feat):\n",
    "        for j in range(i + 1, n_feat):\n",
    "            interaction_pairs.append(\n",
    "                {\n",
    "                    \"feature_1\": feature_cols[i],\n",
    "                    \"feature_2\": feature_cols[j],\n",
    "                    \"interaction_strength\": interaction_matrix[i, j],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    interaction_df = pd.DataFrame(interaction_pairs).sort_values(\n",
    "        \"interaction_strength\", ascending=False\n",
    "    )\n",
    "    print(\"\\nTop 10 interacciones entre features:\")\n",
    "    print(interaction_df.head(10).to_string(index=False))\n",
    "\n",
    "    # Heatmap de top features\n",
    "    N_TOP_HEAT = 15\n",
    "    top_feat_idx = top_idx[:N_TOP_HEAT]\n",
    "    interaction_sub = interaction_matrix[np.ix_(top_feat_idx, top_feat_idx)]\n",
    "    top_feat_names = [feature_cols[i] for i in top_feat_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(interaction_sub, cmap=\"YlOrRd\")\n",
    "    plt.xticks(range(N_TOP_HEAT), top_feat_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(N_TOP_HEAT), top_feat_names)\n",
    "    plt.colorbar(label=\"Mean |SHAP interaction|\")\n",
    "    plt.title(\"SHAP Interaction Matrix (Top 15 features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, \"shap_interaction_heatmap.png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    interaction_df.to_csv(\n",
    "        os.path.join(OUTPUT_DIR, \"shap_interactions.csv\"), index=False\n",
    "    )\n",
    "    print(f\"Guardado en {OUTPUT_DIR}/shap_interactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Analisis de estabilidad de rankings (bootstrap)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "N_BOOTSTRAP = 5  # numero de repeticiones bootstrap\n",
    "BOOTSTRAP_SIZE = min(2000, len(X_sample))\n",
    "MAX_BOOTSTRAP_FEATS = 300\n",
    "if len(feature_cols) > MAX_BOOTSTRAP_FEATS:\n",
    "    print(\n",
    "        f\"Reduciendo N_BOOTSTRAP a 2 porque hay {len(feature_cols)} features (> {MAX_BOOTSTRAP_FEATS})\"\n",
    "    )\n",
    "    N_BOOTSTRAP = 2\n",
    "\n",
    "print(\n",
    "    f\"Analisis de estabilidad: {N_BOOTSTRAP} repeticiones bootstrap de {BOOTSTRAP_SIZE} muestras\"\n",
    ")\n",
    "\n",
    "# Almacenar rankings por repeticion\n",
    "bootstrap_ranks = defaultdict(list)\n",
    "\n",
    "for b in range(N_BOOTSTRAP):\n",
    "    # Muestra bootstrap\n",
    "    idx_boot = np.random.choice(len(X_sample), size=BOOTSTRAP_SIZE, replace=True)\n",
    "    X_boot = X_sample.iloc[idx_boot]\n",
    "\n",
    "    # Calcular SHAP\n",
    "    sv_boot = explainer.shap_values(X_boot)\n",
    "\n",
    "    # Mean absolute SHAP\n",
    "    if isinstance(sv_boot, list):\n",
    "        abs_boot = np.mean([np.abs(sv) for sv in sv_boot], axis=0).mean(axis=0)\n",
    "    elif sv_boot.ndim == 3:\n",
    "        abs_boot = np.mean(np.abs(sv_boot), axis=2).mean(axis=0)\n",
    "    else:\n",
    "        abs_boot = np.abs(sv_boot).mean(axis=0)\n",
    "\n",
    "    # Rankings\n",
    "    ranks_boot = np.argsort(np.argsort(-abs_boot)) + 1  # 1-indexed ranks\n",
    "\n",
    "    for i, fname in enumerate(feature_cols):\n",
    "        bootstrap_ranks[fname].append(ranks_boot[i])\n",
    "\n",
    "    print(f\"  Bootstrap {b+1}/{N_BOOTSTRAP} completado\")\n",
    "\n",
    "# Calcular estabilidad\n",
    "stability_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_cols,\n",
    "        \"rank_mean\": [np.mean(bootstrap_ranks[f]) for f in feature_cols],\n",
    "        \"rank_std\": [np.std(bootstrap_ranks[f]) for f in feature_cols],\n",
    "        \"rank_min\": [np.min(bootstrap_ranks[f]) for f in feature_cols],\n",
    "        \"rank_max\": [np.max(bootstrap_ranks[f]) for f in feature_cols],\n",
    "    }\n",
    ")\n",
    "stability_df[\"rank_range\"] = stability_df[\"rank_max\"] - stability_df[\"rank_min\"]\n",
    "stability_df = stability_df.sort_values(\"rank_mean\")\n",
    "\n",
    "print(\"\\nEstabilidad de rankings (Top 20 features):\")\n",
    "print(stability_df.head(20).to_string(index=False))\n",
    "\n",
    "# Identificar features con rankings inestables\n",
    "unstable = stability_df[stability_df[\"rank_std\"] > 5].head(10)\n",
    "if len(unstable) > 0:\n",
    "    print(\"\\nATENCION: Features con ranking inestable (std > 5):\")\n",
    "    print(\n",
    "        unstable[[\"feature\", \"rank_mean\", \"rank_std\", \"rank_range\"]].to_string(\n",
    "            index=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Visualizar estabilidad de top 20\n",
    "top20_stability = stability_df.head(20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(\n",
    "    range(len(top20_stability)),\n",
    "    top20_stability[\"rank_mean\"],\n",
    "    yerr=top20_stability[\"rank_std\"],\n",
    "    fmt=\"o\",\n",
    "    capsize=3,\n",
    "    capthick=1,\n",
    ")\n",
    "plt.xticks(\n",
    "    range(len(top20_stability)), top20_stability[\"feature\"], rotation=45, ha=\"right\"\n",
    ")\n",
    "plt.ylabel(\"Rank (mean +/- std)\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.title(f\"Estabilidad de rankings SHAP ({N_BOOTSTRAP} bootstrap)\")\n",
    "plt.gca().invert_yaxis()  # Rank 1 arriba\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(OUTPUT_DIR, \"shap_stability_bootstrap.png\"),\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "stability_df.to_csv(os.path.join(OUTPUT_DIR, \"shap_stability.csv\"), index=False)\n",
    "print(f\"\\nGuardado en {OUTPUT_DIR}/shap_stability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Resumen del análisis y metadatos\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUMEN DEL ANÁLISIS DE INTERPRETABILIDAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Metadatos\n",
    "print(f\"\\nFecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Modelo: {type(model).__name__}\")\n",
    "print(f\"Dataset: {DATA_PATH}\")\n",
    "print(f\"N muestras para SHAP: {len(X_sample):,}\")\n",
    "print(f\"N features: {len(feature_cols)}\")\n",
    "print(f\"N clases: {n_classes}\")\n",
    "\n",
    "# Top features global\n",
    "print(\"\\n--- TOP 10 FEATURES (SHAP global) ---\")\n",
    "for rank, idx in enumerate(top_idx[:10], 1):\n",
    "    print(f\"  {rank:2d}. {feature_cols[idx]}\")\n",
    "\n",
    "# Features más importantes por clase\n",
    "print(\"\\n--- FEATURES CLAVE POR ETAPA DE SUEÑO ---\")\n",
    "for class_i in range(n_classes):\n",
    "    class_label = STAGE_LABELS.get(class_i, str(class_i))\n",
    "    if is_multiclass_list:\n",
    "        sv_class = shap_values[class_i]\n",
    "    elif is_multiclass_3d:\n",
    "        sv_class = shap_values[:, :, class_i]\n",
    "    else:\n",
    "        sv_class = shap_values\n",
    "\n",
    "    mean_abs_class = np.abs(sv_class).mean(axis=0)\n",
    "    top3_idx = np.argsort(mean_abs_class)[::-1][:3]\n",
    "    top3_features = [feature_cols[i] for i in top3_idx]\n",
    "    print(f\"  {class_label}: {', '.join(top3_features)}\")\n",
    "\n",
    "# Archivos generados\n",
    "print(\"\\n--- ARCHIVOS GENERADOS ---\")\n",
    "\n",
    "output_files = glob.glob(os.path.join(OUTPUT_DIR, \"*\"))\n",
    "for f in sorted(output_files):\n",
    "    print(f\"  {os.path.basename(f)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Análisis completado. Revisa los plots y CSVs en:\", OUTPUT_DIR)\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
