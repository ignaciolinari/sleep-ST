{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0167e5e",
   "metadata": {},
   "source": [
    "# CNN-LSTM Secuencial para Sleep Staging\n",
    "\n",
    "Este notebook entrena un modelo híbrido **CNN + LSTM** para clasificación de estadios de sueño usando **secuencias de múltiples epochs consecutivos**, capturando dependencias temporales.\n",
    "\n",
    "**Optimizado para Kaggle con 2x Tesla T4 (30GB RAM + 32GB VRAM)**\n",
    "\n",
    "### Arquitectura:\n",
    "- **Nivel 1 (CNN)**: Extrae features de cada epoch de 30s con TimeDistributed Conv1D\n",
    "- **Nivel 2 (BiLSTM)**: Procesa la secuencia de features para capturar contexto temporal\n",
    "- **Predicción Many-to-Many**: Predice el estadio de todos los epochs en la secuencia\n",
    "- **Sample Weights**: Balance de clases minoritarias (N1) por epoch\n",
    "\n",
    "### Modos de ejecución:\n",
    "- **Debug** (`EXECUTION_MODE = \"debug\"`): Carga datos en RAM, subset de sujetos.\n",
    "- **Full** (`EXECUTION_MODE = \"full\"`): **Streaming TFRecord**, todos los sujetos.\n",
    "\n",
    "### Características:\n",
    "- Secuencias de 11 epochs (5.5 min de contexto temporal)\n",
    "- **Train/Val**: stride=2 (overlap alto, ~60k secuencias, ~32GB TFRecord)\n",
    "- **Test**: stride=11 (sin overlap para métricas sin sesgo)\n",
    "- División train/val/test **por SUJETOS** (sin data leakage)\n",
    "- TFRecords cacheados para re-ejecutar sin reprocesar\n",
    "- Semillas fijas para reproducibilidad (SEED=42)\n",
    "- Mixed precision desactivado para mayor estabilidad numérica\n",
    "\n",
    "### Recursos Kaggle:\n",
    "- RAM: 30GB\n",
    "- VRAM: 2×16GB = 32GB\n",
    "- Disco temporal (`/kaggle/tmp`): 50GB → TFRecords (~32GB con stride=2)\n",
    "- Disco output (`/kaggle/working`): 19.5GB \n",
    "\n",
    "### Datos requeridos:\n",
    "- Dataset `sleep-edf-trimmed-f32` en Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN INICIAL - Ejecutar primero\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Detectar si estamos en Kaggle\n",
    "IN_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Entorno: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "# Paths según entorno\n",
    "if IN_KAGGLE:\n",
    "    DATA_PATH = \"/kaggle/input/sleep-edf-trimmed-f32/sleep_trimmed_spt\"\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "else:\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../models\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8729bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR GPUs DISPONIBLES\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPUs disponibles:\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"\\n[OK] {len(gpus)} GPU(s) configuradas con memory growth\")\n",
    "else:\n",
    "    print(\"[WARN] No se detectaron GPUs. El entrenamiento sera lento.\")\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"\\nUsando MirroredStrategy con {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    if len(gpus) == 1:\n",
    "        print(\"\\nUsando estrategia por defecto (1 GPU)\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] Usando CPU - entrenamiento será lento\")\n",
    "\n",
    "print(\"[INFO] Mixed precision se configurará después de definir CONFIG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import gc  # noqa: E402\n",
    "import hashlib  # noqa: E402\n",
    "import json  # noqa: E402\n",
    "import logging  # noqa: E402\n",
    "import math  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import re  # noqa: E402\n",
    "import shutil  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "import zipfile  # noqa: E402\n",
    "from collections import Counter  # noqa: E402\n",
    "from datetime import datetime  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import seaborn as sns  # noqa: E402\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder  # noqa: E402\n",
    "from tensorflow import keras  # noqa: E402\n",
    "from tensorflow.keras import layers  # noqa: E402\n",
    "from tensorflow.keras.callbacks import (  # noqa: E402\n",
    "    Callback,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde20d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALAR DEPENDENCIAS (solo en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import mne\n",
    "except ImportError:\n",
    "    if IN_KAGGLE:\n",
    "        print(\"Instalando dependencias...\")\n",
    "        %pip install -q mne\n",
    "        print(\"[OK] Dependencias instaladas\")\n",
    "        import mne\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b5cb4",
   "metadata": {},
   "source": [
    "## Configuracion del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIPERPARAMETROS - AJUSTAR SEGUN NECESIDAD\n",
    "# ============================================================\n",
    "\n",
    "EXECUTION_MODE = \"full\"  # \"debug\" o \"full\"\n",
    "\n",
    "# Recursos Kaggle:\n",
    "# - RAM: 30GB\n",
    "# - VRAM: 2x16GB = 32GB\n",
    "# - Disco output: 19.5GB\n",
    "# - Disco total: 57GB\n",
    "\n",
    "CONFIG = {\n",
    "    \"execution_mode\": EXECUTION_MODE,\n",
    "    \"manifest_path\": (\n",
    "        \"/kaggle/input/sleep-edf-trimmed-f32/manifest_trimmed_spt.csv\"\n",
    "        if IN_KAGGLE\n",
    "        else f\"{DATA_PATH}/manifest_trimmed_spt.csv\"\n",
    "    ),\n",
    "    \"epoch_length\": 30.0,\n",
    "    \"sfreq\": 100,\n",
    "    \"debug_max_subjects\": 16,\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    # === SECUENCIAS ===\n",
    "    \"seq_length\": 11,  # Epochs por secuencia (5.5 min contexto)\n",
    "    # Stride options para /kaggle/tmp (50GB disponibles):\n",
    "    # - stride=1: ~120k secuencias (~64GB) - NO CABE\n",
    "    # - stride=2: ~60k secuencias (~32GB) - CABE, más datos (RECOMENDADO)\n",
    "    # - stride=3: ~40k secuencias (~21GB) - CABE, fallback conservador\n",
    "    \"seq_stride_train\": 2,  # Stride para train (más overlap = más datos)\n",
    "    \"seq_stride_val\": 2,  # Stride para val\n",
    "    \"seq_stride_test\": 11,  # Stride para test = seq_length (sin overlap)\n",
    "    # === CNN Feature Extractor ===\n",
    "    \"cnn_filters\": [32, 64, 128],\n",
    "    \"cnn_kernel_sizes\": [5, 5, 3],\n",
    "    \"feature_dim\": 128,\n",
    "    # === LSTM Temporal ===\n",
    "    \"lstm_units\": [64, 32],\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    # === Training ===\n",
    "    \"learning_rate_initial\": 3e-4,\n",
    "    \"learning_rate_min\": 1e-6,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 300 if EXECUTION_MODE == \"full\" else 50,\n",
    "    \"use_class_weights\": True,\n",
    "    \"class_weight_clip\": 1.5,\n",
    "    \"std_epsilon\": 1e-6,\n",
    "    \"clip_value\": 5.0,\n",
    "    \"early_stopping_patience\": 40 if EXECUTION_MODE == \"full\" else 10,\n",
    "    \"use_mixed_precision\": False,\n",
    "    # === Control ===\n",
    "    \"run_optimization\": False,\n",
    "    \"max_steps_per_epoch\": 115,\n",
    "    \"shuffle_buffer\": 3000,\n",
    "    # === TFRecord Streaming (modo full) ===\n",
    "    \"streaming\": EXECUTION_MODE == \"full\",\n",
    "    \"tfrecord_dir\": \"/kaggle/tmp/tfrecords_cnn_lstm_seq\"\n",
    "    if IN_KAGGLE\n",
    "    else f\"{OUTPUT_PATH}/tfrecords_cnn_lstm_seq\",\n",
    "}\n",
    "\n",
    "CONFIG[\"effective_batch_size\"] = CONFIG[\"batch_size\"] * strategy.num_replicas_in_sync\n",
    "CONFIG[\"samples_per_epoch\"] = int(CONFIG[\"epoch_length\"] * CONFIG[\"sfreq\"])\n",
    "\n",
    "if CONFIG.get(\"use_mixed_precision\", False) and gpus:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(\"[OK] Mixed precision (float16) habilitado\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No se pudo habilitar mixed precision: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] Mixed precision desactivado (float32) para mayor estabilidad\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if EXECUTION_MODE == \"debug\":\n",
    "    print(\" MODO DEBUG: Carga en RAM, subset de sujetos\")\n",
    "    print(f\"   Max sujetos: {CONFIG['debug_max_subjects']}\")\n",
    "else:\n",
    "    print(\" MODO FULL: Streaming TFRecord, todos los sujetos\")\n",
    "    print(f\"   TFRecord dir: {CONFIG['tfrecord_dir']}\")\n",
    "print(f\"   Epochs training: {CONFIG['epochs']}\")\n",
    "print(\n",
    "    f\"   Seq length: {CONFIG['seq_length']} epochs ({CONFIG['seq_length']*30/60:.1f} min contexto)\"\n",
    ")\n",
    "print(f\"   Seq stride train/val: {CONFIG['seq_stride_train']} (overlapping)\")\n",
    "print(f\"   Seq stride test: {CONFIG['seq_stride_test']} (sin overlap)\")\n",
    "print(f\"   Max steps/epoch: {CONFIG['max_steps_per_epoch']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nConfiguracion del experimento:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d67c6f",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f62c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "STAGE_CANONICAL = {\n",
    "    \"Sleep stage W\": \"W\",\n",
    "    \"Sleep stage 1\": \"N1\",\n",
    "    \"Sleep stage 2\": \"N2\",\n",
    "    \"Sleep stage 3\": \"N3\",\n",
    "    \"Sleep stage 4\": \"N3\",\n",
    "    \"Sleep stage R\": \"REM\",\n",
    "}\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "REQUIRED_CHANNELS = [\"EEG Fpz-Cz\", \"EEG Pz-Oz\"]\n",
    "OPTIONAL_CHANNELS = [\"EOG horizontal\", \"EMG submental\"]\n",
    "\n",
    "\n",
    "def extract_subject_core(subject_id):\n",
    "    \"\"\"Agrupa noches del mismo sujeto. SC4XXNy -> SC4XX\"\"\"\n",
    "    sid = str(subject_id)\n",
    "    match = re.match(r\"(SC4\\d{2})\", sid)\n",
    "    return match.group(1) if match else sid\n",
    "\n",
    "\n",
    "def load_psg_data(psg_path, channels=None, target_sfreq=None):\n",
    "    \"\"\"Carga datos PSG desde archivo .fif.\n",
    "\n",
    "    NOTA: Usa REQUIRED_CHANNELS + OPTIONAL_CHANNELS (EEG + EOG + EMG).\n",
    "    EOG es crucial para detectar REM, EMG ayuda a distinguir estadios.\n",
    "    IMPORTANTE: Valida que TODOS los canales estén presentes para consistencia.\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_fif(str(psg_path), preload=True, verbose=\"ERROR\")\n",
    "    available = set(raw.ch_names)\n",
    "    if channels is None:\n",
    "        # FIXED: Usar todos los canales pero validar que estén presentes\n",
    "        all_expected = REQUIRED_CHANNELS + OPTIONAL_CHANNELS\n",
    "        missing = [ch for ch in all_expected if ch not in available]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Canales faltantes en {psg_path}: {missing}\")\n",
    "        channels = all_expected.copy()\n",
    "    raw.pick(channels)\n",
    "    if target_sfreq and raw.info[\"sfreq\"] != target_sfreq:\n",
    "        raw.resample(target_sfreq)\n",
    "    return raw.get_data(), raw.info[\"sfreq\"], raw.ch_names\n",
    "\n",
    "\n",
    "def load_hypnogram(hyp_path):\n",
    "    df = pd.read_csv(hyp_path)\n",
    "    df[\"stage_canonical\"] = df[\"description\"].map(STAGE_CANONICAL)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_epochs(data, sfreq, epoch_length=30.0):\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // samples_per_epoch\n",
    "    epochs = [\n",
    "        data[:, i * samples_per_epoch : (i + 1) * samples_per_epoch]\n",
    "        for i in range(n_epochs)\n",
    "    ]\n",
    "    epoch_times = [i * epoch_length for i in range(n_epochs)]\n",
    "    return np.array(epochs), np.array(epoch_times)\n",
    "\n",
    "\n",
    "def assign_stages(epoch_times, hypnogram, epoch_length=30.0):\n",
    "    stages = []\n",
    "    for t in epoch_times:\n",
    "        epoch_center = t + epoch_length / 2\n",
    "        mask = (hypnogram[\"onset\"] <= epoch_center) & (\n",
    "            hypnogram[\"onset\"] + hypnogram[\"duration\"] > epoch_center\n",
    "        )\n",
    "        matched = hypnogram[mask]\n",
    "        stages.append(matched.iloc[0][\"stage_canonical\"] if len(matched) > 0 else None)\n",
    "    return stages\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seq_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES PARA GENERACIÓN DE SECUENCIAS\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def create_sequences_from_session(epochs, stages, seq_length, seq_stride=1):\n",
    "    \"\"\"\n",
    "    Crea secuencias deslizantes de epochs consecutivos de una sesión.\n",
    "\n",
    "    Args:\n",
    "        epochs: (N, channels, samples) - epochs de una sesión\n",
    "        stages: (N,) - labels por epoch\n",
    "        seq_length: longitud de secuencia\n",
    "        seq_stride: paso entre secuencias (1=overlap, seq_length=sin overlap)\n",
    "\n",
    "    Returns:\n",
    "        X_seq: (M, seq_length, channels, samples)\n",
    "        y_seq: (M, seq_length) - labels para cada epoch\n",
    "    \"\"\"\n",
    "    sequences_X, sequences_y = [], []\n",
    "    n_epochs = len(epochs)\n",
    "\n",
    "    if n_epochs < seq_length:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    for i in range(0, n_epochs - seq_length + 1, seq_stride):\n",
    "        seq_X = epochs[i : i + seq_length]\n",
    "        seq_y = stages[i : i + seq_length]\n",
    "\n",
    "        # Solo incluir secuencias sin labels inválidos y sin NaN/Inf\n",
    "        if all(s in STAGE_ORDER for s in seq_y) and np.all(np.isfinite(seq_X)):\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append([STAGE_ORDER.index(s) for s in seq_y])\n",
    "\n",
    "    if len(sequences_X) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    return np.array(sequences_X, dtype=np.float32), np.array(\n",
    "        sequences_y, dtype=np.int32\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sequences_for_split(\n",
    "    manifest_path,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    allowed_cores,\n",
    "    seq_length,\n",
    "    seq_stride,\n",
    "    split_name=\"train\",\n",
    "):\n",
    "    \"\"\"Carga secuencias para un split específico.\"\"\"\n",
    "    print(f\"\\n[INFO] Cargando secuencias para {split_name} (stride={seq_stride})...\")\n",
    "    start_time = time.time()\n",
    "    all_seq_X, all_seq_y, all_subject_cores = [], [], []\n",
    "    expected_channels = None\n",
    "    total_epochs, total_seqs = 0, 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        elif list(ch_names) != expected_channels:\n",
    "            raise ValueError(f\"Canales inconsistentes: {list(ch_names)}\")\n",
    "\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        total_epochs += len(epochs)\n",
    "\n",
    "        # Crear secuencias con stride específico para este split\n",
    "        seq_X, seq_y = create_sequences_from_session(\n",
    "            epochs, stages, seq_length, seq_stride\n",
    "        )\n",
    "\n",
    "        if len(seq_X) > 0:\n",
    "            all_seq_X.extend(seq_X)\n",
    "            all_seq_y.extend(seq_y)\n",
    "            all_subject_cores.extend([subject_core] * len(seq_X))\n",
    "            total_seqs += len(seq_X)\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   {split_name}: {i}/{total} sesiones, {total_seqs} secuencias\")\n",
    "\n",
    "    if len(all_seq_X) == 0:\n",
    "        return np.array([]), np.array([]), np.array([]), expected_channels\n",
    "\n",
    "    X = np.array(all_seq_X, dtype=np.float32)\n",
    "    y = np.array(all_seq_y, dtype=np.int32)\n",
    "    subject_cores = np.array(all_subject_cores)\n",
    "\n",
    "    # Transponer: (batch, seq_len, channels, samples) -> (batch, seq_len, samples, channels)\n",
    "    X = np.transpose(X, (0, 1, 3, 2))\n",
    "\n",
    "    print(\n",
    "        f\"[OK] {split_name}: {total_seqs} secuencias en {time.time() - start_time:.1f}s\"\n",
    "    )\n",
    "    print(f\"   Shape: {X.shape}, Epochs originales: {total_epochs}\")\n",
    "\n",
    "    return X, y, subject_cores, expected_channels\n",
    "\n",
    "\n",
    "def get_split_cores(\n",
    "    manifest_path, test_size, val_size, random_state, allowed_cores=None\n",
    "):\n",
    "    \"\"\"Divide sujetos en train/val/test cores.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    subject_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "\n",
    "    if allowed_cores is not None:\n",
    "        subject_cores = np.array([c for c in subject_cores if c in allowed_cores])\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(subject_cores)\n",
    "\n",
    "    n_test = max(1, int(len(subject_cores) * test_size))\n",
    "    n_val = max(1, int(len(subject_cores) * val_size))\n",
    "\n",
    "    test_cores = set(subject_cores[:n_test])\n",
    "    val_cores = set(subject_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(subject_cores[n_test + n_val :])\n",
    "\n",
    "    return train_cores, val_cores, test_cores\n",
    "\n",
    "\n",
    "def normalize_sequences(X_train, X_val, X_test, clip_value, std_epsilon):\n",
    "    \"\"\"Normaliza secuencias usando estadísticas de train (z-score por canal).\"\"\"\n",
    "    # X shape: (batch, seq_len, samples, channels)\n",
    "    mean_ch = X_train.mean(axis=(0, 1, 2))  # (channels,)\n",
    "    var_ch = X_train.var(axis=(0, 1, 2))\n",
    "    std_ch = np.sqrt(var_ch + std_epsilon)\n",
    "\n",
    "    X_train_norm = np.clip((X_train - mean_ch) / std_ch, -clip_value, clip_value)\n",
    "    X_val_norm = (\n",
    "        np.clip((X_val - mean_ch) / std_ch, -clip_value, clip_value)\n",
    "        if len(X_val) > 0\n",
    "        else X_val\n",
    "    )\n",
    "    X_test_norm = (\n",
    "        np.clip((X_test - mean_ch) / std_ch, -clip_value, clip_value)\n",
    "        if len(X_test) > 0\n",
    "        else X_test\n",
    "    )\n",
    "\n",
    "    return X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de secuencias definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfrecord_seq_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES TFRECORD PARA SECUENCIAS\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def make_seq_example(\n",
    "    x_seq, y_seq, sample_weights, seq_length, samples_per_epoch, n_channels\n",
    "):\n",
    "    \"\"\"\n",
    "    Serializa una secuencia completa a TFRecord.\n",
    "    x_seq: (seq_length, samples_per_epoch, n_channels) - ya transpuesto\n",
    "    y_seq: (seq_length,) - labels\n",
    "    sample_weights: (seq_length,) - peso por epoch según su clase\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x_seq.ravel())),\n",
    "        \"y\": tf.train.Feature(int64_list=tf.train.Int64List(value=y_seq.ravel())),\n",
    "        \"sw\": tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=sample_weights.ravel())\n",
    "        ),\n",
    "    }\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    ).SerializeToString()\n",
    "\n",
    "\n",
    "def compute_normalization_stats(manifest_path, epoch_length, sfreq, train_cores):\n",
    "    \"\"\"\n",
    "    Primera pasada: calcula mean/std por canal usando solo datos de train.\n",
    "\n",
    "    NOTA: Lleva running_n como vector por canal para manejar NaN/Inf\n",
    "    desbalanceados entre canales.\n",
    "    \"\"\"\n",
    "    print(\"\\n[INFO] Calculando estadísticas de normalización (solo train)...\")\n",
    "\n",
    "    running_sum = None\n",
    "    running_sumsq = None\n",
    "    running_n = None  # Vector por canal\n",
    "    n_channels = None\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, train_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        n_ch = data.shape[0]\n",
    "\n",
    "        if n_channels is None:\n",
    "            n_channels = n_ch\n",
    "            running_sum = np.zeros(n_ch, dtype=np.float64)\n",
    "            running_sumsq = np.zeros(n_ch, dtype=np.float64)\n",
    "            running_n = np.zeros(n_ch, dtype=np.int64)  # Vector por canal\n",
    "\n",
    "        # Acumular estadísticas por canal (cada canal con su propio count)\n",
    "        for c in range(n_ch):\n",
    "            channel_data = data[c, :]\n",
    "            valid_mask = np.isfinite(channel_data)\n",
    "            valid_data = channel_data[valid_mask]\n",
    "            running_sum[c] += valid_data.sum()\n",
    "            running_sumsq[c] += (valid_data**2).sum()\n",
    "            running_n[c] += len(valid_data)  # Count por canal\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Stats: {i}/{total} sesiones\")\n",
    "\n",
    "    # Calcular mean/std finales (cada canal dividido por su propio count)\n",
    "    # Protect against division by zero if a channel has no valid data\n",
    "    if np.any(running_n == 0):\n",
    "        zero_channels = np.where(running_n == 0)[0]\n",
    "        print(\n",
    "            f\"[WARN] Canales sin datos válidos: {zero_channels}. Usando mean=0, std=1.\"\n",
    "        )\n",
    "        running_n = np.maximum(running_n, 1)  # Avoid division by zero\n",
    "\n",
    "    mean_ch = running_sum / running_n\n",
    "    var_ch = running_sumsq / running_n - mean_ch**2\n",
    "    var_ch = np.maximum(var_ch, 0.0)\n",
    "    std_ch = np.sqrt(var_ch + CONFIG.get(\"std_epsilon\", 1e-6))\n",
    "\n",
    "    # Set default stats for channels with no data\n",
    "    mean_ch = np.where(running_n > 1, mean_ch, 0.0)\n",
    "    std_ch = np.where(running_n > 1, std_ch, 1.0)\n",
    "\n",
    "    print(f\"[OK] Samples por canal: {running_n}\")\n",
    "\n",
    "    return mean_ch.astype(np.float32), std_ch.astype(np.float32), n_channels\n",
    "\n",
    "\n",
    "def compute_class_counts(manifest_path, epoch_length, sfreq, train_cores):\n",
    "    \"\"\"\n",
    "    Calcula distribución de clases recorriendo TODAS las sesiones de train.\n",
    "    No usa break, cuenta todos los stages válidos.\n",
    "    \"\"\"\n",
    "    print(\"\\n[INFO] Calculando distribución de clases en train...\")\n",
    "    class_counts = Counter()\n",
    "    total_epochs = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, train_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for s in stages:\n",
    "            if s in STAGE_ORDER:\n",
    "                class_counts[s] += 1\n",
    "                total_epochs += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Class counts: {i}/{total} sesiones, {total_epochs} epochs\")\n",
    "\n",
    "    print(f\"[OK] Distribución: {dict(class_counts)}\")\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def build_tfrecord_sequences(\n",
    "    manifest_path,\n",
    "    mean_ch,\n",
    "    std_ch,\n",
    "    split_cores,\n",
    "    split_name,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    seq_length,\n",
    "    seq_stride,\n",
    "    tfrecord_dir,\n",
    "    class_weights=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Construye TFRecords con secuencias para un split.\n",
    "    Procesa sesión por sesión para minimizar uso de RAM.\n",
    "    class_weights: dict {class_idx: weight} para generar sample_weights\n",
    "    \"\"\"\n",
    "    tfrecord_dir = Path(tfrecord_dir)\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tfrecord_path = tfrecord_dir / f\"{split_name}.tfrecord\"\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(str(tfrecord_path))\n",
    "    total_seqs = 0\n",
    "    total_epochs = 0\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "    n_channels = len(mean_ch)\n",
    "\n",
    "    print(f\"\\n[INFO] Generando TFRecord para {split_name} (stride={seq_stride})...\")\n",
    "    if class_weights:\n",
    "        print(f\"   [INFO] Usando class_weights para sample_weights: {class_weights}\")\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, split_cores\n",
    "    ):\n",
    "        # Cargar datos de esta sesión\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        total_epochs += len(epochs)\n",
    "\n",
    "        # Normalizar epochs\n",
    "        epochs_norm = np.clip(\n",
    "            (epochs - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "            -clip_value,\n",
    "            clip_value,\n",
    "        )\n",
    "\n",
    "        # Transponer a (n_epochs, samples, channels) para modelo\n",
    "        epochs_norm = np.transpose(epochs_norm, (0, 2, 1))\n",
    "\n",
    "        # Crear secuencias de esta sesión\n",
    "        n_epochs = len(epochs_norm)\n",
    "        if n_epochs < seq_length:\n",
    "            continue\n",
    "\n",
    "        for start_idx in range(0, n_epochs - seq_length + 1, seq_stride):\n",
    "            seq_stages = stages[start_idx : start_idx + seq_length]\n",
    "\n",
    "            if not all(s in STAGE_ORDER for s in seq_stages):\n",
    "                continue\n",
    "\n",
    "            seq_X = epochs_norm[start_idx : start_idx + seq_length]\n",
    "\n",
    "            if not np.all(np.isfinite(seq_X)):\n",
    "                continue\n",
    "\n",
    "            seq_y = np.array([STAGE_ORDER.index(s) for s in seq_stages], dtype=np.int64)\n",
    "\n",
    "            # Generar sample_weights basados en clase de cada epoch\n",
    "            if class_weights:\n",
    "                seq_sw = np.array([class_weights[yi] for yi in seq_y], dtype=np.float32)\n",
    "            else:\n",
    "                seq_sw = np.ones(seq_length, dtype=np.float32)\n",
    "\n",
    "            writer.write(\n",
    "                make_seq_example(\n",
    "                    seq_X.astype(np.float32),\n",
    "                    seq_y,\n",
    "                    seq_sw,\n",
    "                    seq_length,\n",
    "                    samples_per_epoch,\n",
    "                    n_channels,\n",
    "                )\n",
    "            )\n",
    "            total_seqs += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   {split_name}: {i}/{total} sesiones, {total_seqs} secuencias\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    file_size_mb = tfrecord_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"[OK] {split_name}: {total_seqs} secuencias, {file_size_mb:.1f} MB\")\n",
    "\n",
    "    return str(tfrecord_path), total_seqs\n",
    "\n",
    "\n",
    "def make_seq_dataset(\n",
    "    tfrecord_path,\n",
    "    seq_length,\n",
    "    samples_per_epoch,\n",
    "    n_channels,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    repeat=False,\n",
    "):\n",
    "    \"\"\"Crea dataset de secuencias desde TFRecord con sample_weights.\"\"\"\n",
    "\n",
    "    feature_description = {\n",
    "        \"x\": tf.io.FixedLenFeature(\n",
    "            [seq_length * samples_per_epoch * n_channels], tf.float32\n",
    "        ),\n",
    "        \"y\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"sw\": tf.io.FixedLenFeature([seq_length], tf.float32),\n",
    "    }\n",
    "\n",
    "    def _parse(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        x = tf.reshape(example[\"x\"], (seq_length, samples_per_epoch, n_channels))\n",
    "        y = tf.cast(example[\"y\"], tf.int32)\n",
    "        sw = example[\"sw\"]  # (seq_length,)\n",
    "        return x, y, sw\n",
    "\n",
    "    ds = tf.data.TFRecordDataset([tfrecord_path], num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            CONFIG[\"shuffle_buffer\"],\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_tfrecord_seq_cache_key(manifest_path, config):\n",
    "    \"\"\"Genera cache key para detectar si los TFRecords son válidos.\"\"\"\n",
    "    manifest_mtime = os.path.getmtime(manifest_path)\n",
    "    key_data = (\n",
    "        f\"{manifest_path}_{manifest_mtime}_{config['sfreq']}_{config['epoch_length']}\"\n",
    "        f\"_{config['seq_length']}_{config['seq_stride_train']}_{config['seq_stride_val']}_{config['seq_stride_test']}\"\n",
    "        f\"_{config['clip_value']}_{config.get('std_epsilon', 1e-6)}_{config['random_state']}\"\n",
    "        f\"_{config['test_size']}_{config['val_size']}_{config.get('debug_max_subjects', 'all')}\"\n",
    "        f\"_sw_v2\"  # Version tag para invalidar cache cuando cambia formato\n",
    "    )\n",
    "    return hashlib.md5(key_data.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones TFRecord para secuencias definidas (con sample_weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PIPELINE DE DATOS (DEBUG: RAM / FULL: TFRecord)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def resolve_paths(row, manifest_dir, dataset_dir_name):\n",
    "    \"\"\"Resuelve rutas de PSG e hipnograma respetando Kaggle/local.\"\"\"\n",
    "    psg_path_str = row.get(\"psg_trimmed_path\", \"\")\n",
    "    hyp_path_str = row.get(\"hypnogram_trimmed_path\", \"\")\n",
    "    if pd.isna(psg_path_str):\n",
    "        psg_path_str = \"\"\n",
    "    if pd.isna(hyp_path_str):\n",
    "        hyp_path_str = \"\"\n",
    "    base_data_root = manifest_dir.parent\n",
    "\n",
    "    if psg_path_str and hyp_path_str:\n",
    "        if IN_KAGGLE:\n",
    "            psg_rel, hyp_rel = Path(psg_path_str), Path(hyp_path_str)\n",
    "            psg_anchor = next(\n",
    "                (\n",
    "                    i\n",
    "                    for i, p in enumerate(psg_rel.parts)\n",
    "                    if p.startswith(\"sleep_trimmed\")\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            hyp_anchor = next(\n",
    "                (\n",
    "                    i\n",
    "                    for i, p in enumerate(hyp_rel.parts)\n",
    "                    if p.startswith(\"sleep_trimmed\")\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if psg_anchor is not None and hyp_anchor is not None:\n",
    "                psg_path = Path(DATA_PATH) / Path(*psg_rel.parts[psg_anchor:])\n",
    "                hyp_path = Path(DATA_PATH) / Path(*hyp_rel.parts[hyp_anchor:])\n",
    "            else:\n",
    "                psg_path = Path(DATA_PATH) / dataset_dir_name / \"psg\" / psg_rel.name\n",
    "                hyp_path = (\n",
    "                    Path(DATA_PATH) / dataset_dir_name / \"hypnograms\" / hyp_rel.name\n",
    "                )\n",
    "        else:\n",
    "            psg_rel, hyp_rel = Path(psg_path_str), Path(hyp_path_str)\n",
    "            psg_path = (\n",
    "                base_data_root / psg_rel.relative_to(\"data\")\n",
    "                if psg_rel.parts and psg_rel.parts[0] == \"data\"\n",
    "                else manifest_dir / psg_rel\n",
    "                if not psg_rel.is_absolute()\n",
    "                else psg_rel\n",
    "            )\n",
    "            hyp_path = (\n",
    "                base_data_root / hyp_rel.relative_to(\"data\")\n",
    "                if hyp_rel.parts and hyp_rel.parts[0] == \"data\"\n",
    "                else manifest_dir / hyp_rel\n",
    "                if not hyp_rel.is_absolute()\n",
    "                else hyp_rel\n",
    "            )\n",
    "    else:\n",
    "        subset = row.get(\"subset\", \"sleep-cassette\")\n",
    "        version = row.get(\"version\", \"1.0.0\")\n",
    "        dataset_dir = manifest_dir / dataset_dir_name\n",
    "        psg_path = (\n",
    "            dataset_dir\n",
    "            / \"psg\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_raw.fif\"\n",
    "        )\n",
    "        hyp_path = (\n",
    "            dataset_dir\n",
    "            / \"hypnograms\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_annotations.csv\"\n",
    "        )\n",
    "    return psg_path, hyp_path\n",
    "\n",
    "\n",
    "def get_subject_cores_for_mode(\n",
    "    manifest_path, execution_mode, debug_max_subjects, random_state\n",
    "):\n",
    "    \"\"\"Obtiene los cores de sujetos a usar según el modo de ejecución.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    all_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(all_cores)\n",
    "    if execution_mode == \"debug\" and debug_max_subjects:\n",
    "        selected_cores = set(all_cores[:debug_max_subjects])\n",
    "        print(f\"[DEBUG] Usando {len(selected_cores)}/{len(all_cores)} sujetos\")\n",
    "    else:\n",
    "        selected_cores = set(all_cores)\n",
    "        print(f\"[FULL] Usando todos los {len(selected_cores)} sujetos\")\n",
    "    return selected_cores\n",
    "\n",
    "\n",
    "def iter_sessions(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Itera sesiones entregando rutas resueltas.\n",
    "\n",
    "    FIXED: Ahora reporta cuántas sesiones se saltaron por archivos faltantes.\n",
    "    \"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    if allowed_cores is not None:\n",
    "        manifest_ok = manifest_ok[\n",
    "            manifest_ok[\"subject_id\"].apply(extract_subject_core).isin(allowed_cores)\n",
    "        ]\n",
    "    manifest_dir = Path(manifest_path).parent\n",
    "    dataset_dir_name = (\n",
    "        \"sleep_trimmed_resamp200\"\n",
    "        if (manifest_dir / \"sleep_trimmed_resamp200\").exists()\n",
    "        else \"sleep_trimmed_spt\"\n",
    "        if (manifest_dir / \"sleep_trimmed_spt\").exists()\n",
    "        else \"sleep_trimmed\"\n",
    "    )\n",
    "    total_sessions = len(manifest_ok)\n",
    "    skipped_count = 0  # FIXED: Contador de sesiones saltadas\n",
    "    yielded_count = 0\n",
    "    print(f\"\\nProcesando {total_sessions} sesiones...\")\n",
    "    for i, (_, row) in enumerate(manifest_ok.iterrows(), start=1):\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        subject_core = extract_subject_core(subject_id)\n",
    "        if allowed_cores is not None and subject_core not in allowed_cores:\n",
    "            continue\n",
    "        psg_path, hyp_path = resolve_paths(row, manifest_dir, dataset_dir_name)\n",
    "        if not psg_path.exists() or not hyp_path.exists():\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        yielded_count += 1\n",
    "        yield i, total_sessions, subject_id, subject_core, psg_path, hyp_path\n",
    "    # FIXED: Loggear sesiones saltadas para auditoría\n",
    "    if skipped_count > 0:\n",
    "        print(f\"[WARN] Se saltaron {skipped_count} sesiones por archivos faltantes\")\n",
    "    print(f\"[INFO] Sesiones procesadas: {yielded_count}/{total_sessions}\")\n",
    "\n",
    "\n",
    "def update_running_stats(stats, epochs):\n",
    "    if stats is None:\n",
    "        stats = {\"n\": 0, \"sum\": None, \"sumsq\": None}\n",
    "    if stats[\"sum\"] is None:\n",
    "        stats[\"sum\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "        stats[\"sumsq\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "    stats[\"sum\"] += epochs.sum(axis=(0, 2))\n",
    "    stats[\"sumsq\"] += (epochs**2).sum(axis=(0, 2))\n",
    "    stats[\"n\"] += epochs.shape[0] * epochs.shape[2]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def finalize_running_stats(stats, std_epsilon=1e-6):\n",
    "    mean = stats[\"sum\"] / stats[\"n\"]\n",
    "    var = stats[\"sumsq\"] / stats[\"n\"] - mean**2\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + std_epsilon)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_all_data_to_ram(manifest_path, epoch_length, sfreq, allowed_cores):\n",
    "    \"\"\"Carga todos los datos en RAM (modo debug).\"\"\"\n",
    "    print(\"\\n[DEBUG] Cargando datos en RAM...\")\n",
    "    start_time = time.time()\n",
    "    all_epochs, all_stages, all_subject_cores = [], [], []\n",
    "    expected_channels = None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        elif list(ch_names) != expected_channels:\n",
    "            raise ValueError(f\"Canales inconsistentes: {list(ch_names)}\")\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                nan_inf_count += 1\n",
    "                continue\n",
    "            all_epochs.append(epoch)\n",
    "            all_stages.append(stage)\n",
    "            all_subject_cores.append(subject_core)\n",
    "        if i % 10 == 0 or i == total:\n",
    "            print(f\"   Cargando: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf\")\n",
    "    X = np.array(all_epochs, dtype=np.float32)\n",
    "    y = np.array([STAGE_ORDER.index(s) for s in all_stages], dtype=np.int32)\n",
    "    subject_cores = np.array(all_subject_cores)\n",
    "    print(\n",
    "        f\"[OK] Datos cargados en {time.time() - start_time:.1f}s. Shape: {X.shape}, Memoria: {X.nbytes / 1024**3:.2f} GB\"\n",
    "    )\n",
    "    return X, y, subject_cores, expected_channels\n",
    "\n",
    "\n",
    "def split_data_by_subject(X, y, subject_cores, test_size, val_size, random_state):\n",
    "    \"\"\"Divide datos por sujeto (sin data leakage).\"\"\"\n",
    "    unique_cores = np.unique(subject_cores)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(unique_cores)\n",
    "    n_test = max(1, int(len(unique_cores) * test_size))\n",
    "    n_val = max(1, int(len(unique_cores) * val_size))\n",
    "    test_cores = set(unique_cores[:n_test])\n",
    "    val_cores = set(unique_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(unique_cores[n_test + n_val :])\n",
    "    train_mask = np.isin(subject_cores, list(train_cores))\n",
    "    val_mask = np.isin(subject_cores, list(val_cores))\n",
    "    test_mask = np.isin(subject_cores, list(test_cores))\n",
    "    return (\n",
    "        X[train_mask],\n",
    "        y[train_mask],\n",
    "        X[val_mask],\n",
    "        y[val_mask],\n",
    "        X[test_mask],\n",
    "        y[test_mask],\n",
    "        train_cores,\n",
    "        val_cores,\n",
    "        test_cores,\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_data(X_train, X_val, X_test, clip_value, std_epsilon):\n",
    "    \"\"\"Normaliza datos usando estadísticas de train (z-score por canal).\"\"\"\n",
    "    mean_ch = X_train.mean(axis=(0, 2))\n",
    "    var_ch = X_train.var(axis=(0, 2))\n",
    "    std_ch = np.sqrt(var_ch + std_epsilon)\n",
    "    X_train_norm = np.clip(\n",
    "        (X_train - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    X_val_norm = np.clip(\n",
    "        (X_val - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    X_test_norm = np.clip(\n",
    "        (X_test - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    return X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de pipeline definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES TFRECORD\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def pass1_stats(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Primera pasada: mean/std por canal y conteo de clases.\"\"\"\n",
    "    stats, class_counts, input_shape, expected_channels = None, Counter(), None, None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        valid_mask = [s in STAGE_ORDER for s in stages]\n",
    "        if not any(valid_mask):\n",
    "            continue\n",
    "        valid_epochs = epochs[valid_mask]\n",
    "        valid_stages = [s for s in stages if s in STAGE_ORDER]\n",
    "\n",
    "        finite_mask = np.all(np.isfinite(valid_epochs), axis=(1, 2))\n",
    "        if not np.all(finite_mask):\n",
    "            nan_inf_count += np.sum(~finite_mask)\n",
    "            valid_epochs = valid_epochs[finite_mask]\n",
    "            valid_stages = [s for s, m in zip(valid_stages, finite_mask) if m]\n",
    "\n",
    "        if len(valid_epochs) == 0:\n",
    "            continue\n",
    "        if input_shape is None:\n",
    "            input_shape = valid_epochs.shape[1:]\n",
    "        stats = update_running_stats(stats, valid_epochs)\n",
    "        class_counts.update(valid_stages)\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 1: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf\")\n",
    "    assert input_shape is not None, \"No se encontraron epochs validos\"\n",
    "    mean, std = finalize_running_stats(\n",
    "        stats, std_epsilon=CONFIG.get(\"std_epsilon\", 1e-6)\n",
    "    )\n",
    "    return mean, std, class_counts, input_shape, expected_channels\n",
    "\n",
    "\n",
    "def make_example(x, y):\n",
    "    feature = {\n",
    "        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x.ravel())),\n",
    "        \"y\": tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n",
    "    }\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    ).SerializeToString()\n",
    "\n",
    "\n",
    "def assign_subject_splits(\n",
    "    manifest_path, test_size, val_size, random_state, allowed_cores=None\n",
    "):\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    subject_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    if allowed_cores is not None:\n",
    "        subject_cores = np.array([c for c in subject_cores if c in allowed_cores])\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(subject_cores)\n",
    "    n_test = max(1, int(len(subject_cores) * test_size))\n",
    "    n_val = max(1, int(len(subject_cores) * val_size))\n",
    "    test_cores = set(subject_cores[:n_test])\n",
    "    val_cores = set(subject_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(subject_cores[n_test + n_val :])\n",
    "    split_map = dict.fromkeys(train_cores, \"train\")\n",
    "    split_map.update(dict.fromkeys(val_cores, \"val\"))\n",
    "    split_map.update(dict.fromkeys(test_cores, \"test\"))\n",
    "    return split_map, {\n",
    "        \"train\": len(train_cores),\n",
    "        \"val\": len(val_cores),\n",
    "        \"test\": len(test_cores),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_tfrecord_splits(\n",
    "    manifest_path,\n",
    "    mean,\n",
    "    std,\n",
    "    split_map,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    tfrecord_dir,\n",
    "    expected_channels=None,\n",
    "):\n",
    "    tfrecord_dir = Path(tfrecord_dir)\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "    writers = {\n",
    "        k: tf.io.TFRecordWriter(str(tfrecord_dir / f\"{k}.tfrecord\"))\n",
    "        for k in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "    counts, session_counts, subject_sets = (\n",
    "        Counter(),\n",
    "        Counter(),\n",
    "        {k: set() for k in writers},\n",
    "    )\n",
    "    skipped_nan_inf = 0\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "    allowed_cores = set(split_map.keys())\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        split = split_map.get(subject_core)\n",
    "        if split is None:\n",
    "            continue\n",
    "        session_counts[split] += 1\n",
    "        subject_sets[split].add(subject_core)\n",
    "        data, _, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels and list(ch_names) != expected_channels:\n",
    "            raise ValueError(f\"Canales inconsistentes en {psg_path}\")\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "            y = STAGE_ORDER.index(stage)\n",
    "            x = np.clip((epoch - mean[:, None]) / std[:, None], -clip_value, clip_value)\n",
    "            if not np.all(np.isfinite(x)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "            writers[split].write(make_example(x.astype(np.float32), y))\n",
    "            counts[split] += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 2 ({split}): {i}/{total} sesiones\")\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "    if skipped_nan_inf > 0:\n",
    "        print(f\"[WARN] Se descartaron {skipped_nan_inf} epochs con NaN/Inf\")\n",
    "    return (\n",
    "        {k: str(tfrecord_dir / f\"{k}.tfrecord\") for k in writers},\n",
    "        counts,\n",
    "        session_counts,\n",
    "        {k: len(subject_sets[k]) for k in subject_sets},\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(\n",
    "    tfrecord_path, input_shape, batch_size, shuffle=False, repeat=False, for_lstm=True\n",
    "):\n",
    "    \"\"\"Crea dataset desde TFRecord. for_lstm=True transpone para LSTM: (samples, channels).\"\"\"\n",
    "    feature_description = {\n",
    "        \"x\": tf.io.FixedLenFeature([input_shape[0] * input_shape[1]], tf.float32),\n",
    "        \"y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "\n",
    "    def _parse(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        x = tf.reshape(example[\"x\"], input_shape)\n",
    "        x = tf.clip_by_value(x, -clip_value, clip_value)\n",
    "        x = tf.where(tf.math.is_finite(x), x, tf.zeros_like(x))\n",
    "        if for_lstm:\n",
    "            x = tf.transpose(\n",
    "                x, perm=[1, 0]\n",
    "            )  # (channels, samples) -> (samples, channels)\n",
    "        y = tf.cast(example[\"y\"], tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    ds = tf.data.TFRecordDataset([tfrecord_path], num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    ds = ds.with_options(options)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            CONFIG[\"shuffle_buffer\"],\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_tfrecord_cache_key(manifest_path, config):\n",
    "    manifest_mtime = os.path.getmtime(manifest_path)\n",
    "    key_data = (\n",
    "        f\"{manifest_path}_{manifest_mtime}_{config['sfreq']}_{config['epoch_length']}\"\n",
    "        f\"_{config.get('clip_value', 'na')}_{config['random_state']}\"\n",
    "        f\"_{config['test_size']}_{config['val_size']}_{config.get('debug_max_subjects', 'all')}\"\n",
    "    )\n",
    "    return hashlib.md5(key_data.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones TFRecord definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d360aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EJECUTAR PIPELINE SEGÚN MODO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n[INFO] Modo de ejecución: {CONFIG['execution_mode'].upper()}\")\n",
    "print(f\"[INFO] Longitud de secuencia: {CONFIG['seq_length']} epochs\")\n",
    "\n",
    "selected_cores = get_subject_cores_for_mode(\n",
    "    CONFIG[\"manifest_path\"],\n",
    "    CONFIG[\"execution_mode\"],\n",
    "    CONFIG[\"debug_max_subjects\"],\n",
    "    CONFIG[\"random_state\"],\n",
    ")\n",
    "\n",
    "# Dividir sujetos en train/val/test\n",
    "train_cores, val_cores, test_cores = get_split_cores(\n",
    "    CONFIG[\"manifest_path\"],\n",
    "    CONFIG[\"test_size\"],\n",
    "    CONFIG[\"val_size\"],\n",
    "    CONFIG[\"random_state\"],\n",
    "    selected_cores,\n",
    ")\n",
    "print(\n",
    "    f\"\\n[INFO] Split por sujeto: train={len(train_cores)}, val={len(val_cores)}, test={len(test_cores)}\"\n",
    ")\n",
    "\n",
    "USE_STREAMING = CONFIG.get(\"streaming\", False)\n",
    "\n",
    "if USE_STREAMING:\n",
    "    # ========== MODO FULL: TFRecord Streaming ==========\n",
    "    print(\"\\n[FULL] Usando TFRecord streaming...\")\n",
    "\n",
    "    tfrecord_path = Path(CONFIG[\"tfrecord_dir\"])\n",
    "    cache_key = get_tfrecord_seq_cache_key(CONFIG[\"manifest_path\"], CONFIG)\n",
    "    cache_marker = tfrecord_path / f\".cache_{cache_key}\"\n",
    "    cache_metadata_path = tfrecord_path / \"cache_metadata.json\"\n",
    "\n",
    "    if tfrecord_path.exists() and cache_marker.exists():\n",
    "        print(f\"[INFO] Reutilizando TFRecords existentes (cache: {cache_key})\")\n",
    "        with open(cache_metadata_path) as f:\n",
    "            cache_meta = json.load(f)\n",
    "        mean_ch = np.array(cache_meta[\"mean_ch\"], dtype=np.float32)\n",
    "        std_ch = np.array(cache_meta[\"std_ch\"], dtype=np.float32)\n",
    "        n_channels = cache_meta[\"n_channels\"]\n",
    "        train_count = cache_meta[\"train_count\"]\n",
    "        val_count = cache_meta[\"val_count\"]\n",
    "        test_count = cache_meta[\"test_count\"]\n",
    "        class_counts_train = Counter(cache_meta[\"class_counts_train\"])\n",
    "        class_weights = cache_meta.get(\"class_weights\", {})\n",
    "        # Convert string keys back to int\n",
    "        class_weights = {int(k): v for k, v in class_weights.items()}\n",
    "        tfrecord_paths = cache_meta[\"tfrecord_paths\"]\n",
    "    else:\n",
    "        # Regenerar TFRecords\n",
    "        if tfrecord_path.exists():\n",
    "            print(\"[INFO] Cache inválido, regenerando TFRecords...\")\n",
    "            shutil.rmtree(tfrecord_path)\n",
    "\n",
    "        # Paso 1: Calcular estadísticas de normalización (solo train)\n",
    "        mean_ch, std_ch, n_channels = compute_normalization_stats(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            train_cores,\n",
    "        )\n",
    "        print(f\"[OK] Estadísticas: mean={mean_ch}, std={std_ch}\")\n",
    "\n",
    "        # Paso 2: Calcular distribución de clases en train (recorriendo TODAS las sesiones)\n",
    "        class_counts_train = compute_class_counts(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            train_cores,\n",
    "        )\n",
    "\n",
    "        # Paso 3: Calcular class_weights para sample_weights\n",
    "        # FIXED: Cálculo directo sin np.repeat (evita explotar memoria con datasets grandes)\n",
    "        # Fórmula balanced: w_c = N / (K * n_c)\n",
    "        counts_list = [class_counts_train.get(stage, 1) for stage in STAGE_ORDER]\n",
    "        total_samples = sum(counts_list)\n",
    "        n_classes = len(STAGE_ORDER)\n",
    "        class_weight_clip = CONFIG.get(\"class_weight_clip\", 1.5)\n",
    "        class_weights = {}\n",
    "        for k, count in enumerate(counts_list):\n",
    "            weight = total_samples / (n_classes * max(count, 1))\n",
    "            class_weights[k] = float(np.clip(weight, 0.5, class_weight_clip))\n",
    "        print(f\"[OK] Class weights para sample_weights: {class_weights}\")\n",
    "\n",
    "        # Paso 4: Generar TFRecords para cada split (con sample_weights)\n",
    "        tfrecord_paths = {}\n",
    "\n",
    "        train_path, train_count = build_tfrecord_sequences(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            mean_ch,\n",
    "            std_ch,\n",
    "            train_cores,\n",
    "            \"train\",\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            CONFIG[\"seq_length\"],\n",
    "            CONFIG[\"seq_stride_train\"],\n",
    "            CONFIG[\"tfrecord_dir\"],\n",
    "            class_weights=class_weights,\n",
    "        )\n",
    "        tfrecord_paths[\"train\"] = train_path\n",
    "\n",
    "        # Val/test use uniform weights (1.0) - class_weights only for training loss\n",
    "        val_path, val_count = build_tfrecord_sequences(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            mean_ch,\n",
    "            std_ch,\n",
    "            val_cores,\n",
    "            \"val\",\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            CONFIG[\"seq_length\"],\n",
    "            CONFIG[\"seq_stride_val\"],\n",
    "            CONFIG[\"tfrecord_dir\"],\n",
    "            class_weights=None,  # Val: uniform weights\n",
    "        )\n",
    "        tfrecord_paths[\"val\"] = val_path\n",
    "\n",
    "        test_path, test_count = build_tfrecord_sequences(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            mean_ch,\n",
    "            std_ch,\n",
    "            test_cores,\n",
    "            \"test\",\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            CONFIG[\"seq_length\"],\n",
    "            CONFIG[\"seq_stride_test\"],\n",
    "            CONFIG[\"tfrecord_dir\"],\n",
    "            class_weights=None,  # Test: uniform weights\n",
    "        )\n",
    "        tfrecord_paths[\"test\"] = test_path\n",
    "\n",
    "        # Guardar metadata (incluyendo class_weights)\n",
    "        cache_meta = {\n",
    "            \"mean_ch\": mean_ch.tolist(),\n",
    "            \"std_ch\": std_ch.tolist(),\n",
    "            \"n_channels\": n_channels,\n",
    "            \"train_count\": train_count,\n",
    "            \"val_count\": val_count,\n",
    "            \"test_count\": test_count,\n",
    "            \"class_counts_train\": dict(class_counts_train),\n",
    "            \"class_weights\": class_weights,\n",
    "            \"tfrecord_paths\": tfrecord_paths,\n",
    "        }\n",
    "        tfrecord_path.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_metadata_path, \"w\") as f:\n",
    "            json.dump(cache_meta, f, indent=2)\n",
    "        cache_marker.touch()\n",
    "        print(f\"[OK] Cache guardado (key: {cache_key})\")\n",
    "\n",
    "    # Crear datasets desde TFRecords\n",
    "    INPUT_SHAPE = (CONFIG[\"seq_length\"], CONFIG[\"samples_per_epoch\"], n_channels)\n",
    "\n",
    "    train_ds = make_seq_dataset(\n",
    "        tfrecord_paths[\"train\"],\n",
    "        CONFIG[\"seq_length\"],\n",
    "        CONFIG[\"samples_per_epoch\"],\n",
    "        n_channels,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=True,\n",
    "        repeat=True,\n",
    "    )\n",
    "    val_ds = (\n",
    "        make_seq_dataset(\n",
    "            tfrecord_paths[\"val\"],\n",
    "            CONFIG[\"seq_length\"],\n",
    "            CONFIG[\"samples_per_epoch\"],\n",
    "            n_channels,\n",
    "            CONFIG[\"effective_batch_size\"],\n",
    "            shuffle=False,\n",
    "            repeat=False,\n",
    "        )\n",
    "        if val_count > 0\n",
    "        else None\n",
    "    )\n",
    "    test_ds = (\n",
    "        make_seq_dataset(\n",
    "            tfrecord_paths[\"test\"],\n",
    "            CONFIG[\"seq_length\"],\n",
    "            CONFIG[\"samples_per_epoch\"],\n",
    "            n_channels,\n",
    "            CONFIG[\"effective_batch_size\"],\n",
    "            shuffle=False,\n",
    "            repeat=False,\n",
    "        )\n",
    "        if test_count > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # ========== MODO DEBUG: Carga en RAM ==========\n",
    "    print(\"\\n[DEBUG] Cargando secuencias en RAM...\")\n",
    "\n",
    "    # Cargar secuencias para cada split\n",
    "    X_train, y_train, _, expected_channels = load_sequences_for_split(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        train_cores,\n",
    "        CONFIG[\"seq_length\"],\n",
    "        CONFIG[\"seq_stride_train\"],\n",
    "        \"train\",\n",
    "    )\n",
    "    X_val, y_val, _, _ = load_sequences_for_split(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        val_cores,\n",
    "        CONFIG[\"seq_length\"],\n",
    "        CONFIG[\"seq_stride_train\"],\n",
    "        \"val\",\n",
    "    )\n",
    "    X_test, y_test, _, _ = load_sequences_for_split(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        test_cores,\n",
    "        CONFIG[\"seq_length\"],\n",
    "        CONFIG[\"seq_stride_test\"],\n",
    "        \"test\",\n",
    "    )\n",
    "\n",
    "    n_channels = X_train.shape[-1] if len(X_train) > 0 else 4\n",
    "\n",
    "    # Normalizar\n",
    "    X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch = normalize_sequences(\n",
    "        X_train, X_val, X_test, CONFIG[\"clip_value\"], CONFIG[\"std_epsilon\"]\n",
    "    )\n",
    "    print(f\"[OK] Normalización: mean={mean_ch}, std={std_ch}\")\n",
    "\n",
    "    del X_train, X_val, X_test\n",
    "    gc.collect()\n",
    "\n",
    "    INPUT_SHAPE = X_train_norm.shape[1:]\n",
    "    train_count, val_count, test_count = (\n",
    "        len(X_train_norm),\n",
    "        len(X_val_norm),\n",
    "        len(X_test_norm),\n",
    "    )\n",
    "\n",
    "    # Calcular class_weights para sample_weights (modo debug)\n",
    "    # FIXED: Cálculo directo sin np.repeat (evita explotar memoria)\n",
    "    y_flat = y_train.flatten()\n",
    "    class_counts_train = Counter([STAGE_ORDER[yi] for yi in y_flat])\n",
    "    counts_list = [class_counts_train.get(stage, 1) for stage in STAGE_ORDER]\n",
    "    total_samples = sum(counts_list)\n",
    "    n_classes = len(STAGE_ORDER)\n",
    "    class_weight_clip = CONFIG.get(\"class_weight_clip\", 1.5)\n",
    "    class_weights = {}\n",
    "    for k, count in enumerate(counts_list):\n",
    "        weight = total_samples / (n_classes * max(count, 1))\n",
    "        class_weights[k] = float(np.clip(weight, 0.5, class_weight_clip))\n",
    "    print(f\"[OK] Class weights para sample_weights: {class_weights}\")\n",
    "\n",
    "    # Generar sample_weights para cada secuencia\n",
    "    def generate_sample_weights(y, class_weights):\n",
    "        \"\"\"Genera sample_weights (seq_len,) para cada secuencia.\"\"\"\n",
    "        sw = np.array(\n",
    "            [[class_weights[yi] for yi in seq] for seq in y], dtype=np.float32\n",
    "        )\n",
    "        return sw\n",
    "\n",
    "    sw_train = generate_sample_weights(y_train, class_weights)\n",
    "    sw_val = (\n",
    "        generate_sample_weights(y_val, class_weights)\n",
    "        if len(y_val) > 0\n",
    "        else np.array([])\n",
    "    )\n",
    "    sw_test = (\n",
    "        generate_sample_weights(y_test, class_weights)\n",
    "        if len(y_test) > 0\n",
    "        else np.array([])\n",
    "    )\n",
    "\n",
    "    # Crear datasets (tupla de 3 elementos: x, y, sample_weight)\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train_norm, y_train, sw_train))\n",
    "        .shuffle(\n",
    "            min(5000, train_count),\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "        .batch(CONFIG[\"effective_batch_size\"])\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        (\n",
    "            tf.data.Dataset.from_tensor_slices((X_val_norm, y_val, sw_val))\n",
    "            .batch(CONFIG[\"effective_batch_size\"])\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        if val_count > 0\n",
    "        else None\n",
    "    )\n",
    "    test_ds = (\n",
    "        (\n",
    "            tf.data.Dataset.from_tensor_slices((X_test_norm, y_test, sw_test))\n",
    "            .batch(CONFIG[\"effective_batch_size\"])\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        if test_count > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "# Label encoder para compatibilidad\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(STAGE_ORDER)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"[OK] Pipeline {'STREAMING' if USE_STREAMING else 'RAM'} listo\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE} (seq_len, samples, channels)\")\n",
    "print(f\"   Train: {train_count:,} secuencias\")\n",
    "print(f\"   Val: {val_count:,} secuencias\")\n",
    "print(\n",
    "    f\"   Test: {test_count:,} secuencias (stride={CONFIG['seq_stride_test']}, sin overlap)\"\n",
    ")\n",
    "print(f\"   Class distribution (train): {dict(class_counts_train)}\")\n",
    "print(f\"   Class weights (sample_weights): {class_weights}\")\n",
    "print(\"   [OK] sample_weight por epoch incluido en dataset\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57856e0",
   "metadata": {},
   "source": [
    "## Arquitectura CNN-LSTM Secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr_schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEARNING RATE SCHEDULE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def create_lr_schedule(\n",
    "    initial_lr, min_lr, warmup_epochs, total_epochs, steps_per_epoch\n",
    "):\n",
    "    \"\"\"Warmup lineal + cosine decay.\"\"\"\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    decay_steps = max(total_steps - warmup_steps, 1)\n",
    "\n",
    "    class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, initial_lr, min_lr, warmup_steps, decay_steps):\n",
    "            super().__init__()\n",
    "            self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "            self.min_lr = tf.cast(min_lr, tf.float32)\n",
    "            self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "            self.decay_steps = tf.cast(decay_steps, tf.float32)\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            warmup_progress = step / tf.maximum(self.warmup_steps, 1.0)\n",
    "            warmup_lr = self.min_lr + (self.initial_lr - self.min_lr) * tf.minimum(\n",
    "                warmup_progress, 1.0\n",
    "            )\n",
    "            decay_progress = tf.minimum(\n",
    "                tf.maximum((step - self.warmup_steps) / self.decay_steps, 0.0), 1.0\n",
    "            )\n",
    "            cosine_decay = 0.5 * (\n",
    "                1.0 + tf.cos(tf.constant(np.pi, dtype=tf.float32) * decay_progress)\n",
    "            )\n",
    "            decay_lr = self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay\n",
    "            return tf.cond(\n",
    "                step < self.warmup_steps, lambda: warmup_lr, lambda: decay_lr\n",
    "            )\n",
    "\n",
    "        def get_config(self):\n",
    "            # Use numpy() for proper tensor-to-Python conversion\n",
    "            return {\n",
    "                \"initial_lr\": float(\n",
    "                    self.initial_lr.numpy()\n",
    "                    if hasattr(self.initial_lr, \"numpy\")\n",
    "                    else self.initial_lr\n",
    "                ),\n",
    "                \"min_lr\": float(\n",
    "                    self.min_lr.numpy()\n",
    "                    if hasattr(self.min_lr, \"numpy\")\n",
    "                    else self.min_lr\n",
    "                ),\n",
    "                \"warmup_steps\": int(\n",
    "                    self.warmup_steps.numpy()\n",
    "                    if hasattr(self.warmup_steps, \"numpy\")\n",
    "                    else self.warmup_steps\n",
    "                ),\n",
    "                \"decay_steps\": int(\n",
    "                    self.decay_steps.numpy()\n",
    "                    if hasattr(self.decay_steps, \"numpy\")\n",
    "                    else self.decay_steps\n",
    "                ),\n",
    "            }\n",
    "\n",
    "    return WarmupCosineDecay(initial_lr, min_lr, warmup_steps, decay_steps)\n",
    "\n",
    "\n",
    "print(\"[OK] Learning rate schedule definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ARQUITECTURA CNN-LSTM SECUENCIAL\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def build_cnn_feature_extractor(\n",
    "    samples_per_epoch, n_channels, cnn_filters, cnn_kernel_sizes, feature_dim\n",
    "):\n",
    "    \"\"\"CNN para extraer features de un epoch individual.\"\"\"\n",
    "    inputs = keras.Input(shape=(samples_per_epoch, n_channels), name=\"epoch_input\")\n",
    "    x = inputs\n",
    "\n",
    "    for i, (filters, kernel) in enumerate(zip(cnn_filters, cnn_kernel_sizes)):\n",
    "        x = layers.Conv1D(\n",
    "            filters,\n",
    "            kernel,\n",
    "            padding=\"same\",\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "            kernel_initializer=\"he_uniform\",\n",
    "            name=f\"conv_{i+1}\",\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(name=f\"bn_conv_{i+1}\")(x)\n",
    "        x = layers.Activation(\"gelu\")(x)\n",
    "        if i < len(cnn_filters) - 1:\n",
    "            x = layers.MaxPooling1D(2, name=f\"pool_{i+1}\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(name=\"global_pool\")(x)\n",
    "    x = layers.Dense(\n",
    "        feature_dim,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        name=\"feature_dense\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"bn_feature\")(x)\n",
    "    outputs = layers.Activation(\"gelu\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"cnn_feature_extractor\")\n",
    "\n",
    "\n",
    "def build_cnn_lstm_model(\n",
    "    seq_length,\n",
    "    samples_per_epoch,\n",
    "    n_channels,\n",
    "    n_classes=5,\n",
    "    cnn_filters=[32, 64, 128],\n",
    "    cnn_kernel_sizes=[5, 5, 3],\n",
    "    feature_dim=128,\n",
    "    lstm_units=[64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    bidirectional=True,\n",
    "    lr_schedule=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Modelo híbrido CNN-LSTM para sleep staging con secuencias.\n",
    "\n",
    "    Input: (batch, seq_length, samples_per_epoch, n_channels)\n",
    "    Output: (batch, seq_length, n_classes) - many-to-many\n",
    "\n",
    "    NOTA: sample_weight por epoch se pasa como tercer elemento del dataset.\n",
    "    Keras acepta (x, y, sample_weight) para balancear clases minoritarias.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(\n",
    "        shape=(seq_length, samples_per_epoch, n_channels),\n",
    "        name=\"sequence_input\",\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    # Nivel 1: CNN Feature Extractor (TimeDistributed)\n",
    "    cnn_model = build_cnn_feature_extractor(\n",
    "        samples_per_epoch, n_channels, cnn_filters, cnn_kernel_sizes, feature_dim\n",
    "    )\n",
    "    features = layers.TimeDistributed(cnn_model, name=\"td_cnn\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate, name=\"dropout_features\")(features)\n",
    "    # features shape: (batch, seq_length, feature_dim)\n",
    "\n",
    "    # Nivel 2: LSTM Temporal\n",
    "    x = features\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        lstm = layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=True,  # Many-to-many\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "            name=f\"lstm_{i+1}\",\n",
    "        )\n",
    "        if bidirectional:\n",
    "            x = layers.Bidirectional(lstm, name=f\"bilstm_{i+1}\")(x)\n",
    "        else:\n",
    "            x = lstm(x)\n",
    "        # LayerNorm en lugar de BatchNorm para secuencias (más estable)\n",
    "        x = layers.LayerNormalization(name=f\"ln_lstm_{i+1}\")(x)\n",
    "        x = layers.Dropout(dropout_rate, name=f\"dropout_lstm_{i+1}\")(x)\n",
    "\n",
    "    # Output: predicción para cada epoch de la secuencia\n",
    "    outputs = layers.TimeDistributed(\n",
    "        layers.Dense(n_classes, kernel_initializer=\"glorot_uniform\"), name=\"output\"\n",
    "    )(x)\n",
    "    # outputs shape: (batch, seq_length, n_classes) - logits\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"CNN_LSTM_SeqSleepStaging\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule if lr_schedule else 1e-4, clipnorm=1.0\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"[OK] Arquitectura CNN-LSTM Secuencial definida\")\n",
    "print(\"   [OK] sample_weight por epoch soportado para balancear clases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO CNN-LSTM\n",
    "# ============================================================\n",
    "\n",
    "seq_length = CONFIG[\"seq_length\"]\n",
    "samples_per_epoch = CONFIG[\"samples_per_epoch\"]\n",
    "n_channels = INPUT_SHAPE[-1]\n",
    "\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"   seq_length: {seq_length}\")\n",
    "print(f\"   samples_per_epoch: {samples_per_epoch}\")\n",
    "print(f\"   n_channels: {n_channels}\")\n",
    "\n",
    "# Calcular steps per epoch\n",
    "MAX_STEPS_PER_EPOCH = CONFIG[\"max_steps_per_epoch\"]\n",
    "full_steps_train = math.ceil(train_count / CONFIG[\"effective_batch_size\"])\n",
    "full_steps_val = (\n",
    "    math.ceil(val_count / CONFIG[\"effective_batch_size\"]) if val_count else 0\n",
    ")\n",
    "\n",
    "steps_per_epoch_train = min(full_steps_train, MAX_STEPS_PER_EPOCH)\n",
    "steps_per_epoch_val = full_steps_val if USE_STREAMING else None\n",
    "\n",
    "print(\"\\n[INFO] Steps/epoch:\")\n",
    "print(f\"   Full steps: {full_steps_train}\")\n",
    "print(f\"   Actual steps (cap {MAX_STEPS_PER_EPOCH}): {steps_per_epoch_train}\")\n",
    "if full_steps_train > MAX_STEPS_PER_EPOCH:\n",
    "    pct = 100 * steps_per_epoch_train / full_steps_train\n",
    "    print(f\"   [NOTE] Cada 'epoch' procesa {pct:.1f}% de train\")\n",
    "\n",
    "lr_schedule = create_lr_schedule(\n",
    "    initial_lr=CONFIG[\"learning_rate_initial\"],\n",
    "    min_lr=CONFIG[\"learning_rate_min\"],\n",
    "    warmup_epochs=CONFIG[\"warmup_epochs\"],\n",
    "    total_epochs=CONFIG[\"epochs\"],\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n[INFO] LR Schedule: initial={CONFIG['learning_rate_initial']}, min={CONFIG['learning_rate_min']}\"\n",
    ")\n",
    "print(f\"   Warmup: {CONFIG['warmup_epochs']} epochs\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_cnn_lstm_model(\n",
    "        seq_length=seq_length,\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "        n_channels=n_channels,\n",
    "        n_classes=len(STAGE_ORDER),\n",
    "        cnn_filters=CONFIG[\"cnn_filters\"],\n",
    "        cnn_kernel_sizes=CONFIG[\"cnn_kernel_sizes\"],\n",
    "        feature_dim=CONFIG[\"feature_dim\"],\n",
    "        lstm_units=CONFIG[\"lstm_units\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        bidirectional=CONFIG[\"bidirectional\"],\n",
    "        lr_schedule=lr_schedule,\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314001f",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4486e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS PARA SECUENCIAS\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"cnn_lstm_seq_{CONFIG['execution_mode']}_{timestamp}\"\n",
    "\n",
    "\n",
    "class SeqSleepMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Calcula F1-Macro, Kappa, val_loss y val_accuracy para secuencias (many-to-many).\n",
    "\n",
    "    IMPORTANTE: Persiste métricas en epochs intermedios para EarlyStopping.\n",
    "    Calcula val_loss desde logits sin pasadas extra por los datos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val_ds, stage_order, eval_every=1):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.stage_order = stage_order\n",
    "        self.eval_every = max(1, eval_every)\n",
    "        self.best_f1_macro, self.best_kappa, self.best_weights = -1.0, -1.0, None\n",
    "        # Almacenar últimos valores para epochs intermedios\n",
    "        self.last_f1_macro = 0.0\n",
    "        self.last_kappa = 0.0\n",
    "        self.last_val_loss = 0.0\n",
    "        self.last_val_accuracy = 0.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        # Siempre escribir los últimos valores conocidos para que EarlyStopping funcione\n",
    "        if (epoch + 1) % self.eval_every != 0:\n",
    "            logs[\"val_f1_macro\"] = self.last_f1_macro\n",
    "            logs[\"val_kappa\"] = self.last_kappa\n",
    "            logs[\"val_loss\"] = self.last_val_loss\n",
    "            logs[\"val_accuracy\"] = self.last_val_accuracy\n",
    "            return\n",
    "\n",
    "        # Manejar caso de val_ds vacío o None\n",
    "        if self.val_ds is None:\n",
    "            self.last_f1_macro, self.last_kappa = 0.0, 0.0\n",
    "            self.last_val_loss, self.last_val_accuracy = 0.0, 0.0\n",
    "            logs[\"val_f1_macro\"], logs[\"val_kappa\"] = 0.0, 0.0\n",
    "            logs[\"val_loss\"], logs[\"val_accuracy\"] = 0.0, 0.0\n",
    "            return\n",
    "\n",
    "        y_true_list, y_pred_list, logits_list = [], [], []\n",
    "        try:\n",
    "            for batch in self.val_ds:\n",
    "                # Handle both 2-element (x, y) and 3-element (x, y, sw) datasets\n",
    "                x_batch = batch[0]\n",
    "                y_batch = batch[1]\n",
    "                y_pred_logits = self.model.predict(x_batch, verbose=0)\n",
    "                y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "                y_pred_list.append(y_pred.flatten())\n",
    "                y_true_list.append(y_batch.numpy().flatten())\n",
    "                logits_list.append(y_pred_logits.reshape(-1, y_pred_logits.shape[-1]))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error en evaluación: {e}\")\n",
    "            logs[\"val_f1_macro\"], logs[\"val_kappa\"] = (\n",
    "                self.last_f1_macro,\n",
    "                self.last_kappa,\n",
    "            )\n",
    "            logs[\"val_loss\"], logs[\"val_accuracy\"] = (\n",
    "                self.last_val_loss,\n",
    "                self.last_val_accuracy,\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(y_true_list) == 0:\n",
    "            logs[\"val_f1_macro\"], logs[\"val_kappa\"] = (\n",
    "                self.last_f1_macro,\n",
    "                self.last_kappa,\n",
    "            )\n",
    "            logs[\"val_loss\"], logs[\"val_accuracy\"] = (\n",
    "                self.last_val_loss,\n",
    "                self.last_val_accuracy,\n",
    "            )\n",
    "            return\n",
    "\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "        logits = np.concatenate(logits_list)\n",
    "\n",
    "        if len(y_true) == 0:\n",
    "            logs[\"val_f1_macro\"], logs[\"val_kappa\"] = (\n",
    "                self.last_f1_macro,\n",
    "                self.last_kappa,\n",
    "            )\n",
    "            logs[\"val_loss\"], logs[\"val_accuracy\"] = (\n",
    "                self.last_val_loss,\n",
    "                self.last_val_accuracy,\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Calcular val_loss desde logits (sin pasada extra)\n",
    "        val_loss = (\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                y_true, logits, from_logits=True\n",
    "            )\n",
    "            .numpy()\n",
    "            .mean()\n",
    "        )\n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Actualizar últimos valores conocidos\n",
    "        self.last_f1_macro, self.last_kappa = f1, kappa\n",
    "        self.last_val_loss, self.last_val_accuracy = val_loss, val_accuracy\n",
    "        logs[\"val_f1_macro\"], logs[\"val_kappa\"] = f1, kappa\n",
    "        logs[\"val_loss\"], logs[\"val_accuracy\"] = val_loss, val_accuracy\n",
    "\n",
    "        if f1 > self.best_f1_macro:\n",
    "            self.best_f1_macro, self.best_kappa = f1, kappa\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print(f\" - val_f1_macro mejoró a {f1:.4f} (kappa={kappa:.4f})\")\n",
    "        print(\n",
    "            f\" - val_loss: {val_loss:.4f} - val_acc: {val_accuracy:.4f} - val_f1: {f1:.4f} - val_kappa: {kappa:.4f}\"\n",
    "        )\n",
    "\n",
    "    def restore_best_weights(self):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            print(\n",
    "                f\"[OK] Restaurados pesos (F1={self.best_f1_macro:.4f}, Kappa={self.best_kappa:.4f})\"\n",
    "            )\n",
    "\n",
    "\n",
    "class NaNDebugCallback(Callback):\n",
    "    \"\"\"Detecta NaN en loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nan_count, self.last_valid_loss = 0, None\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        loss = (logs or {}).get(\"loss\", 0)\n",
    "        if not (np.isnan(loss) or np.isinf(loss)):\n",
    "            self.last_valid_loss = loss\n",
    "            return\n",
    "        self.nan_count += 1\n",
    "        if self.nan_count == 1:\n",
    "            print(\n",
    "                f\"\\n[WARN] NaN en batch {batch} (último válido={self.last_valid_loss})\"\n",
    "            )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.nan_count > 0:\n",
    "            print(f\"   [INFO] Epoch {epoch+1}: {self.nan_count} batches con NaN\")\n",
    "        self.nan_count = 0\n",
    "\n",
    "\n",
    "# Evalúa métricas completas cada 3 epochs, pero persiste valores para EarlyStopping\n",
    "metrics_callback = SeqSleepMetricsCallback(val_ds, STAGE_ORDER, eval_every=3)\n",
    "\n",
    "callbacks = [\n",
    "    NaNDebugCallback(),\n",
    "    metrics_callback,\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        patience=CONFIG[\"early_stopping_patience\"],\n",
    "        restore_best_weights=False,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{OUTPUT_PATH}/{model_name}_best.keras\",\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Modelo: {model_name}\")\n",
    "print(f\"Checkpoint: {OUTPUT_PATH}/{model_name}_best.keras\")\n",
    "print(\"[INFO] val_f1_macro y val_loss se persisten entre epochs para EarlyStopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203de2e6",
   "metadata": {},
   "source": [
    "## Reanudacion desde Checkpoint (opcional)\n",
    "\n",
    "Si Kaggle se desconecto durante el entrenamiento, puedes reanudar desde el ultimo checkpoint.\n",
    "Solo ejecuta la siguiente celda si necesitas reanudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REANUDAR DESDE CHECKPOINT (ejecutar solo si es necesario)\n",
    "# ============================================================\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # Cambiar a True para reanudar\n",
    "CHECKPOINT_NAME = None  # Ejemplo: \"lstm_full_20251125_143022_best.keras\"\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and CHECKPOINT_NAME:\n",
    "    checkpoint_path = f\"{OUTPUT_PATH}/{CHECKPOINT_NAME}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"[INFO] Cargando modelo desde checkpoint: {checkpoint_path}\")\n",
    "        with strategy.scope():\n",
    "            model = keras.models.load_model(checkpoint_path, custom_objects={})\n",
    "        print(\"[OK] Modelo cargado exitosamente\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Checkpoint no encontrado: {checkpoint_path}\")\n",
    "        for f in os.listdir(OUTPUT_PATH):\n",
    "            if f.endswith(\".keras\"):\n",
    "                print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"[INFO] Modo normal: se usará el modelo recién creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO CNN-LSTM\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nIniciando entrenamiento ({CONFIG['execution_mode'].upper()} mode)...\")\n",
    "print(\"   Arquitectura: CNN-LSTM Secuencial (many-to-many)\")\n",
    "print(f\"   Modo: {'Streaming TFRecord' if USE_STREAMING else 'RAM'}\")\n",
    "print(f\"   Seq length: {CONFIG['seq_length']} epochs\")\n",
    "print(f\"   Batch size efectivo: {CONFIG['effective_batch_size']}\")\n",
    "print(f\"   Epochs maximos: {CONFIG['epochs']}\")\n",
    "print(f\"   Steps/epoch: {steps_per_epoch_train}\")\n",
    "\n",
    "# No usamos class_weight con many-to-many TimeDistributed\n",
    "print(\"   [INFO] sample_weight por epoch activo (corrige class imbalance)\")\n",
    "\n",
    "VALIDATION_FREQ = 3\n",
    "training_start_time = time.time()\n",
    "\n",
    "if USE_STREAMING:\n",
    "    # Streaming mode: train_ds ya tiene repeat\n",
    "    # FIXED: Removido validation_data para evitar doble validación\n",
    "    # SeqSleepMetricsCallback ya hace la evaluación completa en val_ds\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    # RAM mode: necesitamos repeat\n",
    "    train_ds_repeat = train_ds.repeat()\n",
    "    # FIXED: Removido validation_data para evitar doble validación\n",
    "    history = model.fit(\n",
    "        train_ds_repeat,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "print(\n",
    "    f\"\\n[OK] Entrenamiento completado en {training_time / 60:.2f} min ({training_time / 3600:.2f} h)\"\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "metrics_callback.restore_best_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZAR CURVAS DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# FIXED: Manejar caso val_count==0\n",
    "has_val_metrics = \"val_loss\" in history.history and len(history.history[\"val_loss\"]) > 0\n",
    "if has_val_metrics:\n",
    "    val_epochs = np.arange(len(history.history[\"val_loss\"])) * VALIDATION_FREQ + (\n",
    "        VALIDATION_FREQ - 1\n",
    "    )\n",
    "\n",
    "axes[0, 0].plot(history.history[\"loss\"], label=\"Train Loss\", linewidth=2)\n",
    "if has_val_metrics:\n",
    "    axes[0, 0].plot(\n",
    "        val_epochs, history.history[\"val_loss\"], label=\"Val Loss\", linewidth=2\n",
    "    )\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Loss durante entrenamiento\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history[\"accuracy\"], label=\"Train Acc\", linewidth=2)\n",
    "if has_val_metrics:\n",
    "    axes[0, 1].plot(\n",
    "        val_epochs, history.history[\"val_accuracy\"], label=\"Val Acc\", linewidth=2\n",
    "    )\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].set_title(\"Accuracy durante entrenamiento\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if \"val_kappa\" in history.history:\n",
    "    val_epochs_kappa = val_epochs[: len(history.history[\"val_kappa\"])]\n",
    "    axes[1, 0].plot(\n",
    "        val_epochs_kappa,\n",
    "        history.history[\"val_kappa\"],\n",
    "        label=\"Val Kappa\",\n",
    "        linewidth=2,\n",
    "        color=\"green\",\n",
    "    )\n",
    "    axes[1, 0].axhline(\n",
    "        y=metrics_callback.best_kappa,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best={metrics_callback.best_kappa:.4f}\",\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"Cohen's Kappa\")\n",
    "    axes[1, 0].set_title(\"Cohen's Kappa\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, \"Kappa no disponible\", ha=\"center\", va=\"center\")\n",
    "\n",
    "if \"val_f1_macro\" in history.history:\n",
    "    val_epochs_f1 = val_epochs[: len(history.history[\"val_f1_macro\"])]\n",
    "    axes[1, 1].plot(\n",
    "        val_epochs_f1,\n",
    "        history.history[\"val_f1_macro\"],\n",
    "        label=\"Val F1-Macro\",\n",
    "        linewidth=2,\n",
    "        color=\"purple\",\n",
    "    )\n",
    "    axes[1, 1].axhline(\n",
    "        y=metrics_callback.best_f1_macro,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best={metrics_callback.best_f1_macro:.4f}\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"F1-Macro\")\n",
    "    axes[1, 1].set_title(\"F1-Macro (métrica de selección)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, \"F1-Macro no disponible\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f348b62",
   "metadata": {},
   "source": [
    "## Evaluacion en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACION EN TEST SET (SECUENCIAS SIN OVERLAP)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEvaluando en Test Set...\")\n",
    "print(f\"[INFO] Test usa stride={CONFIG['seq_stride_test']} (sin overlap)\")\n",
    "print(\"[INFO] Cada epoch aparece exactamente UNA vez -> métricas sin sesgo\")\n",
    "\n",
    "# Initialize metrics to None so subsequent cells can check\n",
    "accuracy, kappa, f1_macro, f1_weighted = None, None, None, None\n",
    "y_test_enc, y_pred_enc, y_pred_proba = None, None, None\n",
    "\n",
    "if test_ds is None or test_count == 0:\n",
    "    print(\"[WARN] No hay datos de test disponibles\")\n",
    "else:\n",
    "    # Recolectar predicciones y labels\n",
    "    y_true_list, y_pred_list, y_pred_proba_list = [], [], []\n",
    "\n",
    "    for batch in test_ds:\n",
    "        # Handle both 2-element (x, y) and 3-element (x, y, sw) datasets\n",
    "        x_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "        y_pred_logits = model.predict(x_batch, verbose=0)\n",
    "        y_pred_proba = tf.nn.softmax(y_pred_logits, axis=-1).numpy()\n",
    "        y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "\n",
    "        # Flatten (batch, seq_len) -> (batch * seq_len,)\n",
    "        y_true_list.append(y_batch.numpy().flatten())\n",
    "        y_pred_list.append(y_pred.flatten())\n",
    "        y_pred_proba_list.append(y_pred_proba.reshape(-1, y_pred_proba.shape[-1]))\n",
    "\n",
    "    y_test_enc = np.concatenate(y_true_list)\n",
    "    y_pred_enc = np.concatenate(y_pred_list)\n",
    "    y_pred_proba = np.concatenate(y_pred_proba_list)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_enc, y_pred_enc)\n",
    "    kappa = cohen_kappa_score(y_test_enc, y_pred_enc)\n",
    "    f1_macro = f1_score(y_test_enc, y_pred_enc, average=\"macro\", zero_division=0)\n",
    "    f1_weighted = f1_score(y_test_enc, y_pred_enc, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(\"RESULTADOS EN TEST SET\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "    print(f\"   Cohen Kappa: {kappa:.4f}  <- Métrica principal en literatura\")\n",
    "    print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "    print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "    print(f\"   Epochs únicos evaluados: {len(y_test_enc):,}\")\n",
    "    print(\n",
    "        f\"   (stride={CONFIG['seq_stride_test']}, sin overlap -> cada epoch cuenta 1 vez)\"\n",
    "    )\n",
    "    print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "if y_test_enc is not None and y_pred_enc is not None:\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test_enc,\n",
    "            y_pred_enc,\n",
    "            labels=np.arange(len(STAGE_ORDER)),\n",
    "            target_names=STAGE_ORDER,\n",
    "            digits=4,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"[SKIP] Classification report: no hay datos de test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea54046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================================\n",
    "\n",
    "if y_test_enc is None or y_pred_enc is None:\n",
    "    print(\"[SKIP] Confusion matrix: no hay datos de test\")\n",
    "else:\n",
    "    cm = confusion_matrix(y_test_enc, y_pred_enc, labels=np.arange(len(STAGE_ORDER)))\n",
    "    # Protect against division by zero for classes absent from test set\n",
    "    row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "    row_sums = np.where(row_sums == 0, 1, row_sums)  # Avoid NaN\n",
    "    cm_normalized = cm.astype(\"float\") / row_sums\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[0],\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Predicho\")\n",
    "    axes[0].set_ylabel(\"Real\")\n",
    "    axes[0].set_title(\"Matriz de Confusión (Absoluta)\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt=\".2%\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[1],\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Predicho\")\n",
    "    axes[1].set_ylabel(\"Real\")\n",
    "    axes[1].set_title(\"Matriz de Confusión (Normalizada)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_PATH}/{model_name}_confusion_matrix.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f6277",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZACION CON OPTUNA (opcional)\n",
    "# ============================================================\n",
    "\n",
    "if CONFIG[\"run_optimization\"]:\n",
    "    print(\"[WARN] Optuna no implementado para modo streaming. Skip.\")\n",
    "else:\n",
    "    print(\n",
    "        \"[SKIP] Optimización deshabilitada. Cambiar CONFIG['run_optimization'] = True para ejecutar.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf6524",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR MODELO Y ARTEFACTOS\n",
    "# ============================================================\n",
    "\n",
    "model.save(f\"{OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "print(f\"[OK] Modelo guardado: {OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "\n",
    "history_data = {k: pd.Series(v) for k, v in history.history.items()}\n",
    "history_df = pd.DataFrame(history_data)\n",
    "history_df.to_csv(f\"{OUTPUT_PATH}/{model_name}_history.csv\", index=False)\n",
    "\n",
    "# Handle case where test evaluation was not run\n",
    "_accuracy = float(accuracy) if accuracy is not None else None\n",
    "_kappa = float(kappa) if kappa is not None else None\n",
    "_f1_macro = float(f1_macro) if f1_macro is not None else None\n",
    "_f1_weighted = float(f1_weighted) if f1_weighted is not None else None\n",
    "\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"config\": CONFIG,\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": _accuracy,\n",
    "        \"kappa\": _kappa,\n",
    "        \"f1_macro\": _f1_macro,\n",
    "        \"f1_weighted\": _f1_weighted,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"training_time_seconds\": float(training_time),\n",
    "        \"training_time_minutes\": float(training_time / 60),\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "        \"best_val_f1_macro\": float(metrics_callback.best_f1_macro),\n",
    "        \"best_val_kappa\": float(metrics_callback.best_kappa),\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": int(train_count),\n",
    "        \"val_samples\": int(val_count),\n",
    "        \"test_samples\": int(test_count),\n",
    "        \"seq_length\": CONFIG[\"seq_length\"],\n",
    "        \"model_type\": \"CNN-LSTM-Sequential\",\n",
    "        \"execution_mode\": CONFIG[\"execution_mode\"],\n",
    "    },\n",
    "    \"channel_stats\": {\"mean\": mean_ch.tolist(), \"std\": std_ch.tolist()},\n",
    "    \"label_encoder_classes\": list(le.classes_),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"   - {model_name}_final.keras (modelo)\")\n",
    "print(f\"   - {model_name}_best.keras (mejor checkpoint)\")\n",
    "print(f\"   - {model_name}_history.csv (historial)\")\n",
    "print(f\"   - {model_name}_results.json (metricas y config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO CNN-LSTM SECUENCIAL COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResultados finales en Test Set:\")\n",
    "if accuracy is not None:\n",
    "    print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "    print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "    print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "    print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "else:\n",
    "    print(\"   [WARN] No se ejecutó evaluación en test\")\n",
    "print(f\"\\nModelo guardado en: {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5971229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRIMIR ARTEFACTOS PARA DESCARGA\n",
    "# ============================================================\n",
    "\n",
    "zip_path = f\"{OUTPUT_PATH}/{model_name}_artifacts.zip\"\n",
    "exclude_extensions = (\".tfrecord\", \".zip\")  # Excluir TFRecords y otros zips\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_PATH):\n",
    "        fpath = os.path.join(OUTPUT_PATH, fname)\n",
    "        # Solo incluir archivos (no directorios) que empiecen con model_name\n",
    "        if (\n",
    "            os.path.isfile(fpath)\n",
    "            and fname.startswith(model_name)\n",
    "            and not fname.endswith(exclude_extensions)\n",
    "        ):\n",
    "            zf.write(fpath, arcname=fname)\n",
    "\n",
    "print(f\"[OK] Artefactos comprimidos en: {zip_path} (excluyendo TFRecords)\")\n",
    "print(\"Archivos incluidos:\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\" - {info.filename} ({info.file_size / 1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
