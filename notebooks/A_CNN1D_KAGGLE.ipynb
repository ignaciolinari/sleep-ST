{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8211e9f1",
   "metadata": {},
   "source": [
    "# CNN1D para Sleep Staging\n",
    "\n",
    "Este notebook entrena un modelo **CNN1D con conexiones residuales** para clasificacion de estadios de sueno usando datos PSG **trimmed** (preprocesados y recortados del dataset Sleep-EDF).\n",
    "\n",
    "**Optimizado para Kaggle con 2x Tesla T4 (16GB VRAM cada una)**\n",
    "\n",
    "### Modos de ejecución:\n",
    "- **Debug** (`EXECUTION_MODE = \"debug\"`): Carga datos en RAM, usa subset de sujetos, ~30 seg de setup\n",
    "- **Full** (`EXECUTION_MODE = \"full\"`): Streaming TFRecord, todos los sujetos, ~30 min de setup\n",
    "\n",
    "### Caracteristicas:\n",
    "- CNN1D con bloques residuales\n",
    "- Data augmentation con ruido gaussiano (opcional)\n",
    "- Soporte multi-GPU con MirroredStrategy\n",
    "- Optimizacion de hiperparametros con Optuna (opcional)\n",
    "- **División train/val/test por SUJETOS (sin data leakage)** \n",
    "- Soporte para reanudar entrenamiento desde checkpoints\n",
    "- Semillas fijas para reproducibilidad (SEED=42)\n",
    "\n",
    "### Datos requeridos:\n",
    "- Dataset `sleep-edf-trimmed-f32` en Kaggle (https://www.kaggle.com/datasets/ignaciolinari/sleep-edf-trimmed-f32)\n",
    "  - `manifest_trimmed_spt.csv` (1 episodio por noche, 100 Hz)\n",
    "  - `sleep_trimmed_spt/psg/*.fif` (PSG a 100 Hz, float32)\n",
    "  - `sleep_trimmed_spt/hypnograms/*.csv` (anotaciones)\n",
    "- Si usas la versión 200 Hz, apunta a `manifest_trimmed_resamp200.csv` + carpeta `sleep_trimmed_resamp200/`.\n",
    "- Nota Kaggle: la versión 200 Hz puede consumir más VRAM; deja `sfreq=100` o usa el dataset 100 Hz para evitar OOM.\n",
    "- Si usas otro slug, actualiza `DATA_PATH` abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN INICIAL - Ejecutar primero\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Detectar si estamos en Kaggle\n",
    "IN_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Entorno: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "# Paths según entorno\n",
    "if IN_KAGGLE:\n",
    "    # En Kaggle: ajustar al nombre de tu dataset\n",
    "    DATA_PATH = \"/kaggle/input/sleep-edf-trimmed-f32/sleep_trimmed_spt\"  # <- Ajustar al slug/version que uses\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "else:\n",
    "    # Local\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../models\"\n",
    "\n",
    "# Asegurar que el directorio de salida exista antes de guardar artefactos\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR GPUs DISPONIBLES\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "# Semilla global para reproducibilidad\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPUs disponibles:\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "\n",
    "    # Habilitar memory growth para evitar OOM\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    print(f\"\\n[OK] {len(gpus)} GPU(s) configuradas con memory growth\")\n",
    "else:\n",
    "    print(\"[WARN] No se detectaron GPUs. El entrenamiento sera lento.\")\n",
    "\n",
    "# Estrategia de distribución para múltiples GPUs\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"\\nUsando MirroredStrategy con {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    if len(gpus) == 1:\n",
    "        print(\"\\nUsando estrategia por defecto (1 GPU)\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] Usando CPU - entrenamiento será lento\")\n",
    "        print(\"       Para mejor rendimiento, ejecuta en entorno con GPU\")\n",
    "\n",
    "\n",
    "# Habilitar mixed precision para mejor rendimiento en GPUs modernas (T4, V100, A100)\n",
    "# NOTA: Desactivado por defecto porque puede causar NaN en algunos modelos\n",
    "# Se activa desde CONFIG['use_mixed_precision'] después de definir CONFIG\n",
    "print(\"[INFO] Mixed precision se configurará después de definir CONFIG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08963927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import gc  # noqa: E402\n",
    "import json  # noqa: E402\n",
    "import logging  # noqa: E402\n",
    "import math  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import re  # noqa: E402\n",
    "import shutil  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "import zipfile  # noqa: E402\n",
    "from collections import Counter  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import seaborn as sns  # noqa: E402\n",
    "\n",
    "# Semillas globales para reproducibilidad\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from datetime import datetime  # noqa: E402\n",
    "\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder  # noqa: E402\n",
    "from sklearn.utils.class_weight import compute_class_weight  # noqa: E402\n",
    "from tensorflow import keras  # noqa: E402\n",
    "from tensorflow.keras import layers  # noqa: E402\n",
    "from tensorflow.keras.callbacks import (  # noqa: E402\n",
    "    Callback,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Configurar estilo de plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALAR DEPENDENCIAS (solo en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import mne\n",
    "except ImportError:\n",
    "    if IN_KAGGLE:\n",
    "        print(\"Instalando dependencias...\")\n",
    "        %pip install -q mne\n",
    "        print(\"[OK] Dependencias instaladas\")\n",
    "        import mne\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78434b1a",
   "metadata": {},
   "source": [
    "## Configuracion del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIPERPARAMETROS - AJUSTAR SEGUN NECESIDAD\n",
    "# ============================================================\n",
    "\n",
    "# =====================================================\n",
    "# MODO DE EJECUCIÓN: \"debug\" o \"full\"\n",
    "# =====================================================\n",
    "# - \"debug\": Carga datos en RAM (rápido, ~30 seg), usa subset de sujetos\n",
    "# - \"full\":  Streaming TFRecord (lento, ~30 min), usa todos los sujetos\n",
    "# =====================================================\n",
    "EXECUTION_MODE = \"debug\"  # Cambiar a \"full\" para entrenamiento completo\n",
    "# =====================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Modo de ejecución\n",
    "    \"execution_mode\": EXECUTION_MODE,\n",
    "    # Dataset (100 Hz)\n",
    "    \"manifest_path\": (\n",
    "        \"/kaggle/input/sleep-edf-trimmed-f32/manifest_trimmed_spt.csv\"\n",
    "        if IN_KAGGLE\n",
    "        else f\"{DATA_PATH}/manifest_trimmed_spt.csv\"\n",
    "    ),\n",
    "    \"epoch_length\": 30.0,  # segundos\n",
    "    \"sfreq\": 100,  # Hz (re-muestreo a 100 Hz para reducir RAM)\n",
    "    # Límite de sujetos según modo\n",
    "    \"debug_max_subjects\": 30,  # Sujetos en modo debug\n",
    "    \"limit_sessions\": None,  # None = todas las sesiones (obsoleto, usar debug_max_subjects)\n",
    "    # Split por sujeto\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    # Modelo CNN1D\n",
    "    \"n_filters\": 32,  # Reducido para mayor estabilidad\n",
    "    \"kernel_size\": 3,  # Kernel pequeño para evitar gradientes explosivos\n",
    "    \"dropout_rate\": 0.3,  # Aumentado para regularización\n",
    "    \"use_residual\": True,\n",
    "    \"use_augmentation\": False,\n",
    "    # Entrenamiento\n",
    "    \"learning_rate_initial\": 3e-4,  # LR normal\n",
    "    \"learning_rate_min\": 1e-6,  # LR mínimo\n",
    "    \"warmup_epochs\": 3,  # Warmup epochs\n",
    "    \"batch_size\": 64,  # Batch size base\n",
    "    \"epochs\": 300\n",
    "    if EXECUTION_MODE == \"full\"\n",
    "    else 150,  # Más epochs para compensar steps limitados\n",
    "    \"use_class_weights\": True,  # Activar para manejar desbalanceo\n",
    "    \"class_weight_clip\": 1.5,  # Límite para class weights\n",
    "    \"std_epsilon\": 1e-6,  # piso para std en normalizacion\n",
    "    \"clip_value\": 5.0,  # Límite para valores normalizados (reducido de 8.0)\n",
    "    \"early_stopping_patience\": 40\n",
    "    if EXECUTION_MODE == \"full\"\n",
    "    else 25,  # Más paciencia con epochs cortos\n",
    "    # Mixed precision (puede causar NaN en algunos modelos)\n",
    "    \"use_mixed_precision\": False,  # Desactivar para evitar NaN\n",
    "    # Optimizacion (Optuna)\n",
    "    \"run_optimization\": False,\n",
    "    \"n_optuna_trials\": 30,\n",
    "    # Streaming TFRecord (solo en modo full)\n",
    "    \"streaming\": EXECUTION_MODE == \"full\",\n",
    "    \"tfrecord_dir\": f\"{OUTPUT_PATH}/tfrecords\",\n",
    "    \"shuffle_buffer\": 5000,\n",
    "}\n",
    "\n",
    "# Ajustar batch size para multi-GPU\n",
    "CONFIG[\"effective_batch_size\"] = CONFIG[\"batch_size\"] * strategy.num_replicas_in_sync\n",
    "\n",
    "# Configurar mixed precision según CONFIG (después de definir CONFIG)\n",
    "if CONFIG.get(\"use_mixed_precision\", False) and gpus:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(\"[OK] Mixed precision (float16) habilitado\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No se pudo habilitar mixed precision: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] Mixed precision desactivado (float32) para mayor estabilidad\")\n",
    "\n",
    "# Mostrar modo de ejecución de forma prominente\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if EXECUTION_MODE == \"debug\":\n",
    "    print(\" MODO DEBUG: Carga rápida en RAM, subset de sujetos\")\n",
    "    print(f\"   Max sujetos: {CONFIG['debug_max_subjects']}\")\n",
    "    print(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "    print(\"   Para entrenamiento completo, cambiar EXECUTION_MODE = 'full'\")\n",
    "else:\n",
    "    print(\" MODO FULL: Streaming TFRecord, todos los sujetos\")\n",
    "    print(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "    print(\"   Para debugging rápido, cambiar EXECUTION_MODE = 'debug'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nConfiguracion del experimento:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7521f8c",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "# Estadios de sueño (mapeo AASM)\n",
    "STAGE_CANONICAL = {\n",
    "    \"Sleep stage W\": \"W\",\n",
    "    \"Sleep stage 1\": \"N1\",\n",
    "    \"Sleep stage 2\": \"N2\",\n",
    "    \"Sleep stage 3\": \"N3\",\n",
    "    \"Sleep stage 4\": \"N3\",  # AASM fusiona S3+S4 en N3\n",
    "    \"Sleep stage R\": \"REM\",\n",
    "}\n",
    "\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "# Canales REQUERIDOS (orden fijo para consistencia entre archivos)\n",
    "REQUIRED_CHANNELS = [\"EEG Fpz-Cz\", \"EEG Pz-Oz\"]\n",
    "OPTIONAL_CHANNELS = [\"EOG horizontal\", \"EMG submental\"]\n",
    "\n",
    "\n",
    "def extract_subject_core(subject_id):\n",
    "    \"\"\"Agrupa noches del mismo sujeto para evitar data leakage.\n",
    "\n",
    "    Sleep-EDF format: SC4XXNy where:\n",
    "    - XX = subject number (00-82), 2 dígitos\n",
    "    - N = night number (1 or 2)\n",
    "    - y = suffix (E, F, G, etc.)\n",
    "\n",
    "    Examples:\n",
    "    - SC4011E -> sujeto 01 (noche 1) -> core: SC401\n",
    "    - SC4012E -> sujeto 01 (noche 2) -> core: SC401\n",
    "    - SC4001E -> sujeto 00 (noche 1) -> core: SC400\n",
    "    - SC4002E -> sujeto 00 (noche 2) -> core: SC400\n",
    "\n",
    "    IMPORTANTE: Agrupar por sujeto es crítico para evitar data leakage.\n",
    "    Todas las noches del mismo sujeto deben ir al mismo split.\n",
    "\n",
    "    Returns SC4XX (e.g., SC401 for subject 01).\n",
    "    \"\"\"\n",
    "    sid = str(subject_id)\n",
    "    # Extraer SC4 + 2 primeros dígitos (número de sujeto)\n",
    "    match = re.match(r\"(SC4\\d{2})\", sid)\n",
    "    return match.group(1) if match else sid\n",
    "\n",
    "\n",
    "def load_psg_data(psg_path, channels=None, target_sfreq=None):\n",
    "    \"\"\"Carga datos PSG desde archivo .fif.\n",
    "\n",
    "    IMPORTANTE: Mantiene orden fijo de canales para consistencia.\n",
    "    Si falta un canal requerido, lanza error.\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_fif(str(psg_path), preload=True, verbose=\"ERROR\")\n",
    "    available = set(raw.ch_names)\n",
    "\n",
    "    if channels is None:\n",
    "        # Verificar canales requeridos\n",
    "        missing_required = [ch for ch in REQUIRED_CHANNELS if ch not in available]\n",
    "        if missing_required:\n",
    "            raise ValueError(\n",
    "                f\"Canales requeridos faltantes en {psg_path}: {missing_required}. \"\n",
    "                f\"Disponibles: {sorted(available)}\"\n",
    "            )\n",
    "\n",
    "        # Usar canales requeridos + opcionales disponibles (orden fijo)\n",
    "        channels = REQUIRED_CHANNELS.copy()\n",
    "        for ch in OPTIONAL_CHANNELS:\n",
    "            if ch in available:\n",
    "                channels.append(ch)\n",
    "\n",
    "    raw.pick(channels)  # pick_channels() está deprecado\n",
    "\n",
    "    if target_sfreq and raw.info[\"sfreq\"] != target_sfreq:\n",
    "        raw.resample(target_sfreq)\n",
    "\n",
    "    data = raw.get_data()\n",
    "    return data, raw.info[\"sfreq\"], raw.ch_names\n",
    "\n",
    "\n",
    "def load_hypnogram(hyp_path):\n",
    "    \"\"\"Carga hipnograma desde CSV.\"\"\"\n",
    "    df = pd.read_csv(hyp_path)\n",
    "    df[\"stage_canonical\"] = df[\"description\"].map(STAGE_CANONICAL)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_epochs(data, sfreq, epoch_length=30.0):\n",
    "    \"\"\"Divide datos en epochs de longitud fija.\"\"\"\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // samples_per_epoch\n",
    "\n",
    "    epochs = []\n",
    "    epoch_times = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        start = i * samples_per_epoch\n",
    "        end = start + samples_per_epoch\n",
    "        epoch = data[:, start:end]\n",
    "        epochs.append(epoch)\n",
    "        epoch_times.append(i * epoch_length)\n",
    "\n",
    "    return np.array(epochs), np.array(epoch_times)\n",
    "\n",
    "\n",
    "def assign_stages(epoch_times, hypnogram, epoch_length=30.0):\n",
    "    \"\"\"Asigna estadios a cada epoch.\"\"\"\n",
    "    stages = []\n",
    "\n",
    "    for t in epoch_times:\n",
    "        epoch_center = t + epoch_length / 2\n",
    "        mask = (hypnogram[\"onset\"] <= epoch_center) & (\n",
    "            hypnogram[\"onset\"] + hypnogram[\"duration\"] > epoch_center\n",
    "        )\n",
    "        matched = hypnogram[mask]\n",
    "\n",
    "        if len(matched) > 0:\n",
    "            stage = matched.iloc[0][\"stage_canonical\"]\n",
    "            stages.append(stage)\n",
    "        else:\n",
    "            stages.append(None)\n",
    "\n",
    "    return stages\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PIPELINE DE DATOS (DEBUG: RAM / FULL: TFRecord)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def resolve_paths(row, manifest_dir, dataset_dir_name):\n",
    "    \"\"\"Resuelve rutas de PSG e hipnograma respetando Kaggle/local.\"\"\"\n",
    "\n",
    "    psg_path_str = row.get(\"psg_trimmed_path\", \"\")\n",
    "    hyp_path_str = row.get(\"hypnogram_trimmed_path\", \"\")\n",
    "\n",
    "    # Manejar NaN\n",
    "    if pd.isna(psg_path_str):\n",
    "        psg_path_str = \"\"\n",
    "    if pd.isna(hyp_path_str):\n",
    "        hyp_path_str = \"\"\n",
    "\n",
    "    base_data_root = manifest_dir.parent\n",
    "\n",
    "    if psg_path_str and hyp_path_str:\n",
    "        if IN_KAGGLE:\n",
    "            psg_rel = Path(psg_path_str)\n",
    "            hyp_rel = Path(hyp_path_str)\n",
    "            psg_parts = psg_rel.parts\n",
    "            hyp_parts = hyp_rel.parts\n",
    "\n",
    "            psg_anchor_idx = next(\n",
    "                (i for i, p in enumerate(psg_parts) if p.startswith(\"sleep_trimmed\")),\n",
    "                None,\n",
    "            )\n",
    "            hyp_anchor_idx = next(\n",
    "                (i for i, p in enumerate(hyp_parts) if p.startswith(\"sleep_trimmed\")),\n",
    "                None,\n",
    "            )\n",
    "\n",
    "            if psg_anchor_idx is not None and hyp_anchor_idx is not None:\n",
    "                psg_path = Path(DATA_PATH) / Path(*psg_parts[psg_anchor_idx:])\n",
    "                hyp_path = Path(DATA_PATH) / Path(*hyp_parts[hyp_anchor_idx:])\n",
    "            else:\n",
    "                psg_path = Path(DATA_PATH) / dataset_dir_name / \"psg\" / psg_rel.name\n",
    "                hyp_path = (\n",
    "                    Path(DATA_PATH) / dataset_dir_name / \"hypnograms\" / hyp_rel.name\n",
    "                )\n",
    "        else:\n",
    "            psg_rel = Path(psg_path_str)\n",
    "            hyp_rel = Path(hyp_path_str)\n",
    "\n",
    "            if not psg_rel.is_absolute():\n",
    "                if psg_rel.parts and psg_rel.parts[0] == \"data\":\n",
    "                    psg_path = base_data_root / psg_rel.relative_to(\"data\")\n",
    "                else:\n",
    "                    psg_path = manifest_dir / psg_rel\n",
    "            else:\n",
    "                psg_path = psg_rel\n",
    "\n",
    "            if not hyp_rel.is_absolute():\n",
    "                if hyp_rel.parts and hyp_rel.parts[0] == \"data\":\n",
    "                    hyp_path = base_data_root / hyp_rel.relative_to(\"data\")\n",
    "                else:\n",
    "                    hyp_path = manifest_dir / hyp_rel\n",
    "            else:\n",
    "                hyp_path = hyp_rel\n",
    "    else:\n",
    "        subset = row.get(\"subset\", \"sleep-cassette\")\n",
    "        version = row.get(\"version\", \"1.0.0\")\n",
    "        dataset_dir = manifest_dir / dataset_dir_name\n",
    "        psg_path = (\n",
    "            dataset_dir\n",
    "            / \"psg\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_raw.fif\"\n",
    "        )\n",
    "        hyp_path = (\n",
    "            dataset_dir\n",
    "            / \"hypnograms\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_annotations.csv\"\n",
    "        )\n",
    "\n",
    "    return psg_path, hyp_path\n",
    "\n",
    "\n",
    "def get_subject_cores_for_mode(\n",
    "    manifest_path, execution_mode, debug_max_subjects, random_state\n",
    "):\n",
    "    \"\"\"Obtiene los cores de sujetos a usar según el modo de ejecución.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    all_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(all_cores)\n",
    "\n",
    "    if execution_mode == \"debug\" and debug_max_subjects:\n",
    "        selected_cores = set(all_cores[:debug_max_subjects])\n",
    "        print(f\"[DEBUG] Usando {len(selected_cores)}/{len(all_cores)} sujetos\")\n",
    "    else:\n",
    "        selected_cores = set(all_cores)\n",
    "        print(f\"[FULL] Usando todos los {len(selected_cores)} sujetos\")\n",
    "\n",
    "    return selected_cores\n",
    "\n",
    "\n",
    "def iter_sessions(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Itera sesiones entregando rutas resueltas (opcionalmente filtrando sujetos).\"\"\"\n",
    "\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    if allowed_cores is not None:\n",
    "        manifest_ok = manifest_ok[\n",
    "            manifest_ok[\"subject_id\"].apply(extract_subject_core).isin(allowed_cores)\n",
    "        ]\n",
    "\n",
    "    manifest_dir = Path(manifest_path).parent\n",
    "    dataset_dir_name = (\n",
    "        \"sleep_trimmed_resamp200\"\n",
    "        if (manifest_dir / \"sleep_trimmed_resamp200\").exists()\n",
    "        else \"sleep_trimmed_spt\"\n",
    "        if (manifest_dir / \"sleep_trimmed_spt\").exists()\n",
    "        else \"sleep_trimmed\"\n",
    "    )\n",
    "\n",
    "    total_sessions = len(manifest_ok)\n",
    "    print(f\"\\nProcesando {total_sessions} sesiones...\")\n",
    "\n",
    "    for i, (_, row) in enumerate(manifest_ok.iterrows(), start=1):\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        subject_core = extract_subject_core(subject_id)\n",
    "        if allowed_cores is not None and subject_core not in allowed_cores:\n",
    "            continue\n",
    "        psg_path, hyp_path = resolve_paths(row, manifest_dir, dataset_dir_name)\n",
    "\n",
    "        if not psg_path.exists() or not hyp_path.exists():\n",
    "            continue\n",
    "\n",
    "        yield i, total_sessions, subject_id, subject_core, psg_path, hyp_path\n",
    "\n",
    "\n",
    "def update_running_stats(stats, epochs):\n",
    "    \"\"\"Acumula sumas y sumas cuadradas por canal.\"\"\"\n",
    "\n",
    "    if stats is None:\n",
    "        stats = {\"n\": 0, \"sum\": None, \"sumsq\": None}\n",
    "\n",
    "    if stats[\"sum\"] is None:\n",
    "        stats[\"sum\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "        stats[\"sumsq\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "\n",
    "    channel_sum = epochs.sum(axis=(0, 2))\n",
    "    channel_sumsq = (epochs**2).sum(axis=(0, 2))\n",
    "    stats[\"n\"] += epochs.shape[0] * epochs.shape[2]\n",
    "    stats[\"sum\"] += channel_sum\n",
    "    stats[\"sumsq\"] += channel_sumsq\n",
    "    return stats\n",
    "\n",
    "\n",
    "def finalize_running_stats(stats, std_epsilon=1e-6):\n",
    "    \"\"\"Finaliza el cálculo de estadísticas running.\n",
    "\n",
    "    Usa fórmula de varianza: Var(X) = E[X²] - E[X]²\n",
    "    \"\"\"\n",
    "    mean = stats[\"sum\"] / stats[\"n\"]\n",
    "    var = stats[\"sumsq\"] / stats[\"n\"] - mean**2\n",
    "    # Asegurar varianza no negativa (puede ocurrir por errores numéricos)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + std_epsilon)  # Usar std_epsilon consistentemente\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_all_data_to_ram(manifest_path, epoch_length, sfreq, allowed_cores):\n",
    "    \"\"\"Carga todos los datos en RAM (modo debug). Retorna arrays numpy y estadísticas.\"\"\"\n",
    "\n",
    "    print(\"\\n[DEBUG] Cargando datos en RAM...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_epochs = []\n",
    "    all_stages = []\n",
    "    all_subject_cores = []\n",
    "    expected_channels = None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores=allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        elif list(ch_names) != expected_channels:\n",
    "            raise ValueError(\n",
    "                f\"Canales inconsistentes: esperado {expected_channels}, encontrado {list(ch_names)} en {psg_path}\"\n",
    "            )\n",
    "\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                nan_inf_count += 1\n",
    "                continue\n",
    "\n",
    "            all_epochs.append(epoch)\n",
    "            all_stages.append(stage)\n",
    "            all_subject_cores.append(subject_core)\n",
    "\n",
    "        if i % 10 == 0 or i == total:\n",
    "            print(f\"   Cargando: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf\")\n",
    "\n",
    "    # Convertir a arrays\n",
    "    X = np.array(all_epochs, dtype=np.float32)\n",
    "    y = np.array([STAGE_ORDER.index(s) for s in all_stages], dtype=np.int32)\n",
    "    subject_cores = np.array(all_subject_cores)\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[OK] Datos cargados en {load_time:.1f} segundos\")\n",
    "    print(f\"   Shape X: {X.shape}\")\n",
    "    print(f\"   Shape y: {y.shape}\")\n",
    "    print(f\"   Memoria: {X.nbytes / 1024**3:.2f} GB\")\n",
    "\n",
    "    return X, y, subject_cores, expected_channels\n",
    "\n",
    "\n",
    "def split_data_by_subject(X, y, subject_cores, test_size, val_size, random_state):\n",
    "    \"\"\"Divide datos por sujeto (sin data leakage).\"\"\"\n",
    "\n",
    "    unique_cores = np.unique(subject_cores)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(unique_cores)\n",
    "\n",
    "    n_test = max(1, int(len(unique_cores) * test_size))\n",
    "    n_val = max(1, int(len(unique_cores) * val_size))\n",
    "\n",
    "    test_cores = set(unique_cores[:n_test])\n",
    "    val_cores = set(unique_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(unique_cores[n_test + n_val :])\n",
    "\n",
    "    train_mask = np.isin(subject_cores, list(train_cores))\n",
    "    val_mask = np.isin(subject_cores, list(val_cores))\n",
    "    test_mask = np.isin(subject_cores, list(test_cores))\n",
    "\n",
    "    return (\n",
    "        X[train_mask],\n",
    "        y[train_mask],\n",
    "        X[val_mask],\n",
    "        y[val_mask],\n",
    "        X[test_mask],\n",
    "        y[test_mask],\n",
    "        train_cores,\n",
    "        val_cores,\n",
    "        test_cores,\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_data(X_train, X_val, X_test, clip_value, std_epsilon):\n",
    "    \"\"\"Normaliza datos usando estadísticas de train (z-score por canal).\n",
    "\n",
    "    Siguiendo estándares científicos:\n",
    "    - Estadísticas calculadas SOLO en train para evitar data leakage\n",
    "    - Z-score: (x - mean) / std\n",
    "    - Clipping para manejar outliers\n",
    "\n",
    "    NOTA: Usa sqrt(var + epsilon) para consistencia con modo streaming.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcular mean/std por canal usando train\n",
    "    mean_ch = X_train.mean(axis=(0, 2))\n",
    "    var_ch = X_train.var(axis=(0, 2))\n",
    "    # Usar sqrt(var + epsilon) para consistencia con finalize_running_stats\n",
    "    std_ch = np.sqrt(var_ch + std_epsilon)\n",
    "\n",
    "    # Normalizar (z-score)\n",
    "    X_train_norm = (X_train - mean_ch[None, :, None]) / std_ch[None, :, None]\n",
    "    X_val_norm = (X_val - mean_ch[None, :, None]) / std_ch[None, :, None]\n",
    "    X_test_norm = (X_test - mean_ch[None, :, None]) / std_ch[None, :, None]\n",
    "\n",
    "    # Clip\n",
    "    X_train_norm = np.clip(X_train_norm, -clip_value, clip_value)\n",
    "    X_val_norm = np.clip(X_val_norm, -clip_value, clip_value)\n",
    "    X_test_norm = np.clip(X_test_norm, -clip_value, clip_value)\n",
    "\n",
    "    return X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCIONES PARA MODO STREAMING (TFRecord)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def pass1_stats(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Primera pasada: mean/std por canal y conteo de clases (solo sujetos permitidos).\"\"\"\n",
    "\n",
    "    stats = None\n",
    "    class_counts = Counter()\n",
    "    input_shape = None\n",
    "    expected_channels = None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores=allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        elif list(ch_names) != expected_channels:\n",
    "            raise ValueError(\n",
    "                f\"Canales inconsistentes: esperado {expected_channels}, encontrado {list(ch_names)} en {psg_path}\"\n",
    "            )\n",
    "\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        valid_mask = [s in STAGE_ORDER for s in stages]\n",
    "        if not any(valid_mask):\n",
    "            continue\n",
    "\n",
    "        valid_epochs = epochs[valid_mask]\n",
    "        valid_stages = [s for s in stages if s in STAGE_ORDER]\n",
    "\n",
    "        finite_mask = np.all(np.isfinite(valid_epochs), axis=(1, 2))\n",
    "        if not np.all(finite_mask):\n",
    "            nan_inf_count += np.sum(~finite_mask)\n",
    "            valid_epochs = valid_epochs[finite_mask]\n",
    "            valid_stages = [s for s, m in zip(valid_stages, finite_mask) if m]\n",
    "\n",
    "        if len(valid_epochs) == 0:\n",
    "            continue\n",
    "\n",
    "        if input_shape is None and len(valid_epochs) > 0:\n",
    "            input_shape = valid_epochs.shape[1:]\n",
    "\n",
    "        stats = update_running_stats(stats, valid_epochs)\n",
    "        class_counts.update(valid_stages)\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 1: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf en pasada 1\")\n",
    "\n",
    "    assert input_shape is not None, \"No se encontraron epochs validos\"\n",
    "    mean, std = finalize_running_stats(\n",
    "        stats, std_epsilon=CONFIG.get(\"std_epsilon\", 1e-6)\n",
    "    )\n",
    "    return mean, std, class_counts, input_shape, expected_channels\n",
    "\n",
    "\n",
    "def make_example(x, y):\n",
    "    feature = {\n",
    "        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x.ravel())),\n",
    "        \"y\": tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n",
    "    }\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    ).SerializeToString()\n",
    "\n",
    "\n",
    "def assign_subject_splits(\n",
    "    manifest_path, test_size, val_size, random_state, allowed_cores=None\n",
    "):\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    subject_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "\n",
    "    if allowed_cores is not None:\n",
    "        subject_cores = np.array([c for c in subject_cores if c in allowed_cores])\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(subject_cores)\n",
    "\n",
    "    n_test = max(1, int(len(subject_cores) * test_size))\n",
    "    n_val = max(1, int(len(subject_cores) * val_size))\n",
    "\n",
    "    test_cores = set(subject_cores[:n_test])\n",
    "    val_cores = set(subject_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(subject_cores[n_test + n_val :])\n",
    "\n",
    "    split_map = dict.fromkeys(train_cores, \"train\")\n",
    "    split_map.update(dict.fromkeys(val_cores, \"val\"))\n",
    "    split_map.update(dict.fromkeys(test_cores, \"test\"))\n",
    "\n",
    "    return split_map, {\n",
    "        \"train\": len(train_cores),\n",
    "        \"val\": len(val_cores),\n",
    "        \"test\": len(test_cores),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_tfrecord_splits(\n",
    "    manifest_path,\n",
    "    mean,\n",
    "    std,\n",
    "    split_map,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    tfrecord_dir,\n",
    "    expected_channels=None,\n",
    "):\n",
    "    tfrecord_dir = Path(tfrecord_dir)\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {\n",
    "        \"train\": tf.io.TFRecordWriter(str(tfrecord_dir / \"train.tfrecord\")),\n",
    "        \"val\": tf.io.TFRecordWriter(str(tfrecord_dir / \"val.tfrecord\")),\n",
    "        \"test\": tf.io.TFRecordWriter(str(tfrecord_dir / \"test.tfrecord\")),\n",
    "    }\n",
    "\n",
    "    counts = Counter()\n",
    "    session_counts = Counter()\n",
    "    subject_sets = {k: set() for k in writers}\n",
    "    skipped_nan_inf = 0\n",
    "\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "    allowed_cores = set(split_map.keys())\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores=allowed_cores\n",
    "    ):\n",
    "        split = split_map.get(subject_core)\n",
    "        if split is None:\n",
    "            continue\n",
    "        session_counts[split] += 1\n",
    "        subject_sets[split].add(subject_core)\n",
    "\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "\n",
    "        if expected_channels is not None and list(ch_names) != expected_channels:\n",
    "            raise ValueError(\n",
    "                f\"Canales inconsistentes en {psg_path}: esperado {expected_channels}, encontrado {list(ch_names)}\"\n",
    "            )\n",
    "\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "\n",
    "            y = STAGE_ORDER.index(stage)\n",
    "            # std ya incluye epsilon desde finalize_running_stats (sqrt(var + epsilon))\n",
    "            # No agregar epsilon adicional aquí para consistencia con modo RAM\n",
    "            x = (epoch - mean[:, None]) / std[:, None]\n",
    "            x = np.clip(x, -clip_value, clip_value)\n",
    "\n",
    "            if not np.all(np.isfinite(x)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "\n",
    "            writers[split].write(make_example(x.astype(np.float32), y))\n",
    "            counts[split] += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 2 ({split}): {i}/{total} sesiones\")\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "\n",
    "    if skipped_nan_inf > 0:\n",
    "        print(\n",
    "            f\"[WARN] Se descartaron {skipped_nan_inf} epochs con NaN/Inf al escribir TFRecords\"\n",
    "        )\n",
    "\n",
    "    tf_paths = {split: str(tfrecord_dir / f\"{split}.tfrecord\") for split in writers}\n",
    "    subject_counts = {split: len(subject_sets[split]) for split in subject_sets}\n",
    "    return tf_paths, counts, session_counts, subject_counts\n",
    "\n",
    "\n",
    "def make_dataset(tfrecord_path, input_shape, batch_size, shuffle=False, repeat=False):\n",
    "    feature_description = {\n",
    "        \"x\": tf.io.FixedLenFeature([input_shape[0] * input_shape[1]], tf.float32),\n",
    "        \"y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "\n",
    "    def _parse(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        x = tf.reshape(example[\"x\"], input_shape)\n",
    "        x = tf.clip_by_value(x, -clip_value, clip_value)\n",
    "        x = tf.where(tf.math.is_finite(x), x, tf.zeros_like(x))\n",
    "        y = tf.cast(example[\"y\"], tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    ds = tf.data.TFRecordDataset([tfrecord_path], num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            CONFIG[\"shuffle_buffer\"],\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EJECUTAR PIPELINE SEGÚN MODO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n[INFO] Modo de ejecución: {CONFIG['execution_mode'].upper()}\")\n",
    "\n",
    "# Obtener sujetos a usar según modo\n",
    "selected_cores = get_subject_cores_for_mode(\n",
    "    CONFIG[\"manifest_path\"],\n",
    "    CONFIG[\"execution_mode\"],\n",
    "    CONFIG[\"debug_max_subjects\"],\n",
    "    CONFIG[\"random_state\"],\n",
    ")\n",
    "\n",
    "if CONFIG[\"streaming\"]:\n",
    "    # =========== MODO FULL: TFRecord ===========\n",
    "    print(\"\\n[FULL] Iniciando pipeline TFRecord (streaming)...\")\n",
    "\n",
    "    tfrecord_path = Path(CONFIG[\"tfrecord_dir\"])\n",
    "    if tfrecord_path.exists():\n",
    "        print(f\"[INFO] Limpiando TFRecords anteriores en {tfrecord_path}\")\n",
    "        shutil.rmtree(tfrecord_path)\n",
    "\n",
    "    # Asignar splits\n",
    "    split_map, split_subjects = assign_subject_splits(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"test_size\"],\n",
    "        CONFIG[\"val_size\"],\n",
    "        CONFIG[\"random_state\"],\n",
    "        allowed_cores=selected_cores,\n",
    "    )\n",
    "    train_cores = {core for core, split in split_map.items() if split == \"train\"}\n",
    "    val_cores = {core for core, split in split_map.items() if split == \"val\"}\n",
    "    test_cores = {core for core, split in split_map.items() if split == \"test\"}\n",
    "\n",
    "    # Calcular estadísticas SOLO en train (evita data leakage)\n",
    "    mean_ch, std_ch, class_counts_train, input_shape, expected_channels = pass1_stats(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        allowed_cores=train_cores,\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Estadísticas calculadas\")\n",
    "    print(f\"   mean: {mean_ch}\")\n",
    "    print(f\"   std:  {std_ch}\")\n",
    "\n",
    "    if np.any(std_ch < 1e-6):\n",
    "        print(\"[WARN] Algunos canales tienen std muy bajo, ajustando...\")\n",
    "        std_ch = np.maximum(std_ch, 1e-6)\n",
    "\n",
    "    # Escribir TFRecords\n",
    "    tfrecord_paths, split_counts, split_session_counts, split_subject_counts = (\n",
    "        build_tfrecord_splits(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            mean_ch,\n",
    "            std_ch,\n",
    "            split_map,\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            CONFIG[\"tfrecord_dir\"],\n",
    "            expected_channels=expected_channels,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] TFRecords generados:\")\n",
    "    for split, path in tfrecord_paths.items():\n",
    "        print(f\"   {split}: {path} ({split_counts[split]:,} epochs)\")\n",
    "\n",
    "    INPUT_SHAPE = input_shape\n",
    "    train_count = split_counts.get(\"train\", 0)\n",
    "    val_count = split_counts.get(\"val\", 0)\n",
    "    test_count = split_counts.get(\"test\", 0)\n",
    "\n",
    "    # Crear datasets\n",
    "    train_ds = make_dataset(\n",
    "        tfrecord_paths[\"train\"],\n",
    "        INPUT_SHAPE,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=True,\n",
    "        repeat=True,\n",
    "    )\n",
    "    val_ds = make_dataset(\n",
    "        tfrecord_paths[\"val\"],\n",
    "        INPUT_SHAPE,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "        repeat=True,\n",
    "    )\n",
    "    test_ds = make_dataset(\n",
    "        tfrecord_paths[\"test\"],\n",
    "        INPUT_SHAPE,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "        repeat=False,\n",
    "    )\n",
    "\n",
    "    USE_STREAMING = True\n",
    "\n",
    "else:\n",
    "    # =========== MODO DEBUG: RAM ===========\n",
    "    print(\"\\n[DEBUG] Iniciando pipeline en RAM...\")\n",
    "\n",
    "    # Cargar datos\n",
    "    X, y, subject_cores_arr, expected_channels = load_all_data_to_ram(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        selected_cores,\n",
    "    )\n",
    "\n",
    "    # Split por sujeto\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        train_cores,\n",
    "        val_cores,\n",
    "        test_cores,\n",
    "    ) = split_data_by_subject(\n",
    "        X,\n",
    "        y,\n",
    "        subject_cores_arr,\n",
    "        CONFIG[\"test_size\"],\n",
    "        CONFIG[\"val_size\"],\n",
    "        CONFIG[\"random_state\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] División por sujeto:\")\n",
    "    print(f\"   Train: {len(X_train):,} epochs ({len(train_cores)} sujetos)\")\n",
    "    print(f\"   Val:   {len(X_val):,} epochs ({len(val_cores)} sujetos)\")\n",
    "    print(f\"   Test:  {len(X_test):,} epochs ({len(test_cores)} sujetos)\")\n",
    "\n",
    "    # Normalizar\n",
    "    X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch = normalize_data(\n",
    "        X_train,\n",
    "        X_val,\n",
    "        X_test,\n",
    "        CONFIG[\"clip_value\"],\n",
    "        CONFIG[\"std_epsilon\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Normalización aplicada\")\n",
    "    print(f\"   mean: {mean_ch}\")\n",
    "    print(f\"   std:  {std_ch}\")\n",
    "\n",
    "    # Liberar memoria de datos no normalizados\n",
    "    del X, X_train, X_val, X_test\n",
    "    gc.collect()\n",
    "\n",
    "    INPUT_SHAPE = X_train_norm.shape[1:]\n",
    "    train_count = len(X_train_norm)\n",
    "    val_count = len(X_val_norm)\n",
    "    test_count = len(X_test_norm)\n",
    "\n",
    "    # Crear datasets tf.data (más eficiente que numpy arrays directos)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train))\n",
    "    train_ds = train_ds.shuffle(\n",
    "        min(10000, train_count),\n",
    "        seed=CONFIG[\"random_state\"],\n",
    "        reshuffle_each_iteration=True,  # Importante: shuffle diferente cada epoch\n",
    "    )\n",
    "    train_ds = train_ds.batch(CONFIG[\"effective_batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val))\n",
    "    val_ds = val_ds.batch(CONFIG[\"effective_batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_norm, y_test))\n",
    "    test_ds = test_ds.batch(CONFIG[\"effective_batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Conteo de clases para class weights\n",
    "    class_counts_train = Counter([STAGE_ORDER[yi] for yi in y_train])\n",
    "\n",
    "    # Variable para consistencia con modo streaming (no usado en RAM)\n",
    "    tfrecord_paths = None\n",
    "\n",
    "    USE_STREAMING = False\n",
    "\n",
    "# ============================================================\n",
    "# COMÚN: Class weights y verificación\n",
    "# ============================================================\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(STAGE_ORDER)  # fijar orden explícito para inverse_transform\n",
    "\n",
    "# Class weights\n",
    "counts_list = [class_counts_train.get(stage, 0) for stage in STAGE_ORDER]\n",
    "if any(c == 0 for c in counts_list):\n",
    "    print(\"[WARN] Alguna clase no apareció en train; ajustando conteos mínimos a 1\")\n",
    "    counts_list = [max(c, 1) for c in counts_list]\n",
    "\n",
    "y_for_weights = np.repeat(np.arange(len(STAGE_ORDER)), counts_list)\n",
    "class_weights_arr = compute_class_weight(\n",
    "    \"balanced\", classes=np.arange(len(STAGE_ORDER)), y=y_for_weights\n",
    ")\n",
    "\n",
    "class_weight_clip = CONFIG.get(\"class_weight_clip\", 1.5)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "class_weights = {\n",
    "    k: float(np.clip(v, 0.5, class_weight_clip)) for k, v in class_weights.items()\n",
    "}\n",
    "\n",
    "if not CONFIG.get(\"use_class_weights\", True):\n",
    "    class_weights = None\n",
    "    print(\"Class weights desactivados\")\n",
    "else:\n",
    "    print(f\"Class weights (clip 0.5-{class_weight_clip}): {class_weights}\")\n",
    "\n",
    "# Verificación de integridad EXHAUSTIVA\n",
    "print(\"\\n[CHECK] Verificando integridad de datos...\")\n",
    "\n",
    "# Verificar TODO el dataset de entrenamiento (no solo un batch)\n",
    "if not USE_STREAMING:\n",
    "    # Modo RAM: tenemos acceso directo a los arrays\n",
    "    print(\"   Verificando datos normalizados completos...\")\n",
    "    train_min = np.min(X_train_norm)\n",
    "    train_max = np.max(X_train_norm)\n",
    "    train_mean = np.mean(X_train_norm)\n",
    "    train_std = np.std(X_train_norm)\n",
    "    has_nan = np.any(np.isnan(X_train_norm))\n",
    "    has_inf = np.any(np.isinf(X_train_norm))\n",
    "\n",
    "    # Estadísticas por canal\n",
    "    for ch_idx in range(X_train_norm.shape[1]):\n",
    "        ch_data = X_train_norm[:, ch_idx, :]\n",
    "        ch_min, ch_max = np.min(ch_data), np.max(ch_data)\n",
    "        ch_std = np.std(ch_data)\n",
    "        print(\n",
    "            f\"   Canal {ch_idx}: rango=[{ch_min:.3f}, {ch_max:.3f}], std={ch_std:.3f}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\n   [TRAIN] Shape: {X_train_norm.shape}\")\n",
    "    print(f\"   [TRAIN] Rango global: [{train_min:.4f}, {train_max:.4f}]\")\n",
    "    print(f\"   [TRAIN] Mean: {train_mean:.4f}, Std: {train_std:.4f}\")\n",
    "    print(f\"   [TRAIN] Has NaN: {has_nan}\")\n",
    "    print(f\"   [TRAIN] Has Inf: {has_inf}\")\n",
    "\n",
    "    # Verificar distribución de valores extremos\n",
    "    extreme_count = np.sum(np.abs(X_train_norm) > 4.0)\n",
    "    total_values = X_train_norm.size\n",
    "    extreme_pct = extreme_count / total_values * 100\n",
    "    print(f\"   [TRAIN] Valores |x| > 4.0: {extreme_count:,} ({extreme_pct:.2f}%)\")\n",
    "\n",
    "    if has_nan or has_inf:\n",
    "        print(\"\\n   [ERROR] ¡Se detectaron NaN o Inf en los datos de entrenamiento!\")\n",
    "        print(\"   Esto causará problemas durante el entrenamiento.\")\n",
    "        raise ValueError(\"Datos de entrenamiento contienen NaN o Inf\")\n",
    "else:\n",
    "    # Modo streaming: verificar sample batch\n",
    "    sample_batch = next(iter(train_ds.take(1)))\n",
    "    x_sample, y_sample = sample_batch\n",
    "    print(f\"   Sample batch shape: {x_sample.shape}\")\n",
    "    print(\n",
    "        f\"   Sample x range: [{tf.reduce_min(x_sample).numpy():.4f}, {tf.reduce_max(x_sample).numpy():.4f}]\"\n",
    "    )\n",
    "    print(f\"   Sample x mean: {tf.reduce_mean(x_sample).numpy():.4f}\")\n",
    "    print(f\"   Has NaN: {tf.reduce_any(tf.math.is_nan(x_sample)).numpy()}\")\n",
    "    print(f\"   Has Inf: {tf.reduce_any(tf.math.is_inf(x_sample)).numpy()}\")\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"[OK] Pipeline {'STREAMING' if USE_STREAMING else 'RAM'} listo\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"   Train: {train_count:,} epochs\")\n",
    "print(f\"   Val:   {val_count:,} epochs\")\n",
    "print(f\"   Test:  {test_count:,} epochs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Distribución de clases\n",
    "print(\"\\n[INFO] Distribución de clases en Train:\")\n",
    "train_class_counts = [class_counts_train.get(stage, 0) for stage in STAGE_ORDER]\n",
    "total_train = sum(train_class_counts)\n",
    "print(\"   Clase     Epochs      %\")\n",
    "print(\"   \" + \"-\" * 25)\n",
    "for stage, count in zip(STAGE_ORDER, train_class_counts):\n",
    "    pct = count / total_train * 100\n",
    "    print(f\"   {stage:5s}   {count:7,d}   {pct:5.1f}%\")\n",
    "\n",
    "# Gráfico\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.bar(STAGE_ORDER, train_class_counts, color=sns.color_palette(\"husl\", 5))\n",
    "ax.set_xlabel(\"Estadio de sueño\")\n",
    "ax.set_ylabel(\"Número de epochs\")\n",
    "ax.set_title(\n",
    "    f\"Distribución de clases en Train ({CONFIG['execution_mode'].upper()} mode)\"\n",
    ")\n",
    "for bar, count in zip(bars, train_class_counts):\n",
    "    pct = count / total_train * 100\n",
    "    ax.annotate(\n",
    "        f\"{count:,}\\n({pct:.1f}%)\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/class_distribution_train.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN DE DIVISIÓN (info ya mostrada arriba)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[INFO] Modo: {CONFIG['execution_mode'].upper()}\")\n",
    "print(f\"   Train epochs: {train_count:,}\")\n",
    "print(f\"   Val epochs:   {val_count:,}\")\n",
    "print(f\"   Test epochs:  {test_count:,}\")\n",
    "print(f\"   Streaming: {USE_STREAMING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZACION Y PREPARACION (info ya mostrada arriba)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[INFO] Normalización aplicada {'en TFRecords' if USE_STREAMING else 'en RAM'}\")\n",
    "print(f\"   Mean por canal: {mean_ch}\")\n",
    "print(f\"   Std por canal:  {std_ch}\")\n",
    "print(f\"   Clases: {STAGE_ORDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfe977",
   "metadata": {},
   "source": [
    "## Arquitectura CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODELO CNN1D CON CONEXIONES RESIDUALES\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def build_cnn1d_model(\n",
    "    input_shape,\n",
    "    n_classes=5,\n",
    "    n_filters=32,\n",
    "    kernel_size=3,\n",
    "    dropout_rate=0.3,\n",
    "    lr_schedule=None,  # Acepta LearningRateSchedule o float\n",
    "    use_residual=True,\n",
    "    use_augmentation=False,\n",
    "):\n",
    "    \"\"\"Construye modelo CNN1D optimizado para sleep staging.\n",
    "\n",
    "    NOTA: Arquitectura simplificada para mayor estabilidad numérica.\n",
    "    - Usa ReLU en lugar de GELU (más estable)\n",
    "    - BatchNorm con momentum alto para estabilidad\n",
    "    - Inicialización conservadora\n",
    "    \"\"\"\n",
    "\n",
    "    # Input: (n_channels, n_samples) - float32 explícito\n",
    "    input_layer = keras.Input(shape=input_shape, name=\"input\", dtype=\"float32\")\n",
    "\n",
    "    # Transponer: (n_channels, n_samples) -> (n_samples, n_channels)\n",
    "    x = layers.Permute((2, 1))(input_layer)\n",
    "\n",
    "    # Data augmentation (solo durante training) - reducido para estabilidad\n",
    "    if use_augmentation:\n",
    "        x = layers.GaussianNoise(0.05)(x)  # Reducido de 0.1\n",
    "\n",
    "    # Bloque 1\n",
    "    x = layers.Conv1D(\n",
    "        n_filters,\n",
    "        kernel_size,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_uniform\",  # Más estable que he_normal\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)  # Default momentum=0.99\n",
    "    x = layers.Activation(\"relu\")(x)  # ReLU más estable que GELU\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Bloque 2 con residual\n",
    "    conv2_input = x\n",
    "    x = layers.Conv1D(\n",
    "        n_filters * 2,\n",
    "        kernel_size,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(momentum=0.99)(x)\n",
    "\n",
    "    if use_residual:\n",
    "        conv2_input_adj = layers.Conv1D(\n",
    "            n_filters * 2, 1, padding=\"same\", kernel_initializer=\"he_uniform\"\n",
    "        )(conv2_input)\n",
    "        conv2_input_adj = layers.BatchNormalization(momentum=0.99)(conv2_input_adj)\n",
    "        x = layers.Add()([x, conv2_input_adj])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Bloque 3 con residual\n",
    "    conv3_input = x\n",
    "    x = layers.Conv1D(\n",
    "        n_filters * 4,\n",
    "        kernel_size,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(momentum=0.99)(x)\n",
    "\n",
    "    if use_residual:\n",
    "        conv3_input_adj = layers.Conv1D(\n",
    "            n_filters * 4, 1, padding=\"same\", kernel_initializer=\"he_uniform\"\n",
    "        )(conv3_input)\n",
    "        conv3_input_adj = layers.BatchNormalization(momentum=0.99)(conv3_input_adj)\n",
    "        x = layers.Add()([x, conv3_input_adj])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Capas densas - simplificadas\n",
    "    x = layers.Dense(\n",
    "        64,  # Reducido de 128\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(momentum=0.99)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output - usar linear + float32 para estabilidad numérica\n",
    "    # La softmax se computa dentro de la loss con from_logits=True\n",
    "    output_layer = layers.Dense(\n",
    "        n_classes,\n",
    "        kernel_initializer=\"glorot_uniform\",  # Xavier para capa final\n",
    "        name=\"output\",\n",
    "        dtype=\"float32\",\n",
    "    )(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, outputs=output_layer, name=\"CNN1D_SleepStaging\"\n",
    "    )\n",
    "\n",
    "    # Compilar con from_logits=True para mejor estabilidad numérica\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule if lr_schedule is not None else 1e-5,\n",
    "            clipnorm=1.0,  # Gradient clipping por norma\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_lr_schedule(\n",
    "    initial_lr, min_lr, warmup_epochs, total_epochs, steps_per_epoch\n",
    "):\n",
    "    \"\"\"Crea un learning rate schedule con warmup lineal + cosine decay.\n",
    "\n",
    "    - Warmup: LR sube linealmente de min_lr a initial_lr durante warmup_epochs\n",
    "    - Cosine decay: LR baja suavemente de initial_lr a min_lr hasta el final\n",
    "    \"\"\"\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    decay_steps = max(total_steps - warmup_steps, 1)\n",
    "\n",
    "    class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"LR schedule con warmup lineal + cosine decay a nivel de steps.\"\"\"\n",
    "\n",
    "        def __init__(self, initial_lr, min_lr, warmup_steps, decay_steps):\n",
    "            super().__init__()\n",
    "            self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "            self.min_lr = tf.cast(min_lr, tf.float32)\n",
    "            self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "            self.decay_steps = tf.cast(decay_steps, tf.float32)\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "\n",
    "            # Linear warmup\n",
    "            warmup_progress = step / tf.maximum(self.warmup_steps, 1.0)\n",
    "            warmup_lr = self.min_lr + (self.initial_lr - self.min_lr) * tf.minimum(\n",
    "                warmup_progress, 1.0\n",
    "            )\n",
    "\n",
    "            # Cosine decay after warmup\n",
    "            decay_progress = (step - self.warmup_steps) / self.decay_steps\n",
    "            decay_progress = tf.minimum(tf.maximum(decay_progress, 0.0), 1.0)\n",
    "            cosine_decay = 0.5 * (\n",
    "                1.0 + tf.cos(tf.constant(np.pi, dtype=tf.float32) * decay_progress)\n",
    "            )\n",
    "            decay_lr = self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay\n",
    "\n",
    "            # Seleccionar fase\n",
    "            return tf.cond(\n",
    "                step < self.warmup_steps, lambda: warmup_lr, lambda: decay_lr\n",
    "            )\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"initial_lr\": float(self.initial_lr.numpy())\n",
    "                if hasattr(self.initial_lr, \"numpy\")\n",
    "                else float(self.initial_lr),\n",
    "                \"min_lr\": float(self.min_lr.numpy())\n",
    "                if hasattr(self.min_lr, \"numpy\")\n",
    "                else float(self.min_lr),\n",
    "                \"warmup_steps\": int(self.warmup_steps.numpy())\n",
    "                if hasattr(self.warmup_steps, \"numpy\")\n",
    "                else int(self.warmup_steps),\n",
    "                \"decay_steps\": int(self.decay_steps.numpy())\n",
    "                if hasattr(self.decay_steps, \"numpy\")\n",
    "                else int(self.decay_steps),\n",
    "            }\n",
    "\n",
    "    return WarmupCosineDecay(initial_lr, min_lr, warmup_steps, decay_steps)\n",
    "\n",
    "\n",
    "print(\"[OK] Arquitectura CNN1D definida\")\n",
    "print(\"[OK] Learning rate schedule (warmup + cosine decay) definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ceadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO (con estrategia multi-GPU si está disponible)\n",
    "# ============================================================\n",
    "\n",
    "input_shape = INPUT_SHAPE\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "# Limitar steps por epoch para evitar NaN ~batch 119-121\n",
    "MAX_STEPS_PER_EPOCH = 115  # aplicar a ambos modos\n",
    "\n",
    "full_steps_train = math.ceil(train_count / CONFIG[\"effective_batch_size\"])\n",
    "full_steps_val = (\n",
    "    math.ceil(val_count / CONFIG[\"effective_batch_size\"]) if val_count else 0\n",
    ")\n",
    "\n",
    "if USE_STREAMING:\n",
    "    steps_per_epoch_train = min(full_steps_train, MAX_STEPS_PER_EPOCH)\n",
    "    steps_per_epoch_val = (\n",
    "        min(full_steps_val, MAX_STEPS_PER_EPOCH) if full_steps_val else None\n",
    "    )\n",
    "else:\n",
    "    steps_per_epoch_train = min(full_steps_train, MAX_STEPS_PER_EPOCH)\n",
    "    steps_per_epoch_val = None\n",
    "\n",
    "# Crear learning rate schedule usando los steps efectivos\n",
    "lr_schedule = create_lr_schedule(\n",
    "    initial_lr=CONFIG[\"learning_rate_initial\"],\n",
    "    min_lr=CONFIG[\"learning_rate_min\"],\n",
    "    warmup_epochs=CONFIG[\"warmup_epochs\"],\n",
    "    total_epochs=CONFIG[\"epochs\"],\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Learning Rate Schedule:\")\n",
    "print(f\"   Initial LR: {CONFIG['learning_rate_initial']}\")\n",
    "print(f\"   Min LR: {CONFIG['learning_rate_min']}\")\n",
    "print(f\"   Warmup epochs: {CONFIG['warmup_epochs']}\")\n",
    "print(f\"   Total epochs: {CONFIG['epochs']}\")\n",
    "print(f\"   Steps per epoch (train): {steps_per_epoch_train}\")\n",
    "print(f\"   Warmup steps: {CONFIG['warmup_epochs'] * steps_per_epoch_train}\")\n",
    "if USE_STREAMING:\n",
    "    print(f\"   Full steps train: {full_steps_train} -> cap {MAX_STEPS_PER_EPOCH}\")\n",
    "    print(f\"   Full steps val:   {full_steps_val} -> using {steps_per_epoch_val}\")\n",
    "else:\n",
    "    print(f\"   Full steps train: {full_steps_train} -> cap {MAX_STEPS_PER_EPOCH}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_cnn1d_model(\n",
    "        input_shape=input_shape,\n",
    "        n_classes=len(STAGE_ORDER),\n",
    "        n_filters=CONFIG[\"n_filters\"],\n",
    "        kernel_size=CONFIG[\"kernel_size\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        lr_schedule=lr_schedule,\n",
    "        use_residual=CONFIG[\"use_residual\"],\n",
    "        use_augmentation=CONFIG[\"use_augmentation\"],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681d94d",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"cnn1d_{CONFIG['execution_mode']}_{timestamp}\"\n",
    "\n",
    "\n",
    "# Custom callback para métricas de sleep staging\n",
    "class SleepMetricsCallback(Callback):\n",
    "    \"\"\"Calcula F1-Macro y Kappa al final de cada epoch. Guarda mejor modelo por F1-Macro.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, val_ds, val_steps, stage_order, use_streaming=False, eval_every=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.val_steps = val_steps\n",
    "        self.stage_order = stage_order\n",
    "        self.use_streaming = use_streaming\n",
    "        self.eval_every = max(1, eval_every)\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_kappa = -1.0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        # Permite espaciar la evaluación para reducir tiempo (útil en Kaggle)\n",
    "        if (epoch + 1) % self.eval_every != 0:\n",
    "            return\n",
    "\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        if self.use_streaming:\n",
    "            # Streaming: iterar con límite de steps (dataset tiene repeat)\n",
    "            for batch_idx, (x_batch, y_batch) in enumerate(self.val_ds):\n",
    "                if batch_idx >= self.val_steps:\n",
    "                    break\n",
    "                y_pred_batch = self.model.predict(x_batch, verbose=0)\n",
    "                y_pred_list.append(np.argmax(y_pred_batch, axis=1))\n",
    "                y_true_list.append(y_batch.numpy())\n",
    "        else:\n",
    "            # RAM: iterar todo el dataset (sin repeat, recrea cada epoch)\n",
    "            for x_batch, y_batch in self.val_ds:\n",
    "                y_pred_batch = self.model.predict(x_batch, verbose=0)\n",
    "                y_pred_list.append(np.argmax(y_pred_batch, axis=1))\n",
    "                y_true_list.append(y_batch.numpy())\n",
    "\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        logs[\"val_kappa\"] = kappa\n",
    "        logs[\"val_f1_macro\"] = f1\n",
    "\n",
    "        if f1 > self.best_f1_macro:\n",
    "            self.best_f1_macro = f1\n",
    "            self.best_kappa = kappa\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print(f\" - val_f1_macro mejoró a {f1:.4f} (kappa={kappa:.4f})\")\n",
    "\n",
    "        print(f\" - val_f1_macro: {f1:.4f} - val_kappa: {kappa:.4f}\")\n",
    "\n",
    "    def restore_best_weights(self):\n",
    "        if self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            print(\n",
    "                f\"[OK] Restaurados pesos del mejor modelo \"\n",
    "                f\"(F1-Macro={self.best_f1_macro:.4f}, Kappa={self.best_kappa:.4f})\"\n",
    "            )\n",
    "\n",
    "\n",
    "class NaNDebugCallback(Callback):\n",
    "    \"\"\"Detecta NaN en loss. Hace diagnóstico pero NO detiene el entrenamiento.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nan_count = 0\n",
    "        self.last_valid_loss = None\n",
    "        self.first_nan_batch = None\n",
    "        self.diagnosed = False\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"loss\", 0)\n",
    "\n",
    "        if not (np.isnan(loss) or np.isinf(loss)):\n",
    "            self.last_valid_loss = loss\n",
    "            return\n",
    "\n",
    "        # NaN detectado\n",
    "        self.nan_count += 1\n",
    "\n",
    "        if self.first_nan_batch is None:\n",
    "            self.first_nan_batch = batch\n",
    "            print(\n",
    "                f\"\\n[WARN] Primer NaN en batch {batch} (loss={loss}, último válido={self.last_valid_loss})\"\n",
    "            )\n",
    "\n",
    "            # Diagnóstico detallado solo la primera vez\n",
    "            if not self.diagnosed:\n",
    "                self.diagnosed = True\n",
    "                try:\n",
    "                    max_weight = 0\n",
    "                    problem_layers = []\n",
    "                    for layer in self.model.layers:\n",
    "                        weights = layer.get_weights()\n",
    "                        for w in weights:\n",
    "                            w_max = np.max(np.abs(w))\n",
    "                            max_weight = max(max_weight, w_max)\n",
    "                            if np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
    "                                problem_layers.append(layer.name)\n",
    "                    print(f\"   Max |peso|: {max_weight:.2e}\")\n",
    "                    if problem_layers:\n",
    "                        print(f\"   Capas con NaN/Inf: {problem_layers}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Error en diagnóstico: {e}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.nan_count > 0:\n",
    "            print(\n",
    "                f\"   [INFO] Epoch {epoch+1}: {self.nan_count} batches con NaN (primero en batch {self.first_nan_batch})\"\n",
    "            )\n",
    "        self.nan_count = 0\n",
    "        self.first_nan_batch = None\n",
    "\n",
    "\n",
    "# Calcular val_steps para el callback\n",
    "if USE_STREAMING:\n",
    "    val_steps_for_callback = math.ceil(val_count / CONFIG[\"effective_batch_size\"])\n",
    "else:\n",
    "    val_steps_for_callback = None  # No se usa en modo RAM\n",
    "\n",
    "metrics_callback = SleepMetricsCallback(\n",
    "    val_ds=val_ds,\n",
    "    val_steps=val_steps_for_callback,\n",
    "    stage_order=STAGE_ORDER,\n",
    "    use_streaming=USE_STREAMING,\n",
    ")\n",
    "\n",
    "# NOTA: El orden de callbacks importa. metrics_callback debe estar antes de\n",
    "# EarlyStopping y ModelCheckpoint porque agrega val_f1_macro a los logs.\n",
    "callbacks = [\n",
    "    NaNDebugCallback(),  # Solo log, NUNCA detiene el entrenamiento\n",
    "    # NO usamos TerminateOnNaN - dejamos que el modelo se recupere\n",
    "    # NO usamos ReduceLROnPlateau - incompatible con LearningRateSchedule\n",
    "    metrics_callback,  # Agrega val_f1_macro y val_kappa a logs\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        patience=CONFIG[\"early_stopping_patience\"],\n",
    "        restore_best_weights=False,  # Usamos metrics_callback para esto\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{OUTPUT_PATH}/{model_name}_best.keras\",\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Modelo: {model_name}\")\n",
    "print(f\"Checkpoint: {OUTPUT_PATH}/{model_name}_best.keras\")\n",
    "print(\"[INFO] F1-Macro (selección) y Kappa (reporte) se calculan cada epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481af98",
   "metadata": {},
   "source": [
    "## Reanudacion desde Checkpoint (opcional)\n",
    "\n",
    "Si Kaggle se desconecto durante el entrenamiento, puedes reanudar desde el ultimo checkpoint.\n",
    "Solo ejecuta la siguiente celda si necesitas reanudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REANUDAR DESDE CHECKPOINT (ejecutar solo si es necesario)\n",
    "# ============================================================\n",
    "# Descomenta y ajusta el nombre del checkpoint si necesitas reanudar\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # Cambiar a True para reanudar\n",
    "CHECKPOINT_NAME = None  # Ejemplo: \"cnn1d_20251125_143022_best.keras\"\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and CHECKPOINT_NAME:\n",
    "    checkpoint_path = f\"{OUTPUT_PATH}/{CHECKPOINT_NAME}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"[INFO] Cargando modelo desde checkpoint: {checkpoint_path}\")\n",
    "        with strategy.scope():\n",
    "            model = keras.models.load_model(checkpoint_path)\n",
    "        print(\"[OK] Modelo cargado exitosamente\")\n",
    "        print(\"[INFO] Puedes continuar el entrenamiento ejecutando la celda de fit()\")\n",
    "        print(\"[INFO] El modelo continuara desde donde quedo\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Checkpoint no encontrado: {checkpoint_path}\")\n",
    "        print(f\"[INFO] Archivos disponibles en {OUTPUT_PATH}:\")\n",
    "        for f in os.listdir(OUTPUT_PATH):\n",
    "            if f.endswith(\".keras\"):\n",
    "                print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"[INFO] Modo normal: se usara el modelo recien creado\")\n",
    "    print(\"[INFO] Para reanudar desde checkpoint, cambia RESUME_FROM_CHECKPOINT=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nIniciando entrenamiento ({CONFIG['execution_mode'].upper()} mode)...\")\n",
    "print(f\"   Batch size efectivo: {CONFIG['effective_batch_size']}\")\n",
    "print(f\"   Epochs maximos: {CONFIG['epochs']}\")\n",
    "\n",
    "# Calcular steps según modo (ya calculados en celda de modelo)\n",
    "print(f\"   Steps train (efectivos): {steps_per_epoch_train}\")\n",
    "if USE_STREAMING:\n",
    "    print(f\"   Steps val (efectivos): {steps_per_epoch_val}\")\n",
    "else:\n",
    "    print(\"   Steps val: automático\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Verificar class_weights antes de entrenar\n",
    "if CONFIG.get(\"use_class_weights\", True) and class_weights:\n",
    "    print(f\"   Class weights: {class_weights}\")\n",
    "else:\n",
    "    print(\"   Class weights: DESACTIVADOS\")\n",
    "\n",
    "# Ajustar datasets para streaming (necesitan repeat)\n",
    "if USE_STREAMING:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        validation_steps=steps_per_epoch_val,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        class_weight=class_weights if CONFIG.get(\"use_class_weights\", True) else None,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    # Modo RAM: usar steps_per_epoch para evitar NaN en batch ~120\n",
    "    # Necesitamos repeat() en train_ds para que no se agote\n",
    "    train_ds_repeat = train_ds.repeat()\n",
    "    history = model.fit(\n",
    "        train_ds_repeat,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        class_weight=class_weights if CONFIG.get(\"use_class_weights\", True) else None,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "print(\n",
    "    f\"\\n[OK] Entrenamiento completado en {training_time / 60:.2f} minutos ({training_time / 3600:.2f} horas)\"\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Restaurar mejores pesos según F1-Macro\n",
    "metrics_callback.restore_best_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8319e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZAR CURVAS DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history[\"loss\"], label=\"Train Loss\", linewidth=2)\n",
    "axes[0, 0].plot(history.history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Loss durante entrenamiento\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history[\"accuracy\"], label=\"Train Acc\", linewidth=2)\n",
    "axes[0, 1].plot(history.history[\"val_accuracy\"], label=\"Val Acc\", linewidth=2)\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].set_title(\"Accuracy durante entrenamiento\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cohen's Kappa (para comparación con literatura)\n",
    "if \"val_kappa\" in history.history:\n",
    "    axes[1, 0].plot(\n",
    "        history.history[\"val_kappa\"], label=\"Val Kappa\", linewidth=2, color=\"green\"\n",
    "    )\n",
    "    axes[1, 0].axhline(\n",
    "        y=metrics_callback.best_kappa,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best Kappa={metrics_callback.best_kappa:.4f}\",\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"Cohen's Kappa\")\n",
    "    axes[1, 0].set_title(\"Cohen's Kappa (para comparación con literatura)\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(\n",
    "        0.5, 0.5, \"Kappa no disponible\", ha=\"center\", va=\"center\", fontsize=12\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Cohen's Kappa\")\n",
    "\n",
    "# F1 Macro (métrica de selección de modelo)\n",
    "if \"val_f1_macro\" in history.history:\n",
    "    axes[1, 1].plot(\n",
    "        history.history[\"val_f1_macro\"],\n",
    "        label=\"Val F1-Macro\",\n",
    "        linewidth=2,\n",
    "        color=\"purple\",\n",
    "    )\n",
    "    axes[1, 1].axhline(\n",
    "        y=metrics_callback.best_f1_macro,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best F1={metrics_callback.best_f1_macro:.4f}\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"F1-Macro\")\n",
    "    axes[1, 1].set_title(\"F1-Macro (métrica de selección)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(\n",
    "        0.5, 0.5, \"F1-Macro no disponible\", ha=\"center\", va=\"center\", fontsize=12\n",
    "    )\n",
    "    axes[1, 1].set_title(\"F1-Macro\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b28f8b",
   "metadata": {},
   "source": [
    "## Evaluacion en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACION EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEvaluando en Test Set (tf.data)...\")\n",
    "\n",
    "\n",
    "def collect_labels(ds):\n",
    "    return np.concatenate(\n",
    "        list(ds.map(lambda _x, y: y).unbatch().batch(1024).as_numpy_iterator())\n",
    "    )\n",
    "\n",
    "\n",
    "# Etiquetas reales (enteros, alineados con STAGE_ORDER)\n",
    "y_test_enc = collect_labels(test_ds).astype(int)\n",
    "\n",
    "# Predicciones (modelo devuelve logits, aplicar softmax para probabilidades)\n",
    "y_pred_logits = model.predict(test_ds, verbose=1)\n",
    "y_pred_proba = tf.nn.softmax(y_pred_logits, axis=1).numpy()\n",
    "y_pred_enc = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Metricas usando índices y orden explícito\n",
    "labels_idx = np.arange(len(STAGE_ORDER))\n",
    "accuracy = accuracy_score(y_test_enc, y_pred_enc)\n",
    "kappa = cohen_kappa_score(y_test_enc, y_pred_enc)\n",
    "f1_macro = f1_score(y_test_enc, y_pred_enc, average=\"macro\", zero_division=0)\n",
    "f1_weighted = f1_score(y_test_enc, y_pred_enc, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"RESULTADOS EN TEST SET\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}  <- Métrica principal en literatura\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}  <- Para comparar con datasets desbalanceados\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(\n",
    "    \"\\nNOTA: Según literatura (AASM), acuerdo inter-scorer humano tiene Kappa ~0.75-0.85\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408061b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_enc,\n",
    "        y_pred_enc,\n",
    "        labels=np.arange(len(STAGE_ORDER)),\n",
    "        target_names=STAGE_ORDER,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29471139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_test_enc, y_pred_enc, labels=np.arange(len(STAGE_ORDER)))\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absoluta\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicho\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confusión (Absoluta)\")\n",
    "\n",
    "# Normalizada\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_xlabel(\"Predicho\")\n",
    "axes[1].set_ylabel(\"Real\")\n",
    "axes[1].set_title(\"Matriz de Confusión (Normalizada)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f38ef",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZACION CON OPTUNA (opcional)\n",
    "# ============================================================\n",
    "\n",
    "# Optuna no soportado en modo streaming. Si necesitas tuning, desactiva streaming.\n",
    "if CONFIG[\"run_optimization\"]:\n",
    "    print(\"[WARN] Optuna no está implementado para el modo streaming TFRecord. Skip.\")\n",
    "    \"\"\"\n",
    "        # Hiperparametros a optimizar\n",
    "        n_filters = trial.suggest_categorical(\"n_filters\", [32, 64, 128])\n",
    "        kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7, 9])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.6)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "        # Crear modelo\n",
    "        with strategy.scope():\n",
    "            model = build_cnn1d_model(\n",
    "                input_shape=input_shape,\n",
    "                n_classes=len(STAGE_ORDER),\n",
    "                n_filters=n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                learning_rate=learning_rate,\n",
    "            )\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
    "            TFKerasPruningCallback(trial, \"val_loss\"),\n",
    "        ]\n",
    "\n",
    "        # Entrenar\n",
    "        model.fit(\n",
    "            X_train_norm,\n",
    "            y_train_enc,\n",
    "            validation_data=(X_val_norm, y_val_enc),\n",
    "            batch_size=batch_size * strategy.num_replicas_in_sync,\n",
    "            epochs=30,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Evaluar en validacion\n",
    "        y_val_pred = np.argmax(model.predict(X_val_norm, verbose=0), axis=1)\n",
    "        kappa = cohen_kappa_score(y_val_enc, y_val_pred)\n",
    "\n",
    "        # Limpiar\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        return kappa\n",
    "\n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"cnn1d_sleep_staging\",\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nIniciando optimizacion con {CONFIG['n_optuna_trials']} trials...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=CONFIG[\"n_optuna_trials\"], show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Optimizacion completada\")\n",
    "    print(f\"   Mejor Kappa: {study.best_value:.4f}\")\n",
    "    print(\"   Mejores hiperparametros:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    with open(f\"{OUTPUT_PATH}/optuna_cnn1d_results.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"best_value\": study.best_value,\n",
    "                \"best_params\": study.best_params,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\"\"\"\n",
    "else:\n",
    "    print(\n",
    "        \"[SKIP] Optimizacion deshabilitada. Cambiar CONFIG['run_optimization'] = True para ejecutar.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95114f3d",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR MODELO Y ARTEFACTOS\n",
    "# ============================================================\n",
    "\n",
    "# Guardar modelo completo\n",
    "model.save(f\"{OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "print(f\"[OK] Modelo guardado: {OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "\n",
    "# Guardar historial\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f\"{OUTPUT_PATH}/{model_name}_history.csv\", index=False)\n",
    "\n",
    "# Guardar resultados\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"config\": CONFIG,\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"kappa\": float(kappa),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"training_time_seconds\": float(training_time),\n",
    "        \"training_time_minutes\": float(training_time / 60),\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "        \"best_val_f1_macro\": float(metrics_callback.best_f1_macro),\n",
    "        \"best_val_kappa\": float(metrics_callback.best_kappa),\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": int(train_count),\n",
    "        \"val_samples\": int(val_count),\n",
    "        \"test_samples\": int(test_count),\n",
    "        \"tfrecord_paths\": tfrecord_paths if USE_STREAMING else None,\n",
    "        \"execution_mode\": CONFIG[\"execution_mode\"],\n",
    "    },\n",
    "    \"channel_stats\": {\"mean\": mean_ch.tolist(), \"std\": std_ch.tolist()},\n",
    "    \"label_encoder_classes\": list(le.classes_),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[OK] Resultados guardados: {OUTPUT_PATH}/{model_name}_results.json\")\n",
    "\n",
    "# Guardar LabelEncoder\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"   - {model_name}_final.keras (modelo)\")\n",
    "print(f\"   - {model_name}_best.keras (mejor checkpoint)\")\n",
    "print(f\"   - {model_name}_history.csv (historial)\")\n",
    "print(f\"   - {model_name}_results.json (metricas y config)\")\n",
    "print(f\"   - {model_name}_label_encoder.pkl (encoder)\")\n",
    "print(f\"   - {model_name}_training_curves.png\")\n",
    "print(f\"   - {model_name}_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5460876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO CNN1D COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResultados finales en Test Set:\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"\\nModelo guardado en: {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f199418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRIMIR ARTEFACTOS PARA DESCARGA\n",
    "# ============================================================\n",
    "\n",
    "zip_path = f\"{OUTPUT_PATH}/{model_name}_artifacts.zip\"\n",
    "exclude_extensions = (\".tfrecord\", \".zip\")  # Excluir TFRecords y otros zips\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_PATH):\n",
    "        fpath = os.path.join(OUTPUT_PATH, fname)\n",
    "        # Solo incluir archivos (no directorios) que empiecen con model_name\n",
    "        if (\n",
    "            os.path.isfile(fpath)\n",
    "            and fname.startswith(model_name)\n",
    "            and not fname.endswith(exclude_extensions)\n",
    "        ):\n",
    "            zf.write(fpath, arcname=fname)\n",
    "\n",
    "print(f\"[OK] Artefactos comprimidos en: {zip_path} (excluyendo TFRecords)\")\n",
    "print(\"Archivos incluidos:\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\" - {info.filename} ({info.file_size / 1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
