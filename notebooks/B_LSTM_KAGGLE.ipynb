{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0167e5e",
   "metadata": {},
   "source": [
    "# LSTM Bidireccional para Sleep Staging\n",
    "\n",
    "Este notebook entrena un modelo **LSTM Bidireccional** para clasificacion de estadios de sueno usando datos PSG **trimmed** (preprocesados y recortados del dataset Sleep-EDF).\n",
    "\n",
    "**Optimizado para Kaggle con 2x Tesla T4 (16GB VRAM cada una)**\n",
    "\n",
    "### Modos de ejecución:\n",
    "- **Debug** (`EXECUTION_MODE = \"debug\"`): Carga datos en RAM, usa subset de sujetos, ~5 min de setup; 20 min de corrida total\n",
    "- **Full** (`EXECUTION_MODE = \"full\"`): Streaming TFRecord, todos los sujetos, ~30 min de setup; 2 horas de corrida total\n",
    "\n",
    "### Caracteristicas:\n",
    "- LSTM Bidireccional con mecanismo de atencion opcional\n",
    "- Soporte multi-GPU con MirroredStrategy\n",
    "- Optimizacion de hiperparametros con Optuna (opcional)\n",
    "- **División train/val/test por SUJETOS (sin data leakage)** \n",
    "- Soporte para reanudar entrenamiento desde checkpoints\n",
    "- Semillas fijas para reproducibilidad (SEED=42)\n",
    "\n",
    "### Datos requeridos:\n",
    "- Dataset `sleep-edf-trimmed-f32` en Kaggle (https://www.kaggle.com/datasets/ignaciolinari/sleep-edf-trimmed-f32)\n",
    "  - `manifest_trimmed_spt.csv` (1 episodio por noche, 100 Hz)\n",
    "  - `sleep_trimmed_spt/psg/*.fif` (PSG a 100 Hz, float32)\n",
    "  - `sleep_trimmed_spt/hypnograms/*.csv` (anotaciones)\n",
    "- Si usas la versión 200 Hz, apunta a `manifest_trimmed_resamp200.csv` + carpeta `sleep_trimmed_resamp200/`.\n",
    "- Nota Kaggle: la versión 200 Hz puede consumir más VRAM; deja `sfreq=100` o usa el dataset 100 Hz para evitar OOM.\n",
    "- Si usas otro slug, actualiza `DATA_PATH` abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN INICIAL - Ejecutar primero\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Detectar si estamos en Kaggle\n",
    "IN_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Entorno: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "# Paths según entorno\n",
    "if IN_KAGGLE:\n",
    "    DATA_PATH = \"/kaggle/input/sleep-edf-trimmed-f32/sleep_trimmed_spt\"\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "else:\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../models\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8729bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR GPUs DISPONIBLES\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPUs disponibles:\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"\\n[OK] {len(gpus)} GPU(s) configuradas con memory growth\")\n",
    "else:\n",
    "    print(\"[WARN] No se detectaron GPUs. El entrenamiento sera lento.\")\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"\\nUsando MirroredStrategy con {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    if len(gpus) == 1:\n",
    "        print(\"\\nUsando estrategia por defecto (1 GPU)\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] Usando CPU - entrenamiento será lento\")\n",
    "\n",
    "print(\"[INFO] Mixed precision se configurará después de definir CONFIG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import gc  # noqa: E402\n",
    "import hashlib  # noqa: E402\n",
    "import json  # noqa: E402\n",
    "import logging  # noqa: E402\n",
    "import math  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import re  # noqa: E402\n",
    "import shutil  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "import zipfile  # noqa: E402\n",
    "from collections import Counter  # noqa: E402\n",
    "from datetime import datetime  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import seaborn as sns  # noqa: E402\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder  # noqa: E402\n",
    "from sklearn.utils.class_weight import compute_class_weight  # noqa: E402\n",
    "from tensorflow import keras  # noqa: E402\n",
    "from tensorflow.keras import layers  # noqa: E402\n",
    "from tensorflow.keras.callbacks import (  # noqa: E402\n",
    "    Callback,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde20d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALAR DEPENDENCIAS (solo en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import mne\n",
    "except ImportError:\n",
    "    if IN_KAGGLE:\n",
    "        print(\"Instalando dependencias...\")\n",
    "        %pip install -q mne\n",
    "        print(\"[OK] Dependencias instaladas\")\n",
    "        import mne\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b5cb4",
   "metadata": {},
   "source": [
    "## Configuracion del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIPERPARAMETROS - AJUSTAR SEGUN NECESIDAD\n",
    "# ============================================================\n",
    "\n",
    "EXECUTION_MODE = \"debug\"  # \"debug\" o \"full\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"execution_mode\": EXECUTION_MODE,\n",
    "    \"manifest_path\": (\n",
    "        \"/kaggle/input/sleep-edf-trimmed-f32/manifest_trimmed_spt.csv\"\n",
    "        if IN_KAGGLE\n",
    "        else f\"{DATA_PATH}/manifest_trimmed_spt.csv\"\n",
    "    ),\n",
    "    \"epoch_length\": 30.0,\n",
    "    \"sfreq\": 100,\n",
    "    \"debug_max_subjects\": 30,\n",
    "    \"limit_sessions\": None,\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    \"lstm_units\": 128,\n",
    "    \"dropout_rate\": 0.4,\n",
    "    \"bidirectional\": True,  # En sleep staging offline bidireccional es mejor. Para simular real-time, usar unidireccional.\n",
    "    \"use_attention\": True,  # La capa de atención agrega overhead pero suele mejorar rendimiento en N1.\n",
    "    \"learning_rate_initial\": 3e-4,\n",
    "    \"learning_rate_min\": 1e-6,\n",
    "    \"warmup_epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 300 if EXECUTION_MODE == \"full\" else 150,\n",
    "    \"use_class_weights\": True,\n",
    "    \"class_weight_clip\": 1.5,\n",
    "    \"std_epsilon\": 1e-6,\n",
    "    \"clip_value\": 5.0,\n",
    "    \"early_stopping_patience\": 40 if EXECUTION_MODE == \"full\" else 25,\n",
    "    \"use_mixed_precision\": False,\n",
    "    \"run_optimization\": False,\n",
    "    \"n_optuna_trials\": 30,\n",
    "    \"streaming\": EXECUTION_MODE == \"full\",\n",
    "    \"tfrecord_dir\": f\"{OUTPUT_PATH}/tfrecords_lstm\",\n",
    "    \"shuffle_buffer\": 5000,\n",
    "}\n",
    "\n",
    "CONFIG[\"effective_batch_size\"] = CONFIG[\"batch_size\"] * strategy.num_replicas_in_sync\n",
    "\n",
    "if CONFIG.get(\"use_mixed_precision\", False) and gpus:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(\"[OK] Mixed precision (float16) habilitado\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No se pudo habilitar mixed precision: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] Mixed precision desactivado (float32) para mayor estabilidad\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if EXECUTION_MODE == \"debug\":\n",
    "    print(\" MODO DEBUG: Carga rápida en RAM, subset de sujetos\")\n",
    "    print(f\"   Max sujetos: {CONFIG['debug_max_subjects']}\")\n",
    "else:\n",
    "    print(\" MODO FULL: Streaming TFRecord, todos los sujetos\")\n",
    "print(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nConfiguracion del experimento:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d67c6f",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f62c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "STAGE_CANONICAL = {\n",
    "    \"Sleep stage W\": \"W\",\n",
    "    \"Sleep stage 1\": \"N1\",\n",
    "    \"Sleep stage 2\": \"N2\",\n",
    "    \"Sleep stage 3\": \"N3\",\n",
    "    \"Sleep stage 4\": \"N3\",\n",
    "    \"Sleep stage R\": \"REM\",\n",
    "}\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "REQUIRED_CHANNELS = [\"EEG Fpz-Cz\", \"EEG Pz-Oz\"]\n",
    "OPTIONAL_CHANNELS = [\"EOG horizontal\", \"EMG submental\"]\n",
    "\n",
    "\n",
    "def extract_subject_core(subject_id):\n",
    "    \"\"\"Agrupa noches del mismo sujeto. SC4XXNy -> SC4XX\"\"\"\n",
    "    sid = str(subject_id)\n",
    "    match = re.match(r\"(SC4\\d{2})\", sid)\n",
    "    return match.group(1) if match else sid\n",
    "\n",
    "\n",
    "def load_psg_data(psg_path, channels=None, target_sfreq=None):\n",
    "    \"\"\"Carga datos PSG desde archivo .fif.\"\"\"\n",
    "    raw = mne.io.read_raw_fif(str(psg_path), preload=True, verbose=\"ERROR\")\n",
    "    available = set(raw.ch_names)\n",
    "    if channels is None:\n",
    "        missing_required = [ch for ch in REQUIRED_CHANNELS if ch not in available]\n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Canales faltantes: {missing_required}\")\n",
    "        channels = REQUIRED_CHANNELS.copy()\n",
    "        for ch in OPTIONAL_CHANNELS:\n",
    "            if ch in available:\n",
    "                channels.append(ch)\n",
    "    raw.pick(channels)\n",
    "    if target_sfreq and raw.info[\"sfreq\"] != target_sfreq:\n",
    "        raw.resample(target_sfreq)\n",
    "    return raw.get_data(), raw.info[\"sfreq\"], raw.ch_names\n",
    "\n",
    "\n",
    "def load_hypnogram(hyp_path):\n",
    "    df = pd.read_csv(hyp_path)\n",
    "    df[\"stage_canonical\"] = df[\"description\"].map(STAGE_CANONICAL)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_epochs(data, sfreq, epoch_length=30.0):\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // samples_per_epoch\n",
    "    epochs = [\n",
    "        data[:, i * samples_per_epoch : (i + 1) * samples_per_epoch]\n",
    "        for i in range(n_epochs)\n",
    "    ]\n",
    "    epoch_times = [i * epoch_length for i in range(n_epochs)]\n",
    "    return np.array(epochs), np.array(epoch_times)\n",
    "\n",
    "\n",
    "def assign_stages(epoch_times, hypnogram, epoch_length=30.0):\n",
    "    stages = []\n",
    "    for t in epoch_times:\n",
    "        epoch_center = t + epoch_length / 2\n",
    "        mask = (hypnogram[\"onset\"] <= epoch_center) & (\n",
    "            hypnogram[\"onset\"] + hypnogram[\"duration\"] > epoch_center\n",
    "        )\n",
    "        matched = hypnogram[mask]\n",
    "        stages.append(matched.iloc[0][\"stage_canonical\"] if len(matched) > 0 else None)\n",
    "    return stages\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PIPELINE DE DATOS (DEBUG: RAM / FULL: TFRecord)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def resolve_paths(row, manifest_dir, dataset_dir_name):\n",
    "    \"\"\"Resuelve rutas de PSG e hipnograma respetando Kaggle/local.\"\"\"\n",
    "    psg_path_str = row.get(\"psg_trimmed_path\", \"\")\n",
    "    hyp_path_str = row.get(\"hypnogram_trimmed_path\", \"\")\n",
    "    if pd.isna(psg_path_str):\n",
    "        psg_path_str = \"\"\n",
    "    if pd.isna(hyp_path_str):\n",
    "        hyp_path_str = \"\"\n",
    "    base_data_root = manifest_dir.parent\n",
    "\n",
    "    if psg_path_str and hyp_path_str:\n",
    "        if IN_KAGGLE:\n",
    "            psg_rel, hyp_rel = Path(psg_path_str), Path(hyp_path_str)\n",
    "            psg_anchor = next(\n",
    "                (\n",
    "                    i\n",
    "                    for i, p in enumerate(psg_rel.parts)\n",
    "                    if p.startswith(\"sleep_trimmed\")\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            hyp_anchor = next(\n",
    "                (\n",
    "                    i\n",
    "                    for i, p in enumerate(hyp_rel.parts)\n",
    "                    if p.startswith(\"sleep_trimmed\")\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if psg_anchor is not None and hyp_anchor is not None:\n",
    "                psg_path = Path(DATA_PATH) / Path(*psg_rel.parts[psg_anchor:])\n",
    "                hyp_path = Path(DATA_PATH) / Path(*hyp_rel.parts[hyp_anchor:])\n",
    "            else:\n",
    "                psg_path = Path(DATA_PATH) / dataset_dir_name / \"psg\" / psg_rel.name\n",
    "                hyp_path = (\n",
    "                    Path(DATA_PATH) / dataset_dir_name / \"hypnograms\" / hyp_rel.name\n",
    "                )\n",
    "        else:\n",
    "            psg_rel, hyp_rel = Path(psg_path_str), Path(hyp_path_str)\n",
    "            psg_path = (\n",
    "                base_data_root / psg_rel.relative_to(\"data\")\n",
    "                if psg_rel.parts and psg_rel.parts[0] == \"data\"\n",
    "                else manifest_dir / psg_rel\n",
    "                if not psg_rel.is_absolute()\n",
    "                else psg_rel\n",
    "            )\n",
    "            hyp_path = (\n",
    "                base_data_root / hyp_rel.relative_to(\"data\")\n",
    "                if hyp_rel.parts and hyp_rel.parts[0] == \"data\"\n",
    "                else manifest_dir / hyp_rel\n",
    "                if not hyp_rel.is_absolute()\n",
    "                else hyp_rel\n",
    "            )\n",
    "    else:\n",
    "        subset = row.get(\"subset\", \"sleep-cassette\")\n",
    "        version = row.get(\"version\", \"1.0.0\")\n",
    "        dataset_dir = manifest_dir / dataset_dir_name\n",
    "        psg_path = (\n",
    "            dataset_dir\n",
    "            / \"psg\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_raw.fif\"\n",
    "        )\n",
    "        hyp_path = (\n",
    "            dataset_dir\n",
    "            / \"hypnograms\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_annotations.csv\"\n",
    "        )\n",
    "    return psg_path, hyp_path\n",
    "\n",
    "\n",
    "def get_subject_cores_for_mode(\n",
    "    manifest_path, execution_mode, debug_max_subjects, random_state\n",
    "):\n",
    "    \"\"\"Obtiene los cores de sujetos a usar según el modo de ejecución.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    all_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(all_cores)\n",
    "    if execution_mode == \"debug\" and debug_max_subjects:\n",
    "        selected_cores = set(all_cores[:debug_max_subjects])\n",
    "        print(f\"[DEBUG] Usando {len(selected_cores)}/{len(all_cores)} sujetos\")\n",
    "    else:\n",
    "        selected_cores = set(all_cores)\n",
    "        print(f\"[FULL] Usando todos los {len(selected_cores)} sujetos\")\n",
    "    return selected_cores\n",
    "\n",
    "\n",
    "def iter_sessions(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Itera sesiones entregando rutas resueltas.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    if allowed_cores is not None:\n",
    "        manifest_ok = manifest_ok[\n",
    "            manifest_ok[\"subject_id\"].apply(extract_subject_core).isin(allowed_cores)\n",
    "        ]\n",
    "    manifest_dir = Path(manifest_path).parent\n",
    "    dataset_dir_name = (\n",
    "        \"sleep_trimmed_resamp200\"\n",
    "        if (manifest_dir / \"sleep_trimmed_resamp200\").exists()\n",
    "        else \"sleep_trimmed_spt\"\n",
    "        if (manifest_dir / \"sleep_trimmed_spt\").exists()\n",
    "        else \"sleep_trimmed\"\n",
    "    )\n",
    "    total_sessions = len(manifest_ok)\n",
    "    print(f\"\\nProcesando {total_sessions} sesiones...\")\n",
    "    for i, (_, row) in enumerate(manifest_ok.iterrows(), start=1):\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        subject_core = extract_subject_core(subject_id)\n",
    "        if allowed_cores is not None and subject_core not in allowed_cores:\n",
    "            continue\n",
    "        psg_path, hyp_path = resolve_paths(row, manifest_dir, dataset_dir_name)\n",
    "        if not psg_path.exists() or not hyp_path.exists():\n",
    "            continue\n",
    "        yield i, total_sessions, subject_id, subject_core, psg_path, hyp_path\n",
    "\n",
    "\n",
    "def update_running_stats(stats, epochs):\n",
    "    if stats is None:\n",
    "        stats = {\"n\": 0, \"sum\": None, \"sumsq\": None}\n",
    "    if stats[\"sum\"] is None:\n",
    "        stats[\"sum\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "        stats[\"sumsq\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "    stats[\"sum\"] += epochs.sum(axis=(0, 2))\n",
    "    stats[\"sumsq\"] += (epochs**2).sum(axis=(0, 2))\n",
    "    stats[\"n\"] += epochs.shape[0] * epochs.shape[2]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def finalize_running_stats(stats, std_epsilon=1e-6):\n",
    "    mean = stats[\"sum\"] / stats[\"n\"]\n",
    "    var = stats[\"sumsq\"] / stats[\"n\"] - mean**2\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + std_epsilon)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_all_data_to_ram(manifest_path, epoch_length, sfreq, allowed_cores):\n",
    "    \"\"\"Carga todos los datos en RAM (modo debug).\"\"\"\n",
    "    print(\"\\n[DEBUG] Cargando datos en RAM...\")\n",
    "    start_time = time.time()\n",
    "    all_epochs, all_stages, all_subject_cores = [], [], []\n",
    "    expected_channels = None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        elif list(ch_names) != expected_channels:\n",
    "            raise ValueError(f\"Canales inconsistentes: {list(ch_names)}\")\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                nan_inf_count += 1\n",
    "                continue\n",
    "            all_epochs.append(epoch)\n",
    "            all_stages.append(stage)\n",
    "            all_subject_cores.append(subject_core)\n",
    "        if i % 10 == 0 or i == total:\n",
    "            print(f\"   Cargando: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf\")\n",
    "    X = np.array(all_epochs, dtype=np.float32)\n",
    "    y = np.array([STAGE_ORDER.index(s) for s in all_stages], dtype=np.int32)\n",
    "    subject_cores = np.array(all_subject_cores)\n",
    "    print(\n",
    "        f\"[OK] Datos cargados en {time.time() - start_time:.1f}s. Shape: {X.shape}, Memoria: {X.nbytes / 1024**3:.2f} GB\"\n",
    "    )\n",
    "    return X, y, subject_cores, expected_channels\n",
    "\n",
    "\n",
    "def split_data_by_subject(X, y, subject_cores, test_size, val_size, random_state):\n",
    "    \"\"\"Divide datos por sujeto (sin data leakage).\"\"\"\n",
    "    unique_cores = np.unique(subject_cores)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(unique_cores)\n",
    "    n_test = max(1, int(len(unique_cores) * test_size))\n",
    "    n_val = max(1, int(len(unique_cores) * val_size))\n",
    "    test_cores = set(unique_cores[:n_test])\n",
    "    val_cores = set(unique_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(unique_cores[n_test + n_val :])\n",
    "    train_mask = np.isin(subject_cores, list(train_cores))\n",
    "    val_mask = np.isin(subject_cores, list(val_cores))\n",
    "    test_mask = np.isin(subject_cores, list(test_cores))\n",
    "    return (\n",
    "        X[train_mask],\n",
    "        y[train_mask],\n",
    "        X[val_mask],\n",
    "        y[val_mask],\n",
    "        X[test_mask],\n",
    "        y[test_mask],\n",
    "        train_cores,\n",
    "        val_cores,\n",
    "        test_cores,\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_data(X_train, X_val, X_test, clip_value, std_epsilon):\n",
    "    \"\"\"Normaliza datos usando estadísticas de train (z-score por canal).\"\"\"\n",
    "    mean_ch = X_train.mean(axis=(0, 2))\n",
    "    var_ch = X_train.var(axis=(0, 2))\n",
    "    std_ch = np.sqrt(var_ch + std_epsilon)\n",
    "    X_train_norm = np.clip(\n",
    "        (X_train - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    X_val_norm = np.clip(\n",
    "        (X_val - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    X_test_norm = np.clip(\n",
    "        (X_test - mean_ch[None, :, None]) / std_ch[None, :, None],\n",
    "        -clip_value,\n",
    "        clip_value,\n",
    "    )\n",
    "    return X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de pipeline definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES TFRECORD\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def pass1_stats(manifest_path, epoch_length, sfreq, allowed_cores=None):\n",
    "    \"\"\"Primera pasada: mean/std por canal y conteo de clases.\"\"\"\n",
    "    stats, class_counts, input_shape, expected_channels = None, Counter(), None, None\n",
    "    nan_inf_count = 0\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels is None:\n",
    "            expected_channels = list(ch_names)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        valid_mask = [s in STAGE_ORDER for s in stages]\n",
    "        if not any(valid_mask):\n",
    "            continue\n",
    "        valid_epochs = epochs[valid_mask]\n",
    "        valid_stages = [s for s in stages if s in STAGE_ORDER]\n",
    "\n",
    "        finite_mask = np.all(np.isfinite(valid_epochs), axis=(1, 2))\n",
    "        if not np.all(finite_mask):\n",
    "            nan_inf_count += np.sum(~finite_mask)\n",
    "            valid_epochs = valid_epochs[finite_mask]\n",
    "            valid_stages = [s for s, m in zip(valid_stages, finite_mask) if m]\n",
    "\n",
    "        if len(valid_epochs) == 0:\n",
    "            continue\n",
    "        if input_shape is None:\n",
    "            input_shape = valid_epochs.shape[1:]\n",
    "        stats = update_running_stats(stats, valid_epochs)\n",
    "        class_counts.update(valid_stages)\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 1: {i}/{total} sesiones\")\n",
    "\n",
    "    if nan_inf_count > 0:\n",
    "        print(f\"[WARN] Se descartaron {nan_inf_count} epochs con NaN/Inf\")\n",
    "    assert input_shape is not None, \"No se encontraron epochs validos\"\n",
    "    mean, std = finalize_running_stats(\n",
    "        stats, std_epsilon=CONFIG.get(\"std_epsilon\", 1e-6)\n",
    "    )\n",
    "    return mean, std, class_counts, input_shape, expected_channels\n",
    "\n",
    "\n",
    "def make_example(x, y):\n",
    "    feature = {\n",
    "        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x.ravel())),\n",
    "        \"y\": tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n",
    "    }\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    ).SerializeToString()\n",
    "\n",
    "\n",
    "def assign_subject_splits(\n",
    "    manifest_path, test_size, val_size, random_state, allowed_cores=None\n",
    "):\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    subject_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    if allowed_cores is not None:\n",
    "        subject_cores = np.array([c for c in subject_cores if c in allowed_cores])\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(subject_cores)\n",
    "    n_test = max(1, int(len(subject_cores) * test_size))\n",
    "    n_val = max(1, int(len(subject_cores) * val_size))\n",
    "    test_cores = set(subject_cores[:n_test])\n",
    "    val_cores = set(subject_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(subject_cores[n_test + n_val :])\n",
    "    split_map = dict.fromkeys(train_cores, \"train\")\n",
    "    split_map.update(dict.fromkeys(val_cores, \"val\"))\n",
    "    split_map.update(dict.fromkeys(test_cores, \"test\"))\n",
    "    return split_map, {\n",
    "        \"train\": len(train_cores),\n",
    "        \"val\": len(val_cores),\n",
    "        \"test\": len(test_cores),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_tfrecord_splits(\n",
    "    manifest_path,\n",
    "    mean,\n",
    "    std,\n",
    "    split_map,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    tfrecord_dir,\n",
    "    expected_channels=None,\n",
    "):\n",
    "    tfrecord_dir = Path(tfrecord_dir)\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "    writers = {\n",
    "        k: tf.io.TFRecordWriter(str(tfrecord_dir / f\"{k}.tfrecord\"))\n",
    "        for k in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "    counts, session_counts, subject_sets = (\n",
    "        Counter(),\n",
    "        Counter(),\n",
    "        {k: set() for k in writers},\n",
    "    )\n",
    "    skipped_nan_inf = 0\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "    allowed_cores = set(split_map.keys())\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, allowed_cores\n",
    "    ):\n",
    "        split = split_map.get(subject_core)\n",
    "        if split is None:\n",
    "            continue\n",
    "        session_counts[split] += 1\n",
    "        subject_sets[split].add(subject_core)\n",
    "        data, _, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        if expected_channels and list(ch_names) != expected_channels:\n",
    "            raise ValueError(f\"Canales inconsistentes en {psg_path}\")\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            if not np.all(np.isfinite(epoch)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "            y = STAGE_ORDER.index(stage)\n",
    "            x = np.clip((epoch - mean[:, None]) / std[:, None], -clip_value, clip_value)\n",
    "            if not np.all(np.isfinite(x)):\n",
    "                skipped_nan_inf += 1\n",
    "                continue\n",
    "            writers[split].write(make_example(x.astype(np.float32), y))\n",
    "            counts[split] += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 2 ({split}): {i}/{total} sesiones\")\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "    if skipped_nan_inf > 0:\n",
    "        print(f\"[WARN] Se descartaron {skipped_nan_inf} epochs con NaN/Inf\")\n",
    "    return (\n",
    "        {k: str(tfrecord_dir / f\"{k}.tfrecord\") for k in writers},\n",
    "        counts,\n",
    "        session_counts,\n",
    "        {k: len(subject_sets[k]) for k in subject_sets},\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(\n",
    "    tfrecord_path, input_shape, batch_size, shuffle=False, repeat=False, for_lstm=True\n",
    "):\n",
    "    \"\"\"Crea dataset desde TFRecord. for_lstm=True transpone para LSTM: (samples, channels).\"\"\"\n",
    "    feature_description = {\n",
    "        \"x\": tf.io.FixedLenFeature([input_shape[0] * input_shape[1]], tf.float32),\n",
    "        \"y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    clip_value = CONFIG.get(\"clip_value\", 5.0)\n",
    "\n",
    "    def _parse(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        x = tf.reshape(example[\"x\"], input_shape)\n",
    "        x = tf.clip_by_value(x, -clip_value, clip_value)\n",
    "        x = tf.where(tf.math.is_finite(x), x, tf.zeros_like(x))\n",
    "        if for_lstm:\n",
    "            x = tf.transpose(\n",
    "                x, perm=[1, 0]\n",
    "            )  # (channels, samples) -> (samples, channels)\n",
    "        y = tf.cast(example[\"y\"], tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    ds = tf.data.TFRecordDataset([tfrecord_path], num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            CONFIG[\"shuffle_buffer\"],\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_tfrecord_cache_key(manifest_path, config):\n",
    "    manifest_mtime = os.path.getmtime(manifest_path)\n",
    "    key_data = (\n",
    "        f\"{manifest_path}_{manifest_mtime}_{config['sfreq']}_{config['epoch_length']}\"\n",
    "        f\"_{config.get('clip_value', 'na')}_{config['random_state']}\"\n",
    "        f\"_{config['test_size']}_{config['val_size']}_{config.get('debug_max_subjects', 'all')}\"\n",
    "    )\n",
    "    return hashlib.md5(key_data.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones TFRecord definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d360aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EJECUTAR PIPELINE SEGÚN MODO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n[INFO] Modo de ejecución: {CONFIG['execution_mode'].upper()}\")\n",
    "\n",
    "selected_cores = get_subject_cores_for_mode(\n",
    "    CONFIG[\"manifest_path\"],\n",
    "    CONFIG[\"execution_mode\"],\n",
    "    CONFIG[\"debug_max_subjects\"],\n",
    "    CONFIG[\"random_state\"],\n",
    ")\n",
    "\n",
    "if CONFIG[\"streaming\"]:\n",
    "    print(\"\\n[FULL] Iniciando pipeline TFRecord (streaming)...\")\n",
    "    tfrecord_path = Path(CONFIG[\"tfrecord_dir\"])\n",
    "    cache_key = get_tfrecord_cache_key(CONFIG[\"manifest_path\"], CONFIG)\n",
    "    cache_marker = tfrecord_path / f\".cache_{cache_key}\"\n",
    "    cache_metadata_path = tfrecord_path / \"cache_metadata.json\"\n",
    "\n",
    "    if tfrecord_path.exists() and cache_marker.exists():\n",
    "        print(f\"[INFO] Reutilizando TFRecords existentes (cache: {cache_key})\")\n",
    "        REUSE_TFRECORDS = True\n",
    "    else:\n",
    "        if tfrecord_path.exists():\n",
    "            print(\"[INFO] Cache inválido, regenerando TFRecords...\")\n",
    "            shutil.rmtree(tfrecord_path)\n",
    "        REUSE_TFRECORDS = False\n",
    "\n",
    "    if REUSE_TFRECORDS:\n",
    "        with open(cache_metadata_path) as f:\n",
    "            cache_meta = json.load(f)\n",
    "        mean_ch = np.array(cache_meta[\"mean_ch\"], dtype=np.float32)\n",
    "        std_ch = np.array(cache_meta[\"std_ch\"], dtype=np.float32)\n",
    "        class_counts_train = Counter(cache_meta[\"class_counts_train\"])\n",
    "        input_shape = tuple(cache_meta[\"input_shape\"])\n",
    "        split_counts = cache_meta[\"split_counts\"]\n",
    "        tfrecord_paths = {\n",
    "            k: str(tfrecord_path / f\"{k}.tfrecord\") for k in [\"train\", \"val\", \"test\"]\n",
    "        }\n",
    "        split_map, _ = assign_subject_splits(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            CONFIG[\"test_size\"],\n",
    "            CONFIG[\"val_size\"],\n",
    "            CONFIG[\"random_state\"],\n",
    "            selected_cores,\n",
    "        )\n",
    "        train_cores = {c for c, s in split_map.items() if s == \"train\"}\n",
    "        val_cores = {c for c, s in split_map.items() if s == \"val\"}\n",
    "        test_cores = {c for c, s in split_map.items() if s == \"test\"}\n",
    "        print(f\"   mean: {mean_ch}, std: {std_ch}\")\n",
    "        print(\n",
    "            f\"   train: {split_counts['train']:,}, val: {split_counts['val']:,}, test: {split_counts['test']:,} epochs\"\n",
    "        )\n",
    "    else:\n",
    "        split_map, split_subjects = assign_subject_splits(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            CONFIG[\"test_size\"],\n",
    "            CONFIG[\"val_size\"],\n",
    "            CONFIG[\"random_state\"],\n",
    "            selected_cores,\n",
    "        )\n",
    "        train_cores = {c for c, s in split_map.items() if s == \"train\"}\n",
    "        val_cores = {c for c, s in split_map.items() if s == \"val\"}\n",
    "        test_cores = {c for c, s in split_map.items() if s == \"test\"}\n",
    "        mean_ch, std_ch, class_counts_train, input_shape, expected_channels = (\n",
    "            pass1_stats(\n",
    "                CONFIG[\"manifest_path\"],\n",
    "                CONFIG[\"epoch_length\"],\n",
    "                CONFIG[\"sfreq\"],\n",
    "                train_cores,\n",
    "            )\n",
    "        )\n",
    "        print(f\"\\n[OK] Estadísticas: mean={mean_ch}, std={std_ch}\")\n",
    "        if np.any(std_ch < 1e-6):\n",
    "            std_ch = np.maximum(std_ch, 1e-6)\n",
    "        tfrecord_paths, split_counts, _, _ = build_tfrecord_splits(\n",
    "            CONFIG[\"manifest_path\"],\n",
    "            mean_ch,\n",
    "            std_ch,\n",
    "            split_map,\n",
    "            CONFIG[\"epoch_length\"],\n",
    "            CONFIG[\"sfreq\"],\n",
    "            CONFIG[\"tfrecord_dir\"],\n",
    "            expected_channels,\n",
    "        )\n",
    "        print(f\"\\n[OK] TFRecords generados: {split_counts}\")\n",
    "        cache_meta = {\n",
    "            \"mean_ch\": mean_ch.tolist(),\n",
    "            \"std_ch\": std_ch.tolist(),\n",
    "            \"class_counts_train\": dict(class_counts_train),\n",
    "            \"input_shape\": list(input_shape),\n",
    "            \"split_counts\": split_counts,\n",
    "        }\n",
    "        tfrecord_path.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_metadata_path, \"w\") as f:\n",
    "            json.dump(cache_meta, f, indent=2)\n",
    "        cache_marker.touch()\n",
    "        print(f\"[OK] Cache guardado (key: {cache_key})\")\n",
    "\n",
    "    INPUT_SHAPE = (input_shape[1], input_shape[0])  # (samples, channels) for LSTM\n",
    "    train_count, val_count, test_count = (\n",
    "        split_counts.get(\"train\", 0),\n",
    "        split_counts.get(\"val\", 0),\n",
    "        split_counts.get(\"test\", 0),\n",
    "    )\n",
    "    train_ds = make_dataset(\n",
    "        tfrecord_paths[\"train\"],\n",
    "        input_shape,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=True,\n",
    "        repeat=True,\n",
    "        for_lstm=True,\n",
    "    )\n",
    "    val_ds = make_dataset(\n",
    "        tfrecord_paths[\"val\"],\n",
    "        input_shape,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "        repeat=True,\n",
    "        for_lstm=True,\n",
    "    )\n",
    "    test_ds = make_dataset(\n",
    "        tfrecord_paths[\"test\"],\n",
    "        input_shape,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "        repeat=False,\n",
    "        for_lstm=True,\n",
    "    )\n",
    "    USE_STREAMING = True\n",
    "else:\n",
    "    print(\"\\n[DEBUG] Iniciando pipeline en RAM...\")\n",
    "    X, y, subject_cores_arr, expected_channels = load_all_data_to_ram(\n",
    "        CONFIG[\"manifest_path\"], CONFIG[\"epoch_length\"], CONFIG[\"sfreq\"], selected_cores\n",
    "    )\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        train_cores,\n",
    "        val_cores,\n",
    "        test_cores,\n",
    "    ) = split_data_by_subject(\n",
    "        X,\n",
    "        y,\n",
    "        subject_cores_arr,\n",
    "        CONFIG[\"test_size\"],\n",
    "        CONFIG[\"val_size\"],\n",
    "        CONFIG[\"random_state\"],\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n[OK] División: Train={len(X_train):,}, Val={len(X_val):,}, Test={len(X_test):,}\"\n",
    "    )\n",
    "    X_train_norm, X_val_norm, X_test_norm, mean_ch, std_ch = normalize_data(\n",
    "        X_train, X_val, X_test, CONFIG[\"clip_value\"], CONFIG[\"std_epsilon\"]\n",
    "    )\n",
    "    print(f\"[OK] Normalización: mean={mean_ch}, std={std_ch}\")\n",
    "    del X, X_train, X_val, X_test\n",
    "    gc.collect()\n",
    "\n",
    "    # Transponer para LSTM: (epochs, channels, samples) -> (epochs, samples, channels)\n",
    "    X_train_lstm = np.transpose(X_train_norm, (0, 2, 1))\n",
    "    X_val_lstm = np.transpose(X_val_norm, (0, 2, 1))\n",
    "    X_test_lstm = np.transpose(X_test_norm, (0, 2, 1))\n",
    "\n",
    "    INPUT_SHAPE = X_train_lstm.shape[1:]\n",
    "    train_count, val_count, test_count = (\n",
    "        len(X_train_lstm),\n",
    "        len(X_val_lstm),\n",
    "        len(X_test_lstm),\n",
    "    )\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train_lstm, y_train))\n",
    "        .shuffle(\n",
    "            min(10000, train_count),\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "        .batch(CONFIG[\"effective_batch_size\"])\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val_lstm, y_val))\n",
    "        .batch(CONFIG[\"effective_batch_size\"])\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    test_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_test_lstm, y_test))\n",
    "        .batch(CONFIG[\"effective_batch_size\"])\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    class_counts_train = Counter([STAGE_ORDER[yi] for yi in y_train])\n",
    "    tfrecord_paths = None\n",
    "    USE_STREAMING = False\n",
    "\n",
    "# Class weights\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(STAGE_ORDER)\n",
    "counts_list = [class_counts_train.get(stage, 0) for stage in STAGE_ORDER]\n",
    "if any(c == 0 for c in counts_list):\n",
    "    counts_list = [max(c, 1) for c in counts_list]\n",
    "y_for_weights = np.repeat(np.arange(len(STAGE_ORDER)), counts_list)\n",
    "class_weights_arr = compute_class_weight(\n",
    "    \"balanced\", classes=np.arange(len(STAGE_ORDER)), y=y_for_weights\n",
    ")\n",
    "class_weight_clip = CONFIG.get(\"class_weight_clip\", 1.5)\n",
    "class_weights = (\n",
    "    {\n",
    "        k: float(np.clip(v, 0.5, class_weight_clip))\n",
    "        for k, v in enumerate(class_weights_arr)\n",
    "    }\n",
    "    if CONFIG.get(\"use_class_weights\", True)\n",
    "    else None\n",
    ")\n",
    "print(\n",
    "    f\"Class weights: {class_weights}\" if class_weights else \"Class weights desactivados\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"[OK] Pipeline {'STREAMING' if USE_STREAMING else 'RAM'} listo\")\n",
    "print(f\"   Input shape (samples, channels): {INPUT_SHAPE}\")\n",
    "print(f\"   Train: {train_count:,}, Val: {val_count:,}, Test: {test_count:,} epochs\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57856e0",
   "metadata": {},
   "source": [
    "## Arquitectura LSTM Bidireccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fece3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CAPA DE ATENCION PERSONALIZADA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"Capa de atencion simple para LSTM.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"attention_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"attention_bias\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = keras.backend.tanh(keras.backend.dot(x, self.W) + self.b)\n",
    "        a = keras.backend.softmax(e, axis=1)\n",
    "        return keras.backend.sum(x * a, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "\n",
    "print(\"[OK] Capa de Atencion definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEARNING RATE SCHEDULE Y MODELO LSTM\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def create_lr_schedule(\n",
    "    initial_lr, min_lr, warmup_epochs, total_epochs, steps_per_epoch\n",
    "):\n",
    "    \"\"\"Warmup lineal + cosine decay.\"\"\"\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    decay_steps = max(total_steps - warmup_steps, 1)\n",
    "\n",
    "    class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, initial_lr, min_lr, warmup_steps, decay_steps):\n",
    "            super().__init__()\n",
    "            self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "            self.min_lr = tf.cast(min_lr, tf.float32)\n",
    "            self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "            self.decay_steps = tf.cast(decay_steps, tf.float32)\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            warmup_progress = step / tf.maximum(self.warmup_steps, 1.0)\n",
    "            warmup_lr = self.min_lr + (self.initial_lr - self.min_lr) * tf.minimum(\n",
    "                warmup_progress, 1.0\n",
    "            )\n",
    "            decay_progress = tf.minimum(\n",
    "                tf.maximum((step - self.warmup_steps) / self.decay_steps, 0.0), 1.0\n",
    "            )\n",
    "            cosine_decay = 0.5 * (\n",
    "                1.0 + tf.cos(tf.constant(np.pi, dtype=tf.float32) * decay_progress)\n",
    "            )\n",
    "            decay_lr = self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay\n",
    "            return tf.cond(\n",
    "                step < self.warmup_steps, lambda: warmup_lr, lambda: decay_lr\n",
    "            )\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"initial_lr\": float(self.initial_lr),\n",
    "                \"min_lr\": float(self.min_lr),\n",
    "                \"warmup_steps\": int(self.warmup_steps),\n",
    "                \"decay_steps\": int(self.decay_steps),\n",
    "            }\n",
    "\n",
    "    return WarmupCosineDecay(initial_lr, min_lr, warmup_steps, decay_steps)\n",
    "\n",
    "\n",
    "def build_lstm_model(\n",
    "    input_shape,\n",
    "    n_classes=5,\n",
    "    lstm_units=128,\n",
    "    dropout_rate=0.4,\n",
    "    lr_schedule=None,\n",
    "    bidirectional=True,\n",
    "    use_attention=True,\n",
    "):\n",
    "    \"\"\"Construye modelo LSTM Bidireccional para sleep staging.\"\"\"\n",
    "    input_layer = keras.Input(shape=input_shape, name=\"input\", dtype=\"float32\")\n",
    "\n",
    "    lstm_1 = layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_1\",\n",
    "    )\n",
    "    x = (\n",
    "        layers.Bidirectional(lstm_1, name=\"bidirectional_1\")(input_layer)\n",
    "        if bidirectional\n",
    "        else lstm_1(input_layer)\n",
    "    )\n",
    "    x = layers.BatchNormalization(name=\"bn_1\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_1\")(x)\n",
    "\n",
    "    lstm_2 = layers.LSTM(\n",
    "        lstm_units // 2,\n",
    "        return_sequences=use_attention,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_2\",\n",
    "    )\n",
    "    x = (\n",
    "        layers.Bidirectional(lstm_2, name=\"bidirectional_2\")(x)\n",
    "        if bidirectional\n",
    "        else lstm_2(x)\n",
    "    )\n",
    "    x = layers.BatchNormalization(name=\"bn_2\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_2\")(x)\n",
    "\n",
    "    if use_attention:\n",
    "        x = AttentionLayer(name=\"attention\")(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        128,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        name=\"dense_1\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        64,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        name=\"dense_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_2\")(x)\n",
    "\n",
    "    output_layer = layers.Dense(\n",
    "        n_classes, kernel_initializer=\"glorot_uniform\", name=\"output\", dtype=\"float32\"\n",
    "    )(x)  # logits\n",
    "\n",
    "    model_name = (\n",
    "        (\"BiLSTM\" if bidirectional else \"LSTM\")\n",
    "        + (\"_Attention\" if use_attention else \"\")\n",
    "        + \"_SleepStaging\"\n",
    "    )\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=model_name)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule if lr_schedule else 1e-4, clipnorm=1.0\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"[OK] Arquitectura LSTM definida\")\n",
    "print(\"[OK] Learning rate schedule (warmup + cosine decay) definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO\n",
    "# ============================================================\n",
    "\n",
    "input_shape = INPUT_SHAPE\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "MAX_STEPS_PER_EPOCH = 115\n",
    "full_steps_train = math.ceil(train_count / CONFIG[\"effective_batch_size\"])\n",
    "full_steps_val = (\n",
    "    math.ceil(val_count / CONFIG[\"effective_batch_size\"]) if val_count else 0\n",
    ")\n",
    "\n",
    "if USE_STREAMING:\n",
    "    steps_per_epoch_train = min(full_steps_train, MAX_STEPS_PER_EPOCH)\n",
    "    steps_per_epoch_val = (\n",
    "        min(full_steps_val, MAX_STEPS_PER_EPOCH) if full_steps_val else None\n",
    "    )\n",
    "else:\n",
    "    steps_per_epoch_train = min(full_steps_train, MAX_STEPS_PER_EPOCH)\n",
    "    steps_per_epoch_val = None\n",
    "\n",
    "lr_schedule = create_lr_schedule(\n",
    "    initial_lr=CONFIG[\"learning_rate_initial\"],\n",
    "    min_lr=CONFIG[\"learning_rate_min\"],\n",
    "    warmup_epochs=CONFIG[\"warmup_epochs\"],\n",
    "    total_epochs=CONFIG[\"epochs\"],\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n[INFO] LR Schedule: initial={CONFIG['learning_rate_initial']}, min={CONFIG['learning_rate_min']}, warmup={CONFIG['warmup_epochs']} epochs\"\n",
    ")\n",
    "print(f\"   Steps per epoch (train): {steps_per_epoch_train}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_lstm_model(\n",
    "        input_shape=input_shape,\n",
    "        n_classes=len(STAGE_ORDER),\n",
    "        lstm_units=CONFIG[\"lstm_units\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        lr_schedule=lr_schedule,\n",
    "        bidirectional=CONFIG[\"bidirectional\"],\n",
    "        use_attention=CONFIG[\"use_attention\"],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314001f",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4486e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"lstm_{CONFIG['execution_mode']}_{timestamp}\"\n",
    "\n",
    "\n",
    "class SleepMetricsCallback(Callback):\n",
    "    \"\"\"Calcula F1-Macro y Kappa al final de cada epoch.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, val_ds, val_steps, stage_order, use_streaming=False, eval_every=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.val_ds, self.val_steps, self.stage_order = val_ds, val_steps, stage_order\n",
    "        self.use_streaming, self.eval_every = use_streaming, max(1, eval_every)\n",
    "        self.best_f1_macro, self.best_kappa, self.best_weights = -1.0, -1.0, None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if (epoch + 1) % self.eval_every != 0:\n",
    "            return\n",
    "        y_true_list, y_pred_list = [], []\n",
    "        if self.use_streaming:\n",
    "            for batch_idx, (x_batch, y_batch) in enumerate(self.val_ds):\n",
    "                if batch_idx >= self.val_steps:\n",
    "                    break\n",
    "                y_pred_list.append(\n",
    "                    np.argmax(self.model.predict(x_batch, verbose=0), axis=1)\n",
    "                )\n",
    "                y_true_list.append(y_batch.numpy())\n",
    "        else:\n",
    "            for x_batch, y_batch in self.val_ds:\n",
    "                y_pred_list.append(\n",
    "                    np.argmax(self.model.predict(x_batch, verbose=0), axis=1)\n",
    "                )\n",
    "                y_true_list.append(y_batch.numpy())\n",
    "        y_true, y_pred = np.concatenate(y_true_list), np.concatenate(y_pred_list)\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs[\"val_kappa\"], logs[\"val_f1_macro\"] = kappa, f1\n",
    "        if f1 > self.best_f1_macro:\n",
    "            self.best_f1_macro, self.best_kappa, self.best_weights = (\n",
    "                f1,\n",
    "                kappa,\n",
    "                self.model.get_weights(),\n",
    "            )\n",
    "            print(f\" - val_f1_macro mejoró a {f1:.4f} (kappa={kappa:.4f})\")\n",
    "        print(f\" - val_f1_macro: {f1:.4f} - val_kappa: {kappa:.4f}\")\n",
    "\n",
    "    def restore_best_weights(self):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            print(\n",
    "                f\"[OK] Restaurados pesos (F1={self.best_f1_macro:.4f}, Kappa={self.best_kappa:.4f})\"\n",
    "            )\n",
    "\n",
    "\n",
    "class NaNDebugCallback(Callback):\n",
    "    \"\"\"Detecta NaN en loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nan_count, self.last_valid_loss, self.first_nan_batch, self.diagnosed = (\n",
    "            0,\n",
    "            None,\n",
    "            None,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        loss = (logs or {}).get(\"loss\", 0)\n",
    "        if not (np.isnan(loss) or np.isinf(loss)):\n",
    "            self.last_valid_loss = loss\n",
    "            return\n",
    "        self.nan_count += 1\n",
    "        if self.first_nan_batch is None:\n",
    "            self.first_nan_batch = batch\n",
    "            print(\n",
    "                f\"\\n[WARN] NaN en batch {batch} (último válido={self.last_valid_loss})\"\n",
    "            )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.nan_count > 0:\n",
    "            print(f\"   [INFO] Epoch {epoch+1}: {self.nan_count} batches con NaN\")\n",
    "        self.nan_count, self.first_nan_batch = 0, None\n",
    "\n",
    "\n",
    "val_steps_for_callback = steps_per_epoch_val if USE_STREAMING else None\n",
    "metrics_callback = SleepMetricsCallback(\n",
    "    val_ds, val_steps_for_callback, STAGE_ORDER, USE_STREAMING, eval_every=1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    NaNDebugCallback(),\n",
    "    metrics_callback,\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        patience=CONFIG[\"early_stopping_patience\"],\n",
    "        restore_best_weights=False,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{OUTPUT_PATH}/{model_name}_best.keras\",\n",
    "        monitor=\"val_f1_macro\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Modelo: {model_name}\")\n",
    "print(f\"Checkpoint: {OUTPUT_PATH}/{model_name}_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203de2e6",
   "metadata": {},
   "source": [
    "## Reanudacion desde Checkpoint (opcional)\n",
    "\n",
    "Si Kaggle se desconecto durante el entrenamiento, puedes reanudar desde el ultimo checkpoint.\n",
    "Solo ejecuta la siguiente celda si necesitas reanudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REANUDAR DESDE CHECKPOINT (ejecutar solo si es necesario)\n",
    "# ============================================================\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # Cambiar a True para reanudar\n",
    "CHECKPOINT_NAME = None  # Ejemplo: \"lstm_full_20251125_143022_best.keras\"\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and CHECKPOINT_NAME:\n",
    "    checkpoint_path = f\"{OUTPUT_PATH}/{CHECKPOINT_NAME}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"[INFO] Cargando modelo desde checkpoint: {checkpoint_path}\")\n",
    "        with strategy.scope():\n",
    "            model = keras.models.load_model(\n",
    "                checkpoint_path, custom_objects={\"AttentionLayer\": AttentionLayer}\n",
    "            )\n",
    "        print(\"[OK] Modelo cargado exitosamente\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Checkpoint no encontrado: {checkpoint_path}\")\n",
    "        for f in os.listdir(OUTPUT_PATH):\n",
    "            if f.endswith(\".keras\"):\n",
    "                print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"[INFO] Modo normal: se usará el modelo recién creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nIniciando entrenamiento ({CONFIG['execution_mode'].upper()} mode)...\")\n",
    "print(f\"   Batch size efectivo: {CONFIG['effective_batch_size']}\")\n",
    "print(f\"   Epochs maximos: {CONFIG['epochs']}\")\n",
    "print(f\"   Steps train: {steps_per_epoch_train}\")\n",
    "if USE_STREAMING:\n",
    "    print(f\"   Steps val: {steps_per_epoch_val}\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "if CONFIG.get(\"use_class_weights\", True) and class_weights:\n",
    "    print(f\"   Class weights: {class_weights}\")\n",
    "else:\n",
    "    print(\"   Class weights: DESACTIVADOS\")\n",
    "\n",
    "if USE_STREAMING:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        validation_steps=steps_per_epoch_val,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        class_weight=class_weights if CONFIG.get(\"use_class_weights\", True) else None,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    train_ds_repeat = train_ds.repeat()\n",
    "    history = model.fit(\n",
    "        train_ds_repeat,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        class_weight=class_weights if CONFIG.get(\"use_class_weights\", True) else None,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "print(\n",
    "    f\"\\n[OK] Entrenamiento completado en {training_time / 60:.2f} min ({training_time / 3600:.2f} h)\"\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "metrics_callback.restore_best_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZAR CURVAS DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(history.history[\"loss\"], label=\"Train Loss\", linewidth=2)\n",
    "axes[0, 0].plot(history.history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Loss durante entrenamiento\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history[\"accuracy\"], label=\"Train Acc\", linewidth=2)\n",
    "axes[0, 1].plot(history.history[\"val_accuracy\"], label=\"Val Acc\", linewidth=2)\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].set_title(\"Accuracy durante entrenamiento\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if \"val_kappa\" in history.history:\n",
    "    axes[1, 0].plot(\n",
    "        history.history[\"val_kappa\"], label=\"Val Kappa\", linewidth=2, color=\"green\"\n",
    "    )\n",
    "    axes[1, 0].axhline(\n",
    "        y=metrics_callback.best_kappa,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best={metrics_callback.best_kappa:.4f}\",\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"Cohen's Kappa\")\n",
    "    axes[1, 0].set_title(\"Cohen's Kappa\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, \"Kappa no disponible\", ha=\"center\", va=\"center\")\n",
    "\n",
    "if \"val_f1_macro\" in history.history:\n",
    "    axes[1, 1].plot(\n",
    "        history.history[\"val_f1_macro\"],\n",
    "        label=\"Val F1-Macro\",\n",
    "        linewidth=2,\n",
    "        color=\"purple\",\n",
    "    )\n",
    "    axes[1, 1].axhline(\n",
    "        y=metrics_callback.best_f1_macro,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best={metrics_callback.best_f1_macro:.4f}\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"F1-Macro\")\n",
    "    axes[1, 1].set_title(\"F1-Macro (métrica de selección)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, \"F1-Macro no disponible\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f348b62",
   "metadata": {},
   "source": [
    "## Evaluacion en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACION EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEvaluando en Test Set...\")\n",
    "\n",
    "\n",
    "def collect_labels(ds):\n",
    "    return np.concatenate(\n",
    "        list(ds.map(lambda _x, y: y).unbatch().batch(1024).as_numpy_iterator())\n",
    "    )\n",
    "\n",
    "\n",
    "y_test_enc = collect_labels(test_ds).astype(int)\n",
    "y_pred_logits = model.predict(test_ds, verbose=1)\n",
    "y_pred_proba = tf.nn.softmax(y_pred_logits, axis=1).numpy()\n",
    "y_pred_enc = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_enc, y_pred_enc)\n",
    "kappa = cohen_kappa_score(y_test_enc, y_pred_enc)\n",
    "f1_macro = f1_score(y_test_enc, y_pred_enc, average=\"macro\", zero_division=0)\n",
    "f1_weighted = f1_score(y_test_enc, y_pred_enc, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"RESULTADOS EN TEST SET\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}  <- Métrica principal en literatura\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_enc,\n",
    "        y_pred_enc,\n",
    "        labels=np.arange(len(STAGE_ORDER)),\n",
    "        target_names=STAGE_ORDER,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea54046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_test_enc, y_pred_enc, labels=np.arange(len(STAGE_ORDER)))\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicho\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confusión (Absoluta)\")\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_xlabel(\"Predicho\")\n",
    "axes[1].set_ylabel(\"Real\")\n",
    "axes[1].set_title(\"Matriz de Confusión (Normalizada)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f6277",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZACION CON OPTUNA (opcional)\n",
    "# ============================================================\n",
    "\n",
    "if CONFIG[\"run_optimization\"]:\n",
    "    print(\"[WARN] Optuna no implementado para modo streaming. Skip.\")\n",
    "else:\n",
    "    print(\n",
    "        \"[SKIP] Optimización deshabilitada. Cambiar CONFIG['run_optimization'] = True para ejecutar.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf6524",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR MODELO Y ARTEFACTOS\n",
    "# ============================================================\n",
    "\n",
    "model.save(f\"{OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "print(f\"[OK] Modelo guardado: {OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "\n",
    "history_data = {k: pd.Series(v) for k, v in history.history.items()}\n",
    "history_df = pd.DataFrame(history_data)\n",
    "history_df.to_csv(f\"{OUTPUT_PATH}/{model_name}_history.csv\", index=False)\n",
    "\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"config\": CONFIG,\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"kappa\": float(kappa),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"training_time_seconds\": float(training_time),\n",
    "        \"training_time_minutes\": float(training_time / 60),\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "        \"best_val_f1_macro\": float(metrics_callback.best_f1_macro),\n",
    "        \"best_val_kappa\": float(metrics_callback.best_kappa),\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": int(train_count),\n",
    "        \"val_samples\": int(val_count),\n",
    "        \"test_samples\": int(test_count),\n",
    "        \"tfrecord_paths\": tfrecord_paths if USE_STREAMING else None,\n",
    "        \"execution_mode\": CONFIG[\"execution_mode\"],\n",
    "    },\n",
    "    \"channel_stats\": {\"mean\": mean_ch.tolist(), \"std\": std_ch.tolist()},\n",
    "    \"label_encoder_classes\": list(le.classes_),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"   - {model_name}_final.keras (modelo)\")\n",
    "print(f\"   - {model_name}_best.keras (mejor checkpoint)\")\n",
    "print(f\"   - {model_name}_history.csv (historial)\")\n",
    "print(f\"   - {model_name}_results.json (metricas y config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO LSTM COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResultados finales en Test Set:\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"\\nModelo guardado en: {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5971229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRIMIR ARTEFACTOS PARA DESCARGA\n",
    "# ============================================================\n",
    "\n",
    "zip_path = f\"{OUTPUT_PATH}/{model_name}_artifacts.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_PATH):\n",
    "        # Excluir TFRecords (muy grandes) y directorios\n",
    "        if fname.startswith(model_name) and not fname.endswith(\".tfrecord\"):\n",
    "            fpath = os.path.join(OUTPUT_PATH, fname)\n",
    "            if os.path.isfile(fpath):\n",
    "                zf.write(fpath, arcname=fname)\n",
    "\n",
    "print(f\"[OK] Artefactos comprimidos en: {zip_path}\")\n",
    "print(\"[INFO] TFRecords excluidos del zip (muy grandes)\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\" - {info.filename} ({info.file_size/1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
