{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5284e37d",
   "metadata": {},
   "source": [
    "# LSTM Bidireccional para Sleep Staging\n",
    "\n",
    "Este notebook entrena un modelo **LSTM Bidireccional** para clasificacion de estadios de sueno usando datos PSG **trimmed** (preprocesados y recortados del dataset Sleep-EDF).\n",
    "\n",
    "**Optimizado para Kaggle con 2x Tesla T4 (16GB VRAM cada una)**\n",
    "\n",
    "### Caracteristicas:\n",
    "- LSTM Bidireccional con mecanismo de atencion opcional\n",
    "- Soporte multi-GPU con MirroredStrategy\n",
    "- Optimizacion de hiperparametros con Optuna\n",
    "- Division train/val/test por sujetos (sin data leakage)\n",
    "- **Soporte para reanudar entrenamiento desde checkpoints**\n",
    "\n",
    "### Datos requeridos:\n",
    "- Dataset `sleep-trimmed` en Kaggle (https://www.kaggle.com/datasets/ignaciolinari/sleep-trimmed)\n",
    "  - `manifest_trimmed.csv`\n",
    "  - `sleep_trimmed/psg/*.fif` (PSG a 100 Hz, float32)\n",
    "  - `sleep_trimmed/hypnograms/*.csv` (anotaciones)\n",
    "- Si usas otro slug, ajusta `DATA_PATH` en la celda siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACION INICIAL - Ejecutar primero\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Detectar si estamos en Kaggle\n",
    "IN_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Entorno: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "# Paths segun entorno\n",
    "if IN_KAGGLE:\n",
    "    # En Kaggle: ajustar al nombre de tu dataset\n",
    "    DATA_PATH = \"/kaggle/input/sleep-trimmed\"  # <- Ajustar al slug/version que uses\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "else:\n",
    "    # Local\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../models\"\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR GPUs DISPONIBLES\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "# Semilla global para reproducibilidad\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPUs disponibles:\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "\n",
    "    # Habilitar memory growth para evitar OOM\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    print(f\"\\n[OK] {len(gpus)} GPU(s) configuradas con memory growth\")\n",
    "else:\n",
    "    print(\"[WARN] No se detectaron GPUs. El entrenamiento sera lento.\")\n",
    "\n",
    "# Estrategia de distribucion para multiples GPUs\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"\\nUsando MirroredStrategy con {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"\\nUsando estrategia por defecto (1 GPU o CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import seaborn as sns  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "import logging  # noqa: E402\n",
    "import json  # noqa: E402\n",
    "from datetime import datetime  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import gc  # noqa: E402\n",
    "import zipfile  # noqa: E402\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder  # noqa: E402\n",
    "from sklearn.utils.class_weight import compute_class_weight  # noqa: E402\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from tensorflow import keras  # noqa: E402\n",
    "from tensorflow.keras import layers  # noqa: E402\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  # noqa: E402\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Configurar estilo de plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALAR DEPENDENCIAS (solo en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"Instalando dependencias...\")\n",
    "    !pip install -q mne yasa\n",
    "    print(\"[OK] Dependencias instaladas\")\n",
    "\n",
    "import mne  # noqa: E402\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc4ec0",
   "metadata": {},
   "source": [
    "## Configuracion del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c636539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIPERPARAMETROS - AJUSTAR SEGUN NECESIDAD\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Dataset (100 Hz)\n",
    "    \"manifest_path\": f\"{DATA_PATH}/manifest_trimmed.csv\",\n",
    "    \"epoch_length\": 30.0,  # segundos\n",
    "    \"sfreq\": 100,  # Hz (dataset a 100 Hz)\n",
    "    \"limit_sessions\": None,  # None = todas, numero = limitar para debug\n",
    "    # Split\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    # Modelo LSTM\n",
    "    \"lstm_units\": 128,  # Unidades LSTM (se reduce a la mitad en 2da capa)\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"bidirectional\": True,  # LSTM bidireccional\n",
    "    \"use_attention\": True,  # Mecanismo de atencion\n",
    "    # Entrenamiento\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 64,  # Con 2x T4 puede aumentar a 128\n",
    "    \"epochs\": 100,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"reduce_lr_patience\": 7,\n",
    "    # Optimizacion (Optuna)\n",
    "    \"run_optimization\": False,  # Cambiar a True para buscar hiperparametros\n",
    "    \"n_optuna_trials\": 30,\n",
    "}\n",
    "\n",
    "# Ajustar batch size para multi-GPU\n",
    "CONFIG[\"effective_batch_size\"] = CONFIG[\"batch_size\"] * strategy.num_replicas_in_sync\n",
    "\n",
    "print(\"Configuracion del experimento:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdbf32",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81219acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "# Canales y estadios\n",
    "DEFAULT_CHANNELS = {\n",
    "    \"EEG\": [\"EEG Fpz-Cz\", \"EEG Pz-Oz\"],\n",
    "    \"EOG\": [\"EOG horizontal\"],\n",
    "    \"EMG\": [\"EMG submental\"],\n",
    "}\n",
    "\n",
    "STAGE_CANONICAL = {\n",
    "    \"Sleep stage W\": \"W\",\n",
    "    \"Sleep stage 1\": \"N1\",\n",
    "    \"Sleep stage 2\": \"N2\",\n",
    "    \"Sleep stage 3\": \"N3\",\n",
    "    \"Sleep stage 4\": \"N3\",\n",
    "    \"Sleep stage R\": \"REM\",\n",
    "}\n",
    "\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "\n",
    "def load_psg_data(psg_path, channels=None, target_sfreq=None):\n",
    "    \"\"\"Carga datos PSG desde archivo .fif.\"\"\"\n",
    "    raw = mne.io.read_raw_fif(str(psg_path), preload=True, verbose=\"ERROR\")\n",
    "\n",
    "    if channels is None:\n",
    "        available = set(raw.ch_names)\n",
    "        channels = []\n",
    "        for ch_group in DEFAULT_CHANNELS.values():\n",
    "            for ch in ch_group:\n",
    "                if ch in available:\n",
    "                    channels.append(ch)\n",
    "\n",
    "    raw.pick(channels)  # pick_channels() está deprecado\n",
    "\n",
    "    if target_sfreq and raw.info[\"sfreq\"] != target_sfreq:\n",
    "        raw.resample(target_sfreq)\n",
    "\n",
    "    data = raw.get_data()\n",
    "    return data, raw.info[\"sfreq\"], raw.ch_names\n",
    "\n",
    "\n",
    "def load_hypnogram(hyp_path):\n",
    "    \"\"\"Carga hipnograma desde CSV.\"\"\"\n",
    "    df = pd.read_csv(hyp_path)\n",
    "    df[\"stage_canonical\"] = df[\"description\"].map(STAGE_CANONICAL)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_epochs(data, sfreq, epoch_length=30.0):\n",
    "    \"\"\"Divide datos en epochs de longitud fija.\"\"\"\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // samples_per_epoch\n",
    "\n",
    "    epochs = []\n",
    "    epoch_times = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        start = i * samples_per_epoch\n",
    "        end = start + samples_per_epoch\n",
    "        epoch = data[:, start:end]\n",
    "        epochs.append(epoch)\n",
    "        epoch_times.append(i * epoch_length)\n",
    "\n",
    "    return np.array(epochs), np.array(epoch_times)\n",
    "\n",
    "\n",
    "def assign_stages(epoch_times, hypnogram, epoch_length=30.0):\n",
    "    \"\"\"Asigna estadios a cada epoch.\"\"\"\n",
    "    stages = []\n",
    "\n",
    "    for t in epoch_times:\n",
    "        epoch_center = t + epoch_length / 2\n",
    "        mask = (hypnogram[\"onset\"] <= epoch_center) & (\n",
    "            hypnogram[\"onset\"] + hypnogram[\"duration\"] > epoch_center\n",
    "        )\n",
    "        matched = hypnogram[mask]\n",
    "\n",
    "        if len(matched) > 0:\n",
    "            stage = matched.iloc[0][\"stage_canonical\"]\n",
    "            stages.append(stage)\n",
    "        else:\n",
    "            stages.append(None)\n",
    "\n",
    "    return stages\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATASET COMPLETO\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def prepare_raw_epochs_dataset(manifest_path, limit=None, epoch_length=30.0, sfreq=100):\n",
    "    \"\"\"Prepara dataset de epochs raw para LSTM.\"\"\"\n",
    "\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    if limit:\n",
    "        manifest_ok = manifest_ok.head(limit)\n",
    "        print(f\"[WARN] Modo debug: procesando solo {limit} sesiones\")\n",
    "\n",
    "    all_epochs = []\n",
    "    all_stages = []\n",
    "    all_metadata = []\n",
    "    skipped_files = 0\n",
    "    error_count = 0\n",
    "\n",
    "    manifest_dir = Path(manifest_path).parent\n",
    "    dataset_dir_name = (\n",
    "        \"sleep_trimmed_resamp200\"\n",
    "        if (manifest_dir / \"sleep_trimmed_resamp200\").exists()\n",
    "        else \"sleep_trimmed\"\n",
    "    )\n",
    "    total_sessions = len(manifest_ok)\n",
    "\n",
    "    print(f\"\\nProcesando {total_sessions} sesiones...\")\n",
    "\n",
    "    for i, (idx, row) in enumerate(manifest_ok.iterrows()):\n",
    "        subject_id = row[\"subject_id\"]\n",
    "\n",
    "        # Construir paths (manejar NaN)\n",
    "        psg_path_str = row.get(\"psg_trimmed_path\", \"\")\n",
    "        hyp_path_str = row.get(\"hypnogram_trimmed_path\", \"\")\n",
    "\n",
    "        # Manejar valores NaN de pandas\n",
    "        if pd.isna(psg_path_str):\n",
    "            psg_path_str = \"\"\n",
    "        if pd.isna(hyp_path_str):\n",
    "            hyp_path_str = \"\"\n",
    "\n",
    "        base_data_root = manifest_dir.parent\n",
    "\n",
    "        if psg_path_str and hyp_path_str:\n",
    "            # Paths relativos desde el manifest\n",
    "            if IN_KAGGLE:\n",
    "                # Extraer path relativo de forma segura\n",
    "                psg_rel = Path(psg_path_str)\n",
    "                hyp_rel = Path(hyp_path_str)\n",
    "                psg_parts = psg_rel.parts\n",
    "                hyp_parts = hyp_rel.parts\n",
    "\n",
    "                psg_anchor_idx = next(\n",
    "                    (\n",
    "                        i\n",
    "                        for i, p in enumerate(psg_parts)\n",
    "                        if p.startswith(\"sleep_trimmed\")\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                hyp_anchor_idx = next(\n",
    "                    (\n",
    "                        i\n",
    "                        for i, p in enumerate(hyp_parts)\n",
    "                        if p.startswith(\"sleep_trimmed\")\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                if psg_anchor_idx is not None and hyp_anchor_idx is not None:\n",
    "                    psg_path = Path(DATA_PATH) / Path(*psg_parts[psg_anchor_idx:])\n",
    "                    hyp_path = Path(DATA_PATH) / Path(*hyp_parts[hyp_anchor_idx:])\n",
    "                else:\n",
    "                    # Fallback: usar solo el nombre del archivo\n",
    "                    psg_path = Path(DATA_PATH) / dataset_dir_name / \"psg\" / psg_rel.name\n",
    "                    hyp_path = (\n",
    "                        Path(DATA_PATH) / dataset_dir_name / \"hypnograms\" / hyp_rel.name\n",
    "                    )\n",
    "            else:\n",
    "                psg_rel = Path(psg_path_str)\n",
    "                hyp_rel = Path(hyp_path_str)\n",
    "\n",
    "                if not psg_rel.is_absolute():\n",
    "                    if psg_rel.parts and psg_rel.parts[0] == \"data\":\n",
    "                        psg_path = base_data_root / psg_rel.relative_to(\"data\")\n",
    "                    else:\n",
    "                        psg_path = manifest_dir / psg_rel\n",
    "                else:\n",
    "                    psg_path = psg_rel\n",
    "\n",
    "                if not hyp_rel.is_absolute():\n",
    "                    if hyp_rel.parts and hyp_rel.parts[0] == \"data\":\n",
    "                        hyp_path = base_data_root / hyp_rel.relative_to(\"data\")\n",
    "                    else:\n",
    "                        hyp_path = manifest_dir / hyp_rel\n",
    "                else:\n",
    "                    hyp_path = hyp_rel\n",
    "        else:\n",
    "            # Construir paths manualmente\n",
    "            subset = row.get(\"subset\", \"sleep-cassette\")\n",
    "            version = row.get(\"version\", \"1.0.0\")\n",
    "            dataset_dir = manifest_dir / dataset_dir_name\n",
    "            psg_path = (\n",
    "                dataset_dir / \"psg\" / f\"{subject_id}_{subset}_{version}_trimmed_raw.fif\"\n",
    "            )\n",
    "            hyp_path = (\n",
    "                dataset_dir\n",
    "                / \"hypnograms\"\n",
    "                / f\"{subject_id}_{subset}_{version}_trimmed_annotations.csv\"\n",
    "            )\n",
    "\n",
    "        if not psg_path.exists() or not hyp_path.exists():\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Cargar datos\n",
    "            data, actual_sfreq, ch_names = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "            hypnogram = load_hypnogram(hyp_path)\n",
    "\n",
    "            # Crear epochs\n",
    "            epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "\n",
    "            # Asignar estadios\n",
    "            stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "            # Filtrar epochs validos\n",
    "            for epoch_idx, (epoch, stage, epoch_time) in enumerate(\n",
    "                zip(epochs, stages, epoch_times)\n",
    "            ):\n",
    "                if stage is not None and stage in STAGE_ORDER:\n",
    "                    all_epochs.append(epoch)\n",
    "                    all_stages.append(stage)\n",
    "                    all_metadata.append(\n",
    "                        {\n",
    "                            \"subject_id\": subject_id,\n",
    "                            \"subject_core\": subject_id[:5],\n",
    "                            \"epoch_time_start\": epoch_time,\n",
    "                            \"epoch_index\": epoch_idx,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"   Procesadas {i + 1}/{total_sessions} sesiones...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"   [WARN] Error en {subject_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Resumen de carga\n",
    "    if skipped_files > 0:\n",
    "        print(f\"   [INFO] Archivos no encontrados: {skipped_files}\")\n",
    "    if error_count > 0:\n",
    "        print(f\"   [WARN] Errores durante carga: {error_count}\")\n",
    "\n",
    "    # Validaciones\n",
    "    assert len(all_epochs) > 0, \"ERROR: No se cargaron epochs válidos\"\n",
    "\n",
    "    X_raw = np.array(all_epochs)\n",
    "    y = np.array(all_stages)\n",
    "    metadata_df = pd.DataFrame(all_metadata)\n",
    "\n",
    "    n_classes_found = len(set(y))\n",
    "    assert (\n",
    "        n_classes_found >= 2\n",
    "    ), f\"ERROR: Se esperaban al menos 2 clases, se encontraron {n_classes_found}\"\n",
    "\n",
    "    print(\"\\n[OK] Dataset cargado:\")\n",
    "    print(f\"   X_raw shape: {X_raw.shape}\")\n",
    "    print(f\"   Clases encontradas: {n_classes_found}\")\n",
    "    print(\"   Distribucion de clases:\")\n",
    "    for stage in STAGE_ORDER:\n",
    "        count = (y == stage).sum()\n",
    "        pct = count / len(y) * 100 if len(y) > 0 else 0\n",
    "        print(f\"      {stage}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    return X_raw, y, metadata_df\n",
    "\n",
    "\n",
    "# Cargar datos\n",
    "X_raw, y, metadata_df = prepare_raw_epochs_dataset(\n",
    "    CONFIG[\"manifest_path\"],\n",
    "    limit=CONFIG[\"limit_sessions\"],\n",
    "    epoch_length=CONFIG[\"epoch_length\"],\n",
    "    sfreq=CONFIG[\"sfreq\"],\n",
    ")\n",
    "\n",
    "# Validar shapes\n",
    "print(\"\\n[CHECK] Validacion de datos:\")\n",
    "print(f\"   X_raw dtype: {X_raw.dtype}\")\n",
    "print(\n",
    "    f\"   Epochs por sujeto (promedio): {len(X_raw) / metadata_df['subject_core'].nunique():.1f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d38e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DIVISION TRAIN/VAL/TEST POR SUJETOS\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def split_by_subjects(\n",
    "    X, y, metadata_df, test_size=0.15, val_size=0.15, random_state=42\n",
    "):\n",
    "    \"\"\"Divide dataset respetando sujetos (sin data leakage).\"\"\"\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    subject_cores = metadata_df[\"subject_core\"].unique()\n",
    "    n_cores = len(subject_cores)\n",
    "    shuffled_cores = np.random.permutation(subject_cores)\n",
    "\n",
    "    n_test = max(1, int(n_cores * test_size))\n",
    "    n_val = max(1, int(n_cores * val_size))\n",
    "\n",
    "    test_cores = set(shuffled_cores[:n_test])\n",
    "    val_cores = set(shuffled_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(shuffled_cores[n_test + n_val :])\n",
    "\n",
    "    train_mask = metadata_df[\"subject_core\"].isin(train_cores)\n",
    "    val_mask = metadata_df[\"subject_core\"].isin(val_cores)\n",
    "    test_mask = metadata_df[\"subject_core\"].isin(test_cores)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    print(\"\\nDivision del dataset:\")\n",
    "    print(f\"   Train: {len(X_train):,} epochs de {len(train_cores)} sujetos\")\n",
    "    print(f\"   Val:   {len(X_val):,} epochs de {len(val_cores)} sujetos\")\n",
    "    print(f\"   Test:  {len(X_test):,} epochs de {len(test_cores)} sujetos\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_by_subjects(\n",
    "    X_raw,\n",
    "    y,\n",
    "    metadata_df,\n",
    "    test_size=CONFIG[\"test_size\"],\n",
    "    val_size=CONFIG[\"val_size\"],\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    ")\n",
    "\n",
    "# Liberar memoria\n",
    "del X_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZACION Y PREPARACION PARA LSTM\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def normalize_for_lstm(X_train, X_val, X_test):\n",
    "    \"\"\"Normaliza y transpone datos para LSTM.\n",
    "\n",
    "    LSTM espera: (samples, timesteps, features)\n",
    "    Input viene como: (samples, channels, timesteps)\n",
    "    Transponemos a: (samples, timesteps, channels)\n",
    "    \"\"\"\n",
    "\n",
    "    n_channels = X_train.shape[1]\n",
    "    channel_stats = []\n",
    "\n",
    "    X_train_norm = np.zeros_like(X_train, dtype=np.float32)\n",
    "    X_val_norm = np.zeros_like(X_val, dtype=np.float32)\n",
    "    X_test_norm = np.zeros_like(X_test, dtype=np.float32)\n",
    "\n",
    "    # Normalizar por canal\n",
    "    for ch in range(n_channels):\n",
    "        mean = X_train[:, ch, :].mean()\n",
    "        std = X_train[:, ch, :].std()\n",
    "        channel_stats.append({\"mean\": mean, \"std\": std})\n",
    "\n",
    "        if std > 0:\n",
    "            X_train_norm[:, ch, :] = (X_train[:, ch, :] - mean) / std\n",
    "            X_val_norm[:, ch, :] = (X_val[:, ch, :] - mean) / std\n",
    "            X_test_norm[:, ch, :] = (X_test[:, ch, :] - mean) / std\n",
    "        else:\n",
    "            X_train_norm[:, ch, :] = X_train[:, ch, :]\n",
    "            X_val_norm[:, ch, :] = X_val[:, ch, :]\n",
    "            X_test_norm[:, ch, :] = X_test[:, ch, :]\n",
    "\n",
    "    # Transponer: (samples, channels, timesteps) -> (samples, timesteps, channels)\n",
    "    X_train_lstm = np.transpose(X_train_norm, (0, 2, 1))\n",
    "    X_val_lstm = np.transpose(X_val_norm, (0, 2, 1))\n",
    "    X_test_lstm = np.transpose(X_test_norm, (0, 2, 1))\n",
    "\n",
    "    print(\"[OK] Normalizacion y transposicion completada\")\n",
    "    print(f\"   Shape para LSTM: {X_train_lstm.shape}\")\n",
    "    return X_train_lstm, X_val_lstm, X_test_lstm, channel_stats\n",
    "\n",
    "\n",
    "# Normalizar y preparar para LSTM\n",
    "X_train_lstm, X_val_lstm, X_test_lstm, channel_stats = normalize_for_lstm(\n",
    "    X_train, X_val, X_test\n",
    ")\n",
    "\n",
    "# Codificar etiquetas\n",
    "le = LabelEncoder()\n",
    "le.fit(STAGE_ORDER)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "print(f\"\\nClases: {le.classes_}\")\n",
    "\n",
    "# Calcular class weights\n",
    "class_weights_arr = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(y_train_enc), y=y_train_enc\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Liberar memoria de arrays no normalizados\n",
    "del X_train, X_val, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5344f4",
   "metadata": {},
   "source": [
    "## Arquitectura LSTM Bidireccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43094281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CAPA DE ATENCION PERSONALIZADA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"Capa de atencion simple para LSTM.\n",
    "\n",
    "    Permite al modelo enfocarse en las partes mas relevantes\n",
    "    de la secuencia temporal para la clasificacion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"attention_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"attention_bias\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch, timesteps, features)\n",
    "        # Calcular scores de atencion\n",
    "        e = keras.backend.tanh(keras.backend.dot(x, self.W) + self.b)\n",
    "        # Softmax sobre timesteps\n",
    "        a = keras.backend.softmax(e, axis=1)\n",
    "        # Weighted sum\n",
    "        output = x * a\n",
    "        return keras.backend.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "\n",
    "print(\"[OK] Capa de Atencion definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODELO LSTM BIDIRECCIONAL CON ATENCION\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def build_lstm_model(\n",
    "    input_shape,\n",
    "    n_classes=5,\n",
    "    lstm_units=128,\n",
    "    dropout_rate=0.5,\n",
    "    learning_rate=0.001,\n",
    "    bidirectional=True,\n",
    "    use_attention=True,\n",
    "):\n",
    "    \"\"\"Construye modelo LSTM Bidireccional para sleep staging.\n",
    "\n",
    "    Arquitectura:\n",
    "    - 2 capas LSTM (bidireccionales opcionales)\n",
    "    - BatchNorm y Dropout entre capas\n",
    "    - Capa de atencion opcional\n",
    "    - Capas densas para clasificacion\n",
    "\n",
    "    NOTA: No usamos recurrent_dropout para mantener compatibilidad con cuDNN\n",
    "    \"\"\"\n",
    "\n",
    "    # Input: (timesteps, features)\n",
    "    input_layer = keras.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    # Primera capa LSTM (retorna secuencias)\n",
    "    lstm_1 = layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_1\",\n",
    "    )\n",
    "\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(lstm_1, name=\"bidirectional_1\")(input_layer)\n",
    "    else:\n",
    "        x = lstm_1(input_layer)\n",
    "\n",
    "    x = layers.BatchNormalization(name=\"bn_1\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_1\")(x)\n",
    "\n",
    "    # Segunda capa LSTM\n",
    "    lstm_2 = layers.LSTM(\n",
    "        lstm_units // 2,\n",
    "        return_sequences=use_attention,  # Solo retorna secuencias si usamos atencion\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_2\",\n",
    "    )\n",
    "\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(lstm_2, name=\"bidirectional_2\")(x)\n",
    "    else:\n",
    "        x = lstm_2(x)\n",
    "\n",
    "    x = layers.BatchNormalization(name=\"bn_2\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_2\")(x)\n",
    "\n",
    "    # Capa de atencion (opcional)\n",
    "    if use_attention:\n",
    "        x = AttentionLayer(name=\"attention\")(x)\n",
    "\n",
    "    # Capas densas para clasificacion\n",
    "    x = layers.Dense(\n",
    "        128,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"dense_1\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        64,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"dense_2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_2\")(x)\n",
    "\n",
    "    # Output\n",
    "    output_layer = layers.Dense(n_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    # Nombre del modelo segun configuracion\n",
    "    model_name = \"BiLSTM\" if bidirectional else \"LSTM\"\n",
    "    if use_attention:\n",
    "        model_name += \"_Attention\"\n",
    "    model_name += \"_SleepStaging\"\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=model_name)\n",
    "\n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"[OK] Arquitectura LSTM definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO (con estrategia multi-GPU si está disponible)\n",
    "# ============================================================\n",
    "\n",
    "# Input shape: (timesteps, features) = (X_train_lstm.shape[1], n_channels)\n",
    "input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_lstm_model(\n",
    "        input_shape=input_shape,\n",
    "        n_classes=len(STAGE_ORDER),\n",
    "        lstm_units=CONFIG[\"lstm_units\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        bidirectional=CONFIG[\"bidirectional\"],\n",
    "        use_attention=CONFIG[\"use_attention\"],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5941b",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"lstm_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=CONFIG[\"early_stopping_patience\"],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=CONFIG[\"reduce_lr_patience\"],\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{OUTPUT_PATH}/{model_name}_best.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Modelo se guardara en: {OUTPUT_PATH}/{model_name}_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50deee9",
   "metadata": {},
   "source": [
    "## Reanudacion desde Checkpoint (opcional)\n",
    "\n",
    "Si Kaggle se desconecto durante el entrenamiento, puedes reanudar desde el ultimo checkpoint.\n",
    "Solo ejecuta la siguiente celda si necesitas reanudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REANUDAR DESDE CHECKPOINT (ejecutar solo si es necesario)\n",
    "# ============================================================\n",
    "# Descomenta y ajusta el nombre del checkpoint si necesitas reanudar\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # Cambiar a True para reanudar\n",
    "CHECKPOINT_NAME = None  # Ejemplo: \"lstm_20251125_143022_best.keras\"\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and CHECKPOINT_NAME:\n",
    "    checkpoint_path = f\"{OUTPUT_PATH}/{CHECKPOINT_NAME}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"[INFO] Cargando modelo desde checkpoint: {checkpoint_path}\")\n",
    "        # Registrar la capa de atencion personalizada\n",
    "        with strategy.scope():\n",
    "            model = keras.models.load_model(\n",
    "                checkpoint_path, custom_objects={\"AttentionLayer\": AttentionLayer}\n",
    "            )\n",
    "        print(\"[OK] Modelo cargado exitosamente\")\n",
    "        print(\"[INFO] Puedes continuar el entrenamiento ejecutando la celda de fit()\")\n",
    "        print(\"[INFO] El modelo continuara desde donde quedo\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Checkpoint no encontrado: {checkpoint_path}\")\n",
    "        print(f\"[INFO] Archivos disponibles en {OUTPUT_PATH}:\")\n",
    "        for f in os.listdir(OUTPUT_PATH):\n",
    "            if f.endswith(\".keras\"):\n",
    "                print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"[INFO] Modo normal: se usara el modelo recien creado\")\n",
    "    print(\"[INFO] Para reanudar desde checkpoint, cambia RESUME_FROM_CHECKPOINT=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b374b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nIniciando entrenamiento LSTM...\")\n",
    "print(f\"   Batch size efectivo: {CONFIG['effective_batch_size']}\")\n",
    "print(f\"   Epochs maximos: {CONFIG['epochs']}\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_val_lstm, y_val_enc),\n",
    "    batch_size=CONFIG[\"effective_batch_size\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\n[OK] Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZAR CURVAS DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history[\"loss\"], label=\"Train Loss\", linewidth=2)\n",
    "axes[0].plot(history.history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss durante entrenamiento\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history[\"accuracy\"], label=\"Train Acc\", linewidth=2)\n",
    "axes[1].plot(history.history[\"val_accuracy\"], label=\"Val Acc\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Accuracy durante entrenamiento\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc1252",
   "metadata": {},
   "source": [
    "## Evaluacion en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACION EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEvaluando en Test Set...\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba = model.predict(\n",
    "    X_test_lstm, batch_size=CONFIG[\"batch_size\"] * 2, verbose=1\n",
    ")\n",
    "y_pred_enc = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Metricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTADOS EN TEST SET\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ecf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=STAGE_ORDER, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=STAGE_ORDER)\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absoluta\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicho\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confusión (Absoluta)\")\n",
    "\n",
    "# Normalizada\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_xlabel(\"Predicho\")\n",
    "axes[1].set_ylabel(\"Real\")\n",
    "axes[1].set_title(\"Matriz de Confusión (Normalizada)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d090aed",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46882682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZACION CON OPTUNA (opcional)\n",
    "# ============================================================\n",
    "\n",
    "if CONFIG[\"run_optimization\"]:\n",
    "    !pip install -q optuna\n",
    "    import optuna\n",
    "    from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Funcion objetivo para Optuna.\"\"\"\n",
    "\n",
    "        # Hiperparametros a optimizar\n",
    "        lstm_units = trial.suggest_categorical(\"lstm_units\", [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.6)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "        use_attention = trial.suggest_categorical(\"use_attention\", [True, False])\n",
    "\n",
    "        # Crear modelo\n",
    "        with strategy.scope():\n",
    "            trial_model = build_lstm_model(\n",
    "                input_shape=input_shape,\n",
    "                n_classes=len(STAGE_ORDER),\n",
    "                lstm_units=lstm_units,\n",
    "                dropout_rate=dropout_rate,\n",
    "                learning_rate=learning_rate,\n",
    "                bidirectional=bidirectional,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "\n",
    "        # Callbacks\n",
    "        trial_callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
    "            TFKerasPruningCallback(trial, \"val_loss\"),\n",
    "        ]\n",
    "\n",
    "        # Entrenar\n",
    "        trial_model.fit(\n",
    "            X_train_lstm,\n",
    "            y_train_enc,\n",
    "            validation_data=(X_val_lstm, y_val_enc),\n",
    "            batch_size=batch_size * strategy.num_replicas_in_sync,\n",
    "            epochs=30,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=trial_callbacks,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Evaluar en validacion\n",
    "        y_val_pred = np.argmax(trial_model.predict(X_val_lstm, verbose=0), axis=1)\n",
    "        kappa = cohen_kappa_score(y_val_enc, y_val_pred)\n",
    "\n",
    "        # Limpiar\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        return kappa\n",
    "\n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"lstm_sleep_staging\",\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nIniciando optimizacion con {CONFIG['n_optuna_trials']} trials...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=CONFIG[\"n_optuna_trials\"], show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Optimizacion completada\")\n",
    "    print(f\"   Mejor Kappa: {study.best_value:.4f}\")\n",
    "    print(\"   Mejores hiperparametros:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    with open(f\"{OUTPUT_PATH}/optuna_lstm_results.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"best_value\": study.best_value,\n",
    "                \"best_params\": study.best_params,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "else:\n",
    "    print(\n",
    "        \"[SKIP] Optimizacion deshabilitada. Cambiar CONFIG['run_optimization'] = True para ejecutar.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c684123",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR MODELO Y ARTEFACTOS\n",
    "# ============================================================\n",
    "\n",
    "# Guardar modelo completo\n",
    "model.save(f\"{OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "print(f\"[OK] Modelo guardado: {OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "\n",
    "# Guardar historial\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f\"{OUTPUT_PATH}/{model_name}_history.csv\", index=False)\n",
    "\n",
    "# Guardar resultados\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"config\": CONFIG,\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"kappa\": float(kappa),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": len(X_train_lstm),\n",
    "        \"val_samples\": len(X_val_lstm),\n",
    "        \"test_samples\": len(X_test_lstm),\n",
    "    },\n",
    "    \"channel_stats\": channel_stats,\n",
    "    \"label_encoder_classes\": list(le.classes_),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[OK] Resultados guardados: {OUTPUT_PATH}/{model_name}_results.json\")\n",
    "\n",
    "# Guardar LabelEncoder\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"   - {model_name}_final.keras (modelo)\")\n",
    "print(f\"   - {model_name}_best.keras (mejor checkpoint)\")\n",
    "print(f\"   - {model_name}_history.csv (historial)\")\n",
    "print(f\"   - {model_name}_results.json (metricas y config)\")\n",
    "print(f\"   - {model_name}_label_encoder.pkl (encoder)\")\n",
    "print(f\"   - {model_name}_training_curves.png\")\n",
    "print(f\"   - {model_name}_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0274dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO LSTM COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResultados finales en Test Set:\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"\\nModelo guardado en: {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6577bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRIMIR ARTEFACTOS PARA DESCARGA\n",
    "# ============================================================\n",
    "\n",
    "zip_path = f\"{OUTPUT_PATH}/{model_name}_artifacts.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_PATH):\n",
    "        if fname.startswith(model_name):\n",
    "            zf.write(os.path.join(OUTPUT_PATH, fname), arcname=fname)\n",
    "\n",
    "print(f\"[OK] Artefactos comprimidos en: {zip_path}\")\n",
    "print(\"Archivos incluidos:\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\" - {info.filename} ({info.file_size/1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
