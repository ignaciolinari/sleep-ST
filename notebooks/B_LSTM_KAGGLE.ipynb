{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5284e37d",
   "metadata": {},
   "source": [
    "# LSTM Bidireccional para Sleep Staging\n",
    "\n",
    "Este notebook entrena un modelo **LSTM Bidireccional** para clasificacion de estadios de sueno usando datos PSG **trimmed** (preprocesados y recortados del dataset Sleep-EDF).\n",
    "\n",
    "**Optimizado para Kaggle con 2x Tesla T4 (16GB VRAM cada una)**\n",
    "\n",
    "### Caracteristicas:\n",
    "- LSTM Bidireccional con mecanismo de atencion opcional\n",
    "- Soporte multi-GPU con MirroredStrategy\n",
    "- Optimizacion de hiperparametros con Optuna\n",
    "- Division train/val/test por sujetos (sin data leakage)\n",
    "- **Soporte para reanudar entrenamiento desde checkpoints**\n",
    "\n",
    "### Datos requeridos:\n",
    "- Dataset `sleep-edf-trimmed-f32` en Kaggle (https://www.kaggle.com/datasets/ignaciolinari/sleep-edf-trimmed-f32)\n",
    "  - `manifest_trimmed_spt.csv` (1 episodio por noche, 100 Hz)\n",
    "  - `sleep_trimmed_spt/psg/*.fif` (PSG a 100 Hz, float32)\n",
    "  - `sleep_trimmed_spt/hypnograms/*.csv` (anotaciones)\n",
    "- Si usas la versión 200 Hz, apunta a `manifest_trimmed_resamp200.csv` + carpeta `sleep_trimmed_resamp200/`.\n",
    "- Nota Kaggle: la versión 200 Hz puede consumir más VRAM; deja `sfreq=100` o usa el dataset 100 Hz para evitar OOM.\n",
    "- Si usas otro slug, ajusta `DATA_PATH` en la celda siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACION INICIAL - Ejecutar primero\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Detectar si estamos en Kaggle\n",
    "IN_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Entorno: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "\n",
    "# Paths segun entorno\n",
    "if IN_KAGGLE:\n",
    "    # Detectar slug automáticamente (por defecto sleep-edf-trimmed-f32)\n",
    "    base_input = Path(\"/kaggle/input\")\n",
    "    default_slug = base_input / \"sleep-edf-trimmed-f32\"\n",
    "    if default_slug.exists():\n",
    "        dataset_root = default_slug\n",
    "    else:\n",
    "        # Tomar la primera carpeta que contenga \"sleep-edf\" si el slug difiere\n",
    "        candidates = sorted(\n",
    "            [p for p in base_input.iterdir() if p.is_dir() and \"sleep-edf\" in p.name]\n",
    "        )\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError(\n",
    "                \"No se encontró dataset sleep-edf* en /kaggle/input; ajusta DATA_PATH manualmente.\"\n",
    "            )\n",
    "        dataset_root = candidates[0]\n",
    "\n",
    "    # DATA_PATH apunta al root del dataset; subcarpetas se resuelven más abajo\n",
    "    DATA_PATH = str(dataset_root)\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "else:\n",
    "    # Local\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../models\"\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR GPUs DISPONIBLES\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "# Semilla global para reproducibilidad\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPUs disponibles:\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "\n",
    "    # Habilitar memory growth para evitar OOM\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    print(f\"\\n[OK] {len(gpus)} GPU(s) configuradas con memory growth\")\n",
    "else:\n",
    "    print(\"[WARN] No se detectaron GPUs. El entrenamiento sera lento.\")\n",
    "\n",
    "# Estrategia de distribucion para multiples GPUs\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"\\nUsando MirroredStrategy con {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"\\nUsando estrategia por defecto (1 GPU o CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import seaborn as sns  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "from collections import Counter  # noqa: E402\n",
    "import logging  # noqa: E402\n",
    "import json  # noqa: E402\n",
    "from datetime import datetime  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import zipfile  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import re  # noqa: E402\n",
    "import math  # noqa: E402\n",
    "import gc  # noqa: E402\n",
    "\n",
    "# Semillas globales para reproducibilidad\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder  # noqa: E402\n",
    "from sklearn.utils.class_weight import compute_class_weight  # noqa: E402\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from tensorflow import keras  # noqa: E402\n",
    "from tensorflow.keras import layers  # noqa: E402\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  # noqa: E402\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Configurar estilo de plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALAR DEPENDENCIAS (solo en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import mne\n",
    "except ImportError:\n",
    "    if IN_KAGGLE:\n",
    "        print(\"Instalando dependencias...\")\n",
    "        %pip install -q mne\n",
    "        print(\"[OK] Dependencias instaladas\")\n",
    "        import mne\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc4ec0",
   "metadata": {},
   "source": [
    "## Configuracion del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c636539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIPERPARAMETROS - AJUSTAR SEGUN NECESIDAD\n",
    "# ============================================================\n",
    "\n",
    "# Localizar manifest disponible (prioridad: resamp200 -> spt -> trimmed)\n",
    "manifest_candidates = [\n",
    "    Path(DATA_PATH) / \"manifest_trimmed_resamp200.csv\",\n",
    "    Path(DATA_PATH) / \"manifest_trimmed_spt.csv\",\n",
    "    Path(DATA_PATH) / \"manifest_trimmed.csv\",\n",
    "]\n",
    "manifest_path = next((str(p) for p in manifest_candidates if p.exists()), None)\n",
    "if manifest_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontró manifest_*.csv en {DATA_PATH}; ajusta DATA_PATH o el slug/version en Inputs.\"\n",
    "    )\n",
    "\n",
    "CONFIG = {\n",
    "    # Dataset (100 Hz)\n",
    "    \"manifest_path\": manifest_path,\n",
    "    \"epoch_length\": 30.0,  # segundos\n",
    "    \"sfreq\": 100,  # Hz (dataset a 100 Hz)\n",
    "    \"limit_sessions\": None,  # None = todas, numero = limitar para debug\n",
    "    # Split por sujeto\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    # Modelo LSTM\n",
    "    \"lstm_units\": 128,  # Unidades LSTM (se reduce a la mitad en 2da capa)\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"bidirectional\": True,  # LSTM bidireccional\n",
    "    \"use_attention\": True,  # Mecanismo de atencion\n",
    "    # Entrenamiento\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 64,  # Con 2x T4 puede aumentar a 128\n",
    "    \"epochs\": 100,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"reduce_lr_patience\": 7,\n",
    "    # Optimizacion (Optuna)\n",
    "    \"run_optimization\": False,  # Cambiar a True para buscar hiperparametros\n",
    "    \"n_optuna_trials\": 30,\n",
    "    # Streaming TFRecord (evita OOM en Kaggle)\n",
    "    \"streaming\": True if IN_KAGGLE else False,\n",
    "    \"tfrecord_dir\": f\"{OUTPUT_PATH}/tfrecords_lstm\",\n",
    "    \"shuffle_buffer\": 5000,\n",
    "}\n",
    "\n",
    "# Ajustar batch size para multi-GPU\n",
    "CONFIG[\"effective_batch_size\"] = CONFIG[\"batch_size\"] * strategy.num_replicas_in_sync\n",
    "\n",
    "print(\"Configuracion del experimento:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdbf32",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81219acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "# Canales y estadios\n",
    "DEFAULT_CHANNELS = {\n",
    "    \"EEG\": [\"EEG Fpz-Cz\", \"EEG Pz-Oz\"],\n",
    "    \"EOG\": [\"EOG horizontal\"],\n",
    "    \"EMG\": [\"EMG submental\"],\n",
    "}\n",
    "\n",
    "STAGE_CANONICAL = {\n",
    "    \"Sleep stage W\": \"W\",\n",
    "    \"Sleep stage 1\": \"N1\",\n",
    "    \"Sleep stage 2\": \"N2\",\n",
    "    \"Sleep stage 3\": \"N3\",\n",
    "    \"Sleep stage 4\": \"N3\",\n",
    "    \"Sleep stage R\": \"REM\",\n",
    "}\n",
    "\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "\n",
    "def extract_subject_core(subject_id):\n",
    "    \"\"\"Agrupa noches del mismo sujeto (ej. SC4001E0/E1 -> SC4001).\"\"\"\n",
    "    match = re.match(r\"(.+?)[Ee]\\d+$\", str(subject_id))\n",
    "    return match.group(1) if match else str(subject_id)\n",
    "\n",
    "\n",
    "def load_psg_data(psg_path, channels=None, target_sfreq=None):\n",
    "    \"\"\"Carga datos PSG desde archivo .fif.\"\"\"\n",
    "    raw = mne.io.read_raw_fif(str(psg_path), preload=True, verbose=\"ERROR\")\n",
    "\n",
    "    if channels is None:\n",
    "        available = set(raw.ch_names)\n",
    "        channels = []\n",
    "        for ch_group in DEFAULT_CHANNELS.values():\n",
    "            for ch in ch_group:\n",
    "                if ch in available:\n",
    "                    channels.append(ch)\n",
    "\n",
    "    raw.pick(channels)  # pick_channels() está deprecado\n",
    "\n",
    "    if target_sfreq and raw.info[\"sfreq\"] != target_sfreq:\n",
    "        raw.resample(target_sfreq)\n",
    "\n",
    "    data = raw.get_data()\n",
    "    return data, raw.info[\"sfreq\"], raw.ch_names\n",
    "\n",
    "\n",
    "def load_hypnogram(hyp_path):\n",
    "    \"\"\"Carga hipnograma desde CSV.\"\"\"\n",
    "    df = pd.read_csv(hyp_path)\n",
    "    df[\"stage_canonical\"] = df[\"description\"].map(STAGE_CANONICAL)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_epochs(data, sfreq, epoch_length=30.0):\n",
    "    \"\"\"Divide datos en epochs de longitud fija.\"\"\"\n",
    "    samples_per_epoch = int(epoch_length * sfreq)\n",
    "    n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // samples_per_epoch\n",
    "\n",
    "    epochs = []\n",
    "    epoch_times = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        start = i * samples_per_epoch\n",
    "        end = start + samples_per_epoch\n",
    "        epoch = data[:, start:end]\n",
    "        epochs.append(epoch)\n",
    "        epoch_times.append(i * epoch_length)\n",
    "\n",
    "    return np.array(epochs), np.array(epoch_times)\n",
    "\n",
    "\n",
    "def assign_stages(epoch_times, hypnogram, epoch_length=30.0):\n",
    "    \"\"\"Asigna estadios a cada epoch.\"\"\"\n",
    "    stages = []\n",
    "\n",
    "    for t in epoch_times:\n",
    "        epoch_center = t + epoch_length / 2\n",
    "        mask = (hypnogram[\"onset\"] <= epoch_center) & (\n",
    "            hypnogram[\"onset\"] + hypnogram[\"duration\"] > epoch_center\n",
    "        )\n",
    "        matched = hypnogram[mask]\n",
    "\n",
    "        if len(matched) > 0:\n",
    "            stage = matched.iloc[0][\"stage_canonical\"]\n",
    "            stages.append(stage)\n",
    "        else:\n",
    "            stages.append(None)\n",
    "\n",
    "    return stages\n",
    "\n",
    "\n",
    "print(\"[OK] Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PIPELINE STREAMING CON TFRECORD (evita OOM en Kaggle)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def resolve_paths(row, manifest_dir, dataset_dir_name):\n",
    "    \"\"\"Resuelve rutas de PSG e hipnograma respetando Kaggle/local.\"\"\"\n",
    "\n",
    "    psg_path_str = row.get(\"psg_trimmed_path\", \"\")\n",
    "    hyp_path_str = row.get(\"hypnogram_trimmed_path\", \"\")\n",
    "\n",
    "    # Manejar NaN\n",
    "    if pd.isna(psg_path_str):\n",
    "        psg_path_str = \"\"\n",
    "    if pd.isna(hyp_path_str):\n",
    "        hyp_path_str = \"\"\n",
    "\n",
    "    base_data_root = manifest_dir.parent\n",
    "\n",
    "    if psg_path_str and hyp_path_str:\n",
    "        if IN_KAGGLE:\n",
    "            psg_rel = Path(psg_path_str)\n",
    "            hyp_rel = Path(hyp_path_str)\n",
    "            psg_parts = psg_rel.parts\n",
    "            hyp_parts = hyp_rel.parts\n",
    "\n",
    "            psg_anchor_idx = next(\n",
    "                (i for i, p in enumerate(psg_parts) if p.startswith(\"sleep_trimmed\")),\n",
    "                None,\n",
    "            )\n",
    "            hyp_anchor_idx = next(\n",
    "                (i for i, p in enumerate(hyp_parts) if p.startswith(\"sleep_trimmed\")),\n",
    "                None,\n",
    "            )\n",
    "\n",
    "            if psg_anchor_idx is not None and hyp_anchor_idx is not None:\n",
    "                psg_path = Path(DATA_PATH) / Path(*psg_parts[psg_anchor_idx:])\n",
    "                hyp_path = Path(DATA_PATH) / Path(*hyp_parts[hyp_anchor_idx:])\n",
    "            else:\n",
    "                psg_path = Path(DATA_PATH) / dataset_dir_name / \"psg\" / psg_rel.name\n",
    "                hyp_path = (\n",
    "                    Path(DATA_PATH) / dataset_dir_name / \"hypnograms\" / hyp_rel.name\n",
    "                )\n",
    "        else:\n",
    "            psg_rel = Path(psg_path_str)\n",
    "            hyp_rel = Path(hyp_path_str)\n",
    "\n",
    "            if not psg_rel.is_absolute():\n",
    "                if psg_rel.parts and psg_rel.parts[0] == \"data\":\n",
    "                    psg_path = base_data_root / psg_rel.relative_to(\"data\")\n",
    "                else:\n",
    "                    psg_path = manifest_dir / psg_rel\n",
    "            else:\n",
    "                psg_path = psg_rel\n",
    "\n",
    "            if not hyp_rel.is_absolute():\n",
    "                if hyp_rel.parts and hyp_rel.parts[0] == \"data\":\n",
    "                    hyp_path = base_data_root / hyp_rel.relative_to(\"data\")\n",
    "                else:\n",
    "                    hyp_path = manifest_dir / hyp_rel\n",
    "            else:\n",
    "                hyp_path = hyp_rel\n",
    "    else:\n",
    "        subset = row.get(\"subset\", \"sleep-cassette\")\n",
    "        version = row.get(\"version\", \"1.0.0\")\n",
    "        dataset_dir = manifest_dir / dataset_dir_name\n",
    "        psg_path = (\n",
    "            dataset_dir\n",
    "            / \"psg\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_raw.fif\"\n",
    "        )\n",
    "        hyp_path = (\n",
    "            dataset_dir\n",
    "            / \"hypnograms\"\n",
    "            / f\"{row['subject_id']}_{subset}_{version}_trimmed_annotations.csv\"\n",
    "        )\n",
    "\n",
    "    return psg_path, hyp_path\n",
    "\n",
    "\n",
    "def iter_sessions(manifest_path, epoch_length, sfreq, limit=None):\n",
    "    \"\"\"Itera sesiones entregando rutas resueltas.\"\"\"\n",
    "\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    if limit:\n",
    "        manifest_ok = manifest_ok.head(limit)\n",
    "        print(f\"[WARN] Modo debug: procesando solo {limit} sesiones\")\n",
    "\n",
    "    manifest_dir = Path(manifest_path).parent\n",
    "    dataset_dir_name = (\n",
    "        \"sleep_trimmed_resamp200\"\n",
    "        if (manifest_dir / \"sleep_trimmed_resamp200\").exists()\n",
    "        else \"sleep_trimmed_spt\"\n",
    "        if (manifest_dir / \"sleep_trimmed_spt\").exists()\n",
    "        else \"sleep_trimmed\"\n",
    "    )\n",
    "\n",
    "    total_sessions = len(manifest_ok)\n",
    "    print(f\"\\nProcesando {total_sessions} sesiones (streaming)...\")\n",
    "\n",
    "    for i, (_, row) in enumerate(manifest_ok.iterrows(), start=1):\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        subject_core = extract_subject_core(subject_id)\n",
    "        psg_path, hyp_path = resolve_paths(row, manifest_dir, dataset_dir_name)\n",
    "\n",
    "        if not psg_path.exists() or not hyp_path.exists():\n",
    "            print(f\"[WARN] Archivos faltantes para {subject_id}; se omite esta sesion\")\n",
    "            continue\n",
    "\n",
    "        yield i, total_sessions, subject_id, subject_core, psg_path, hyp_path\n",
    "\n",
    "\n",
    "def update_running_stats(stats, epochs):\n",
    "    \"\"\"Acumula sumas y sumas cuadradas por canal.\"\"\"\n",
    "\n",
    "    if stats is None:\n",
    "        stats = {\"n\": 0, \"sum\": None, \"sumsq\": None}\n",
    "\n",
    "    if stats[\"sum\"] is None:\n",
    "        stats[\"sum\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "        stats[\"sumsq\"] = np.zeros(epochs.shape[1], dtype=np.float64)\n",
    "\n",
    "    channel_sum = epochs.sum(axis=(0, 2))\n",
    "    channel_sumsq = (epochs**2).sum(axis=(0, 2))\n",
    "    stats[\"n\"] += epochs.shape[0] * epochs.shape[2]\n",
    "    stats[\"sum\"] += channel_sum\n",
    "    stats[\"sumsq\"] += channel_sumsq\n",
    "    return stats\n",
    "\n",
    "\n",
    "def finalize_running_stats(stats):\n",
    "    mean = stats[\"sum\"] / stats[\"n\"]\n",
    "    var = stats[\"sumsq\"] / stats[\"n\"] - mean**2\n",
    "    std = np.sqrt(np.maximum(var, 1e-8))\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def pass1_stats(manifest_path, epoch_length, sfreq, limit=None):\n",
    "    \"\"\"Primera pasada: mean/std por canal y conteo de clases.\"\"\"\n",
    "\n",
    "    stats = None\n",
    "    class_counts = Counter()\n",
    "    input_shape = None\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, limit\n",
    "    ):\n",
    "        data, actual_sfreq, _ = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        valid_mask = [s in STAGE_ORDER for s in stages]\n",
    "        if not any(valid_mask):\n",
    "            del data, epochs, epoch_times, stages, hypnogram\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        valid_epochs = epochs[valid_mask]\n",
    "        valid_stages = [s for s in stages if s in STAGE_ORDER]\n",
    "\n",
    "        if input_shape is None and len(valid_epochs) > 0:\n",
    "            input_shape = valid_epochs.shape[1:]  # (channels, samples)\n",
    "\n",
    "        stats = update_running_stats(stats, valid_epochs)\n",
    "        class_counts.update(valid_stages)\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 1: {i}/{total} sesiones\")\n",
    "\n",
    "        # Liberar memoria por iteracion\n",
    "        del data, epochs, epoch_times, stages, hypnogram, valid_epochs, valid_stages\n",
    "        gc.collect()\n",
    "\n",
    "    assert input_shape is not None, \"No se encontraron epochs validos\"\n",
    "    mean, std = finalize_running_stats(stats)\n",
    "    return mean, std, class_counts, input_shape\n",
    "\n",
    "\n",
    "def make_example(x, y):\n",
    "    feature = {\n",
    "        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x.ravel())),\n",
    "        \"y\": tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n",
    "    }\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    ).SerializeToString()\n",
    "\n",
    "\n",
    "def assign_subject_splits(manifest_path, test_size, val_size, random_state, limit=None):\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    manifest_ok = manifest[manifest[\"status\"] == \"ok\"].copy()\n",
    "    if limit:\n",
    "        manifest_ok = manifest_ok.head(limit)\n",
    "\n",
    "    subject_cores = manifest_ok[\"subject_id\"].apply(extract_subject_core).unique()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(subject_cores)\n",
    "\n",
    "    n_test = max(1, int(len(subject_cores) * test_size))\n",
    "    n_val = max(1, int(len(subject_cores) * val_size))\n",
    "\n",
    "    test_cores = set(subject_cores[:n_test])\n",
    "    val_cores = set(subject_cores[n_test : n_test + n_val])\n",
    "    train_cores = set(subject_cores[n_test + n_val :])\n",
    "\n",
    "    split_map = {core: \"train\" for core in train_cores}\n",
    "    split_map.update({core: \"val\" for core in val_cores})\n",
    "    split_map.update({core: \"test\" for core in test_cores})\n",
    "\n",
    "    return split_map, {\n",
    "        \"train\": len(train_cores),\n",
    "        \"val\": len(val_cores),\n",
    "        \"test\": len(test_cores),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_tfrecord_splits(\n",
    "    manifest_path,\n",
    "    mean,\n",
    "    std,\n",
    "    split_map,\n",
    "    epoch_length,\n",
    "    sfreq,\n",
    "    tfrecord_dir,\n",
    "    limit=None,\n",
    "):\n",
    "    tfrecord_dir = Path(tfrecord_dir)\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {\n",
    "        \"train\": tf.io.TFRecordWriter(str(tfrecord_dir / \"train.tfrecord\")),\n",
    "        \"val\": tf.io.TFRecordWriter(str(tfrecord_dir / \"val.tfrecord\")),\n",
    "        \"test\": tf.io.TFRecordWriter(str(tfrecord_dir / \"test.tfrecord\")),\n",
    "    }\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for i, total, subject_id, subject_core, psg_path, hyp_path in iter_sessions(\n",
    "        manifest_path, epoch_length, sfreq, limit\n",
    "    ):\n",
    "        split = split_map.get(subject_core)\n",
    "        if split is None:\n",
    "            continue\n",
    "\n",
    "        data, actual_sfreq, _ = load_psg_data(psg_path, target_sfreq=sfreq)\n",
    "        hypnogram = load_hypnogram(hyp_path)\n",
    "        epochs, epoch_times = create_epochs(data, actual_sfreq, epoch_length)\n",
    "        stages = assign_stages(epoch_times, hypnogram, epoch_length)\n",
    "\n",
    "        for epoch, stage in zip(epochs, stages):\n",
    "            if stage not in STAGE_ORDER:\n",
    "                continue\n",
    "            y = STAGE_ORDER.index(stage)\n",
    "            x = (epoch - mean[:, None]) / std[:, None]\n",
    "            writers[split].write(make_example(x.astype(np.float32), y))\n",
    "            counts[split] += 1\n",
    "\n",
    "        if i % 20 == 0 or i == total:\n",
    "            print(f\"   Pasada 2 ({split}): {i}/{total} sesiones\")\n",
    "\n",
    "        # Liberar memoria por iteracion\n",
    "        del data, epochs, epoch_times, stages, hypnogram\n",
    "        gc.collect()\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "\n",
    "    tf_paths = {split: str(tfrecord_dir / f\"{split}.tfrecord\") for split in writers}\n",
    "    return tf_paths, counts\n",
    "\n",
    "\n",
    "def make_dataset(tfrecord_path, input_shape_ch_first, batch_size, shuffle=False):\n",
    "    feature_description = {\n",
    "        \"x\": tf.io.FixedLenFeature(\n",
    "            [input_shape_ch_first[0] * input_shape_ch_first[1]], tf.float32\n",
    "        ),\n",
    "        \"y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _parse(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        x = tf.reshape(example[\"x\"], input_shape_ch_first)  # (channels, samples)\n",
    "        # LSTM espera (timesteps, features) = (samples, channels)\n",
    "        x = tf.transpose(x, perm=[1, 0])\n",
    "        y = example[\"y\"]\n",
    "        return x, y\n",
    "\n",
    "    ds = tf.data.TFRecordDataset([tfrecord_path], num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            CONFIG[\"shuffle_buffer\"],\n",
    "            seed=CONFIG[\"random_state\"],\n",
    "            reshuffle_each_iteration=True,\n",
    "        )\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "if CONFIG[\"streaming\"]:\n",
    "    print(\"\\n[INFO] Modo streaming activado (TFRecord)\")\n",
    "\n",
    "    mean_ch, std_ch, class_counts, input_shape_ch_first = pass1_stats(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        limit=CONFIG[\"limit_sessions\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Estadisticas globales calculadas\")\n",
    "    print(f\"   mean shape: {mean_ch.shape}\")\n",
    "    print(f\"   std  shape: {std_ch.shape}\")\n",
    "\n",
    "    # Class weights (evitar division por cero si falta alguna clase)\n",
    "    counts_list = [class_counts.get(stage, 0) for stage in STAGE_ORDER]\n",
    "    if any(c == 0 for c in counts_list):\n",
    "        print(\n",
    "            \"[WARN] Alguna clase no apareció en la pasada 1; ajustando conteos mínimos a 1\"\n",
    "        )\n",
    "        counts_list = [max(c, 1) for c in counts_list]\n",
    "\n",
    "    y_for_weights = np.repeat(np.arange(len(STAGE_ORDER)), counts_list)\n",
    "    class_weights_arr = compute_class_weight(\n",
    "        \"balanced\", classes=np.arange(len(STAGE_ORDER)), y=y_for_weights\n",
    "    )\n",
    "    class_weights = dict(enumerate(class_weights_arr))\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "    # Asignar splits por sujeto y escribir TFRecords\n",
    "    split_map, split_subjects = assign_subject_splits(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        CONFIG[\"test_size\"],\n",
    "        CONFIG[\"val_size\"],\n",
    "        CONFIG[\"random_state\"],\n",
    "        limit=CONFIG[\"limit_sessions\"],\n",
    "    )\n",
    "\n",
    "    tfrecord_paths, split_counts = build_tfrecord_splits(\n",
    "        CONFIG[\"manifest_path\"],\n",
    "        mean_ch,\n",
    "        std_ch,\n",
    "        split_map,\n",
    "        CONFIG[\"epoch_length\"],\n",
    "        CONFIG[\"sfreq\"],\n",
    "        CONFIG[\"tfrecord_dir\"],\n",
    "        limit=CONFIG[\"limit_sessions\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] TFRecords generados:\")\n",
    "    for split, path in tfrecord_paths.items():\n",
    "        print(f\"   {split}: {path} ({split_counts[split]:,} epochs)\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(STAGE_ORDER)\n",
    "\n",
    "    INPUT_SHAPE = (\n",
    "        input_shape_ch_first[1],\n",
    "        input_shape_ch_first[0],\n",
    "    )  # (timesteps, channels)\n",
    "\n",
    "    train_ds = make_dataset(\n",
    "        tfrecord_paths[\"train\"],\n",
    "        input_shape_ch_first,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_ds = make_dataset(\n",
    "        tfrecord_paths[\"val\"],\n",
    "        input_shape_ch_first,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_ds = make_dataset(\n",
    "        tfrecord_paths[\"test\"],\n",
    "        input_shape_ch_first,\n",
    "        CONFIG[\"effective_batch_size\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    train_count = split_counts.get(\"train\", 0)\n",
    "    val_count = split_counts.get(\"val\", 0)\n",
    "    test_count = split_counts.get(\"test\", 0)\n",
    "\n",
    "    print(\"\\n[CHECK] Resumen dataset:\")\n",
    "    print(f\"   Input shape (timesteps, channels): {INPUT_SHAPE}\")\n",
    "    print(f\"   Train epochs: {train_count:,} (sujetos: {split_subjects['train']})\")\n",
    "    print(f\"   Val epochs:   {val_count:,} (sujetos: {split_subjects['val']})\")\n",
    "    print(f\"   Test epochs:  {test_count:,} (sujetos: {split_subjects['test']})\")\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        \"Desactiva CONFIG['streaming'] para usar el pipeline antiguo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d38e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DIVISION TRAIN/VAL/TEST POR SUJETOS\n",
    "# ============================================================\n",
    "\n",
    "if CONFIG[\"streaming\"]:\n",
    "    print(\"[SKIP] División manejada en la escritura de TFRecords (por sujeto).\")\n",
    "    print(f\"   Train epochs: {train_count:,}\")\n",
    "    print(f\"   Val epochs:   {val_count:,}\")\n",
    "    print(f\"   Test epochs:  {test_count:,}\")\n",
    "else:\n",
    "    raise NotImplementedError(\"Desactiva streaming para usar la división en memoria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZACION Y PREPARACION PARA LSTM\n",
    "# ============================================================\n",
    "\n",
    "if CONFIG[\"streaming\"]:\n",
    "    print(\"[SKIP] Normalización aplicada al escribir TFRecords (float32 normalizado).\")\n",
    "    print(f\"Clases: {STAGE_ORDER}\")\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        \"Desactiva streaming para usar la normalización en memoria\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5344f4",
   "metadata": {},
   "source": [
    "## Arquitectura LSTM Bidireccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43094281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CAPA DE ATENCION PERSONALIZADA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"Capa de atencion simple para LSTM.\n",
    "\n",
    "    Permite al modelo enfocarse en las partes mas relevantes\n",
    "    de la secuencia temporal para la clasificacion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"attention_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"attention_bias\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch, timesteps, features)\n",
    "        # Calcular scores de atencion\n",
    "        e = keras.backend.tanh(keras.backend.dot(x, self.W) + self.b)\n",
    "        # Softmax sobre timesteps\n",
    "        a = keras.backend.softmax(e, axis=1)\n",
    "        # Weighted sum\n",
    "        output = x * a\n",
    "        return keras.backend.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "\n",
    "print(\"[OK] Capa de Atencion definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODELO LSTM BIDIRECCIONAL CON ATENCION\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def build_lstm_model(\n",
    "    input_shape,\n",
    "    n_classes=5,\n",
    "    lstm_units=128,\n",
    "    dropout_rate=0.5,\n",
    "    learning_rate=0.001,\n",
    "    bidirectional=True,\n",
    "    use_attention=True,\n",
    "):\n",
    "    \"\"\"Construye modelo LSTM Bidireccional para sleep staging.\n",
    "\n",
    "    Arquitectura:\n",
    "    - 2 capas LSTM (bidireccionales opcionales)\n",
    "    - BatchNorm y Dropout entre capas\n",
    "    - Capa de atencion opcional\n",
    "    - Capas densas para clasificacion\n",
    "\n",
    "    NOTA: No usamos recurrent_dropout para mantener compatibilidad con cuDNN\n",
    "    \"\"\"\n",
    "\n",
    "    # Input: (timesteps, features)\n",
    "    input_layer = keras.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    # Primera capa LSTM (retorna secuencias)\n",
    "    lstm_1 = layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_1\",\n",
    "    )\n",
    "\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(lstm_1, name=\"bidirectional_1\")(input_layer)\n",
    "    else:\n",
    "        x = lstm_1(input_layer)\n",
    "\n",
    "    x = layers.BatchNormalization(name=\"bn_1\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_1\")(x)\n",
    "\n",
    "    # Segunda capa LSTM\n",
    "    lstm_2 = layers.LSTM(\n",
    "        lstm_units // 2,\n",
    "        return_sequences=use_attention,  # Solo retorna secuencias si usamos atencion\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"lstm_2\",\n",
    "    )\n",
    "\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(lstm_2, name=\"bidirectional_2\")(x)\n",
    "    else:\n",
    "        x = lstm_2(x)\n",
    "\n",
    "    x = layers.BatchNormalization(name=\"bn_2\")(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_lstm_2\")(x)\n",
    "\n",
    "    # Capa de atencion (opcional)\n",
    "    if use_attention:\n",
    "        x = AttentionLayer(name=\"attention\")(x)\n",
    "\n",
    "    # Capas densas para clasificacion\n",
    "    x = layers.Dense(\n",
    "        128,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"dense_1\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        64,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "        name=\"dense_2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate, name=\"dropout_2\")(x)\n",
    "\n",
    "    # Output\n",
    "    output_layer = layers.Dense(n_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    # Nombre del modelo segun configuracion\n",
    "    model_name = \"BiLSTM\" if bidirectional else \"LSTM\"\n",
    "    if use_attention:\n",
    "        model_name += \"_Attention\"\n",
    "    model_name += \"_SleepStaging\"\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=model_name)\n",
    "\n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"[OK] Arquitectura LSTM definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO (con estrategia multi-GPU si está disponible)\n",
    "# ============================================================\n",
    "\n",
    "# Input shape: (timesteps, features)\n",
    "input_shape = INPUT_SHAPE\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_lstm_model(\n",
    "        input_shape=input_shape,\n",
    "        n_classes=len(STAGE_ORDER),\n",
    "        lstm_units=CONFIG[\"lstm_units\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        bidirectional=CONFIG[\"bidirectional\"],\n",
    "        use_attention=CONFIG[\"use_attention\"],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5941b",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"lstm_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=CONFIG[\"early_stopping_patience\"],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=CONFIG[\"reduce_lr_patience\"],\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{OUTPUT_PATH}/{model_name}_best.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Modelo se guardara en: {OUTPUT_PATH}/{model_name}_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50deee9",
   "metadata": {},
   "source": [
    "## Reanudacion desde Checkpoint (opcional)\n",
    "\n",
    "Si Kaggle se desconecto durante el entrenamiento, puedes reanudar desde el ultimo checkpoint.\n",
    "Solo ejecuta la siguiente celda si necesitas reanudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REANUDAR DESDE CHECKPOINT (ejecutar solo si es necesario)\n",
    "# ============================================================\n",
    "# Descomenta y ajusta el nombre del checkpoint si necesitas reanudar\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # Cambiar a True para reanudar\n",
    "CHECKPOINT_NAME = None  # Ejemplo: \"lstm_20251125_143022_best.keras\"\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and CHECKPOINT_NAME:\n",
    "    checkpoint_path = f\"{OUTPUT_PATH}/{CHECKPOINT_NAME}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"[INFO] Cargando modelo desde checkpoint: {checkpoint_path}\")\n",
    "        # Registrar la capa de atencion personalizada\n",
    "        with strategy.scope():\n",
    "            model = keras.models.load_model(\n",
    "                checkpoint_path, custom_objects={\"AttentionLayer\": AttentionLayer}\n",
    "            )\n",
    "        print(\"[OK] Modelo cargado exitosamente\")\n",
    "        print(\"[INFO] Puedes continuar el entrenamiento ejecutando la celda de fit()\")\n",
    "        print(\"[INFO] El modelo continuara desde donde quedo\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Checkpoint no encontrado: {checkpoint_path}\")\n",
    "        print(f\"[INFO] Archivos disponibles en {OUTPUT_PATH}:\")\n",
    "        for f in os.listdir(OUTPUT_PATH):\n",
    "            if f.endswith(\".keras\"):\n",
    "                print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"[INFO] Modo normal: se usara el modelo recien creado\")\n",
    "    print(\"[INFO] Para reanudar desde checkpoint, cambia RESUME_FROM_CHECKPOINT=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b374b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nIniciando entrenamiento LSTM (streaming tf.data)...\")\n",
    "print(f\"   Batch size efectivo: {CONFIG['effective_batch_size']}\")\n",
    "print(f\"   Epochs maximos: {CONFIG['epochs']}\")\n",
    "print(f\"   Steps train: {math.ceil(train_count / CONFIG['effective_batch_size'])}\")\n",
    "print(f\"   Steps val:   {math.ceil(val_count / CONFIG['effective_batch_size'])}\")\n",
    "\n",
    "if train_count == 0 or val_count == 0:\n",
    "    raise ValueError(\n",
    "        f\"Dataset vacio: train={train_count}, val={val_count}. Revisa manifest en {CONFIG['manifest_path']} y DATA_PATH={DATA_PATH}\"\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    steps_per_epoch=math.ceil(train_count / CONFIG[\"effective_batch_size\"]),\n",
    "    validation_steps=math.ceil(val_count / CONFIG[\"effective_batch_size\"]),\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\n[OK] Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZAR CURVAS DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history[\"loss\"], label=\"Train Loss\", linewidth=2)\n",
    "axes[0].plot(history.history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss durante entrenamiento\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history[\"accuracy\"], label=\"Train Acc\", linewidth=2)\n",
    "axes[1].plot(history.history[\"val_accuracy\"], label=\"Val Acc\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Accuracy durante entrenamiento\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc1252",
   "metadata": {},
   "source": [
    "## Evaluacion en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACION EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEvaluando en Test Set (tf.data)...\")\n",
    "\n",
    "\n",
    "def collect_labels(ds):\n",
    "    return np.concatenate(\n",
    "        list(ds.map(lambda _x, y: y).unbatch().batch(1024).as_numpy_iterator())\n",
    "    )\n",
    "\n",
    "\n",
    "# Etiquetas reales\n",
    "y_test_enc = collect_labels(test_ds)\n",
    "y_test = le.inverse_transform(y_test_enc.astype(int))\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba = model.predict(test_ds, verbose=1)\n",
    "y_pred_enc = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Metricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTADOS EN TEST SET\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ecf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=STAGE_ORDER, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=STAGE_ORDER)\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absoluta\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicho\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confusión (Absoluta)\")\n",
    "\n",
    "# Normalizada\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=STAGE_ORDER,\n",
    "    yticklabels=STAGE_ORDER,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_xlabel(\"Predicho\")\n",
    "axes[1].set_ylabel(\"Real\")\n",
    "axes[1].set_title(\"Matriz de Confusión (Normalizada)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/{model_name}_confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d090aed",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46882682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZACION CON OPTUNA (opcional)\n",
    "# ============================================================\n",
    "\n",
    "# Optuna no soportado en modo streaming. Si necesitas tuning, desactiva streaming.\n",
    "if CONFIG[\"run_optimization\"]:\n",
    "    print(\"[WARN] Optuna no está implementado para el modo streaming TFRecord. Skip.\")\n",
    "\n",
    "    \"\"\"\n",
    "        # Hiperparametros a optimizar\n",
    "        lstm_units = trial.suggest_categorical(\"lstm_units\", [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.6)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "        use_attention = trial.suggest_categorical(\"use_attention\", [True, False])\n",
    "\n",
    "        # Crear modelo\n",
    "        with strategy.scope():\n",
    "            trial_model = build_lstm_model(\n",
    "                input_shape=input_shape,\n",
    "                n_classes=len(STAGE_ORDER),\n",
    "                lstm_units=lstm_units,\n",
    "                dropout_rate=dropout_rate,\n",
    "                learning_rate=learning_rate,\n",
    "                bidirectional=bidirectional,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "\n",
    "        # Callbacks\n",
    "        trial_callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
    "            TFKerasPruningCallback(trial, \"val_loss\"),\n",
    "        ]\n",
    "\n",
    "        # Entrenar\n",
    "        trial_model.fit(\n",
    "            X_train_lstm,\n",
    "            y_train_enc,\n",
    "            validation_data=(X_val_lstm, y_val_enc),\n",
    "            batch_size=batch_size * strategy.num_replicas_in_sync,\n",
    "            epochs=30,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=trial_callbacks,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Evaluar en validacion\n",
    "        y_val_pred = np.argmax(trial_model.predict(X_val_lstm, verbose=0), axis=1)\n",
    "        kappa = cohen_kappa_score(y_val_enc, y_val_pred)\n",
    "\n",
    "        # Limpiar\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        return kappa\n",
    "\n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"lstm_sleep_staging\",\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nIniciando optimizacion con {CONFIG['n_optuna_trials']} trials...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=CONFIG[\"n_optuna_trials\"], show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n[OK] Optimizacion completada\")\n",
    "    print(f\"   Mejor Kappa: {study.best_value:.4f}\")\n",
    "    print(\"   Mejores hiperparametros:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    with open(f\"{OUTPUT_PATH}/optuna_lstm_results.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"best_value\": study.best_value,\n",
    "                \"best_params\": study.best_params,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(\n",
    "        \"[SKIP] Optimizacion deshabilitada. Cambiar CONFIG['run_optimization'] = True para ejecutar.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c684123",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR MODELO Y ARTEFACTOS\n",
    "# ============================================================\n",
    "\n",
    "# Guardar modelo completo\n",
    "model.save(f\"{OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "print(f\"[OK] Modelo guardado: {OUTPUT_PATH}/{model_name}_final.keras\")\n",
    "\n",
    "# Guardar historial\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f\"{OUTPUT_PATH}/{model_name}_history.csv\", index=False)\n",
    "\n",
    "# Guardar resultados\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"config\": CONFIG,\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"kappa\": float(kappa),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": int(train_count),\n",
    "        \"val_samples\": int(val_count),\n",
    "        \"test_samples\": int(test_count),\n",
    "        \"tfrecord_paths\": tfrecord_paths,\n",
    "    },\n",
    "    \"channel_stats\": {\"mean\": mean_ch.tolist(), \"std\": std_ch.tolist()},\n",
    "    \"label_encoder_classes\": list(le.classes_),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[OK] Resultados guardados: {OUTPUT_PATH}/{model_name}_results.json\")\n",
    "\n",
    "# Guardar LabelEncoder\n",
    "with open(f\"{OUTPUT_PATH}/{model_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"   - {model_name}_final.keras (modelo)\")\n",
    "print(f\"   - {model_name}_best.keras (mejor checkpoint)\")\n",
    "print(f\"   - {model_name}_history.csv (historial)\")\n",
    "print(f\"   - {model_name}_results.json (metricas y config)\")\n",
    "print(f\"   - {model_name}_label_encoder.pkl (encoder)\")\n",
    "print(f\"   - {model_name}_training_curves.png\")\n",
    "print(f\"   - {model_name}_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0274dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO LSTM COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResultados finales en Test Set:\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Cohen Kappa: {kappa:.4f}\")\n",
    "print(f\"   F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"\\nModelo guardado en: {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6577bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRIMIR ARTEFACTOS PARA DESCARGA\n",
    "# ============================================================\n",
    "\n",
    "zip_path = f\"{OUTPUT_PATH}/{model_name}_artifacts.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in os.listdir(OUTPUT_PATH):\n",
    "        if fname.startswith(model_name):\n",
    "            zf.write(os.path.join(OUTPUT_PATH, fname), arcname=fname)\n",
    "\n",
    "print(f\"[OK] Artefactos comprimidos en: {zip_path}\")\n",
    "print(\"Archivos incluidos:\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\" - {info.filename} ({info.file_size/1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
