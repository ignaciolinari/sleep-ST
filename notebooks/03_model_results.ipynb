{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de Resultados de Modelos\n",
    "\n",
    "Este notebook permite cargar y visualizar los resultados de entrenamiento de los modelos de clasificación de estadios de sueño.\n",
    "\n",
    "Carga métricas desde archivos JSON generados por `src/models.py` y genera visualizaciones interactivas.\n",
    "\n",
    "**Modelos soportados:**\n",
    "- Random Forest\n",
    "- XGBoost  \n",
    "- CNN1D (Deep Learning)\n",
    "- LSTM (Deep Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from statistics import NormalDist\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "STAGE_COLORS = {\n",
    "    \"W\": \"#fdae61\",\n",
    "    \"N1\": \"#fee090\",\n",
    "    \"N2\": \"#abd9e9\",\n",
    "    \"N3\": \"#2c7bb6\",\n",
    "    \"REM\": \"#d7191c\",\n",
    "}\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "\n",
    "def load_metrics(path: Path) -> dict:\n",
    "    \"\"\"Carga métricas desde un archivo JSON.\"\"\"\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def wilson_ci(\n",
    "    successes: float, total: float, alpha: float = 0.05\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Intervalo de confianza de Wilson para proporciones (binomial).\"\"\"\n",
    "    if total <= 0:\n",
    "        return (np.nan, np.nan)\n",
    "    z = NormalDist().inv_cdf(1 - alpha / 2)\n",
    "    phat = successes / total\n",
    "    denom = 1 + (z**2) / total\n",
    "    center = (phat + (z**2) / (2 * total)) / denom\n",
    "    margin = (\n",
    "        z * np.sqrt((phat * (1 - phat) / total) + ((z**2) / (4 * total**2)))\n",
    "    ) / denom\n",
    "    return (max(0, center - margin), min(1, center + margin))\n",
    "\n",
    "\n",
    "def compute_class_metrics(\n",
    "    cm: np.ndarray, labels=STAGE_ORDER, alpha: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calcula métricas por clase a partir de la matriz de confusión.\"\"\"\n",
    "    cm = np.asarray(cm)\n",
    "    metrics = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        tp = cm[idx, idx]\n",
    "        fn = cm[idx, :].sum() - tp\n",
    "        fp = cm[:, idx].sum() - tp\n",
    "        tn = cm.sum() - tp - fn - fp\n",
    "\n",
    "        support = tp + fn\n",
    "        predicted = tp + fp\n",
    "\n",
    "        precision = tp / (predicted + EPS)\n",
    "        recall = tp / (support + EPS)\n",
    "        specificity = tn / (tn + fp + EPS)\n",
    "        npv = tn / (tn + fn + EPS)\n",
    "        f1 = 2 * precision * recall / (precision + recall + EPS)\n",
    "        fpr = fp / (fp + tn + EPS)\n",
    "        fnr = fn / (fn + tp + EPS)\n",
    "        fdr = fp / (fp + tp + EPS)\n",
    "\n",
    "        recall_ci_low, recall_ci_high = wilson_ci(tp, support, alpha=alpha)\n",
    "        spec_ci_low, spec_ci_high = wilson_ci(tn, tn + fp, alpha=alpha)\n",
    "\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"Stage\": label,\n",
    "                \"support\": support,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"specificity\": specificity,\n",
    "                \"npv\": npv,\n",
    "                \"f1\": f1,\n",
    "                \"fpr\": fpr,\n",
    "                \"fnr\": fnr,\n",
    "                \"fdr\": fdr,\n",
    "                \"recall_ci_low\": recall_ci_low,\n",
    "                \"recall_ci_high\": recall_ci_high,\n",
    "                \"spec_ci_low\": spec_ci_low,\n",
    "                \"spec_ci_high\": spec_ci_high,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "def summarize_class_metrics(df: pd.DataFrame) -> dict[str, float]:\n",
    "    \"\"\"Resumen macro con métricas robustas a desbalance (balanced accuracy).\"\"\"\n",
    "    n_classes = len(df)\n",
    "    chance = 1 / n_classes if n_classes else np.nan\n",
    "    balanced_accuracy = df[\"recall\"].mean()\n",
    "    macro_specificity = df[\"specificity\"].mean()\n",
    "    return {\n",
    "        \"balanced_accuracy\": balanced_accuracy,\n",
    "        \"macro_specificity\": macro_specificity,\n",
    "        \"macro_precision\": df[\"precision\"].mean(),\n",
    "        \"macro_f1\": df[\"f1\"].mean(),\n",
    "        \"gmean_sens_spec\": np.sqrt(balanced_accuracy * macro_specificity),\n",
    "        \"chance_accuracy\": chance,\n",
    "        \"balanced_accuracy_lift\": (balanced_accuracy / chance - 1)\n",
    "        if chance\n",
    "        else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def most_confused_pairs(\n",
    "    cm: np.ndarray, labels=STAGE_ORDER, top_n: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Top pares de confusión (verdadero vs predicho) ordenados por frecuencia.\"\"\"\n",
    "    cm = np.asarray(cm)\n",
    "    records = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        row_sum = cm[i, :].sum()\n",
    "        for j, pred_label in enumerate(labels):\n",
    "            if i == j:\n",
    "                continue\n",
    "            count = int(cm[i, j])\n",
    "            if count > 0:\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"Verdadero\": true_label,\n",
    "                        \"Predicho\": pred_label,\n",
    "                        \"Conteo\": count,\n",
    "                        \"Proporción_sobre_clase\": count / (row_sum + EPS),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"Verdadero\", \"Predicho\", \"Conteo\", \"Proporción_sobre_clase\"]\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df.sort_values(by=\"Conteo\", ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"random_forest\": MODELS_DIR / \"rf_opt_bayes_best\" / \"random_forest_metrics.json\",\n",
    "    \"xgboost\": MODELS_DIR / \"xgb_loso_best\" / \"xgboost_metrics.json\",\n",
    "    \"cnn1d\": ARTIFACTS_DIR\n",
    "    / \"cnn1d_full_20251210_201502_artifacts\"\n",
    "    / \"cnn1d_full_20251210_201502_results.json\",\n",
    "    \"lstm_unidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_unidir_artifacts\"\n",
    "    / \"lstm_full_20251210_193039_results.json\",\n",
    "    \"lstm_bidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_artifacts\"\n",
    "    / \"lstm_full_20251211_031820_results.json\",\n",
    "    \"lstm_bidir_attention\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_attention_artifacts\"\n",
    "    / \"lstm_full_20251211_145034_results.json\",\n",
    "}\n",
    "\n",
    "AVAILABLE_MODELS = list(MODEL_PATHS.keys())\n",
    "\n",
    "print(\"Modelos disponibles:\", AVAILABLE_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar métricas de un modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar según el modelo que quieras visualizar\n",
    "model_type = \"xgboost\"  # Opciones: \"random_forest\", \"xgboost\", \"cnn1d\", \"lstm_unidir\", \"lstm_bidir\", \"lstm_bidir_attention\"\n",
    "\n",
    "metrics_path = MODEL_PATHS.get(model_type)\n",
    "\n",
    "if metrics_path and metrics_path.exists():\n",
    "    metrics = load_metrics(metrics_path)\n",
    "    print(f\"Métricas cargadas para {model_type}\")\n",
    "    print(f\"Timestamp: {metrics.get('timestamp', 'N/A')}\")\n",
    "    print(\n",
    "        f\"Tipo de modelo: {metrics.get('model_type', metrics.get('model_name', 'N/A'))}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Parámetros/Config: {metrics.get('model_params', metrics.get('config', {}))}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"No se encontró archivo de métricas para {model_type}\")\n",
    "    print(f\"Path buscado: {metrics_path}\")\n",
    "    print(\"\\nModelos disponibles con archivos:\")\n",
    "    for name, path in MODEL_PATHS.items():\n",
    "        status = \"✓\" if path.exists() else \"✗\"\n",
    "        print(f\"  {status} {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas generales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"metrics\" in metrics:\n",
    "    m = metrics[\"metrics\"]\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MÉTRICAS GENERALES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:        {m.get('accuracy', 0):.4f}\")\n",
    "    print(f\"Cohen's Kappa:   {m.get('kappa', 0):.4f}\")\n",
    "    print(f\"F1-score (macro): {m.get('f1_macro', 0):.4f}\")\n",
    "    print(f\"F1-score (weighted): {m.get('f1_weighted', 0):.4f}\")\n",
    "\n",
    "    if \"cv_results\" in metrics:\n",
    "        cv = metrics[\"cv_results\"]\n",
    "        print(\"\\nCross-Validation:\")\n",
    "        print(\n",
    "            f\"  Mean F1-macro: {cv.get('mean_score', 0):.4f} ± {cv.get('std_score', 0):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-score por estadio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"metrics\" in metrics and \"f1_per_class\" in metrics[\"metrics\"]:\n",
    "    f1_per_class = metrics[\"metrics\"][\"f1_per_class\"]\n",
    "\n",
    "    # Crear DataFrame para visualización\n",
    "    f1_df = pd.DataFrame(\n",
    "        [\n",
    "            {\"Stage\": stage, \"F1-score\": f1_per_class.get(stage, 0.0)}\n",
    "            for stage in STAGE_ORDER\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Gráfico de barras\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = [STAGE_COLORS.get(stage, \"gray\") for stage in STAGE_ORDER]\n",
    "    bars = ax.bar(\n",
    "        f1_df[\"Stage\"], f1_df[\"F1-score\"], color=colors, alpha=0.7, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Agregar valores en las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"F1-score\", fontsize=12)\n",
    "    ax.set_xlabel(\"Estadio de Sueño\", fontsize=12)\n",
    "    ax.set_title(\n",
    "        f\"F1-score por Estadio - {model_type.upper()}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar tabla\n",
    "    display(\n",
    "        f1_df.style.format({\"F1-score\": \"{:.4f}\"}).background_gradient(\n",
    "            cmap=\"YlGnBu\", subset=[\"F1-score\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"confusion_matrix\" in metrics:\n",
    "    cm = np.array(metrics[\"confusion_matrix\"])\n",
    "\n",
    "    # Normalizar por filas (recall por clase)\n",
    "    cm_normalized = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Matriz absoluta\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[0].set_title(\"Matriz de Confusión (Absoluta)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Verdadero\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Predicho\", fontsize=12)\n",
    "\n",
    "    # Matriz normalizada\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={\"label\": \"Proporción\"},\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[1].set_title(\n",
    "        \"Matriz de Confusión (Normalizada)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Verdadero\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Predicho\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis complementario\n",
    "\n",
    "Se amplía el análisis para interpretar los resultados con mayor rigor: métricas derivadas de la matriz de confusión (recall, especificidad, precisión, NPV, tasas de error), intervalos de confianza binomiales (Wilson) por clase, balanceo de desempeño (balanced accuracy) y pares de confusión más frecuentes que indican errores sistemáticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05  # IC 95%\n",
    "if \"confusion_matrix\" in metrics:\n",
    "    cm = np.array(metrics[\"confusion_matrix\"])\n",
    "    class_metrics = compute_class_metrics(cm, labels=STAGE_ORDER, alpha=alpha)\n",
    "    summary = summarize_class_metrics(class_metrics)\n",
    "\n",
    "    # Distribución de soportes por clase\n",
    "    support_df = class_metrics[[\"Stage\", \"support\"]].copy()\n",
    "    support_df[\"proportion\"] = support_df[\"support\"] / support_df[\"support\"].sum()\n",
    "    print(\"Distribución de soportes por clase:\")\n",
    "    display(\n",
    "        support_df.style.format(\n",
    "            {\"support\": \"{:,.0f}\", \"proportion\": \"{:.2%}\"}\n",
    "        ).background_gradient(cmap=\"Purples\", subset=[\"proportion\"])\n",
    "    )\n",
    "\n",
    "    # Métricas por clase con IC 95% (Wilson)\n",
    "    print(\"\\nMétricas por clase con IC 95% (Wilson):\")\n",
    "    display(\n",
    "        class_metrics[\n",
    "            [\n",
    "                \"Stage\",\n",
    "                \"precision\",\n",
    "                \"recall\",\n",
    "                \"specificity\",\n",
    "                \"npv\",\n",
    "                \"f1\",\n",
    "                \"fpr\",\n",
    "                \"fnr\",\n",
    "                \"fdr\",\n",
    "                \"recall_ci_low\",\n",
    "                \"recall_ci_high\",\n",
    "                \"spec_ci_low\",\n",
    "                \"spec_ci_high\",\n",
    "            ]\n",
    "        ]\n",
    "        .rename(columns={\"npv\": \"npv (TN rate)\", \"fpr\": \"fpr\", \"fnr\": \"fnr\"})\n",
    "        .style.format(\n",
    "            {\n",
    "                \"precision\": \"{:.3f}\",\n",
    "                \"recall\": \"{:.3f}\",\n",
    "                \"specificity\": \"{:.3f}\",\n",
    "                \"npv (TN rate)\": \"{:.3f}\",\n",
    "                \"f1\": \"{:.3f}\",\n",
    "                \"fpr\": \"{:.3f}\",\n",
    "                \"fnr\": \"{:.3f}\",\n",
    "                \"fdr\": \"{:.3f}\",\n",
    "                \"recall_ci_low\": \"{:.3f}\",\n",
    "                \"recall_ci_high\": \"{:.3f}\",\n",
    "                \"spec_ci_low\": \"{:.3f}\",\n",
    "                \"spec_ci_high\": \"{:.3f}\",\n",
    "            }\n",
    "        )\n",
    "        .background_gradient(\n",
    "            cmap=\"YlGnBu\",\n",
    "            subset=[\"precision\", \"recall\", \"specificity\", \"npv (TN rate)\", \"f1\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Resumen macro robusto y lift sobre azar\n",
    "    print(\"\\nResumen macro robusto:\")\n",
    "    summary_labels = {\n",
    "        \"balanced_accuracy\": \"Balanced accuracy (sensibilidad macro)\",\n",
    "        \"macro_specificity\": \"Especificidad macro\",\n",
    "        \"macro_precision\": \"Precisión macro\",\n",
    "        \"macro_f1\": \"F1 macro\",\n",
    "        \"gmean_sens_spec\": \"G-mean (sensibilidad x especificidad)\",\n",
    "        \"chance_accuracy\": \"Azar (1/n clases)\",\n",
    "        \"balanced_accuracy_lift\": \"Lift sobre azar (balanced acc)\",\n",
    "    }\n",
    "    for key, label in summary_labels.items():\n",
    "        value = summary.get(key, np.nan)\n",
    "        if pd.isna(value):\n",
    "            print(f\"{label}: N/A\")\n",
    "        else:\n",
    "            print(f\"{label}: {value:.3f}\")\n",
    "\n",
    "    # Gráfico de sensibilidad por clase con IC 95%\n",
    "    colors = [STAGE_COLORS.get(stage, \"gray\") for stage in STAGE_ORDER]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    yerr = np.vstack(\n",
    "        [\n",
    "            class_metrics[\"recall\"] - class_metrics[\"recall_ci_low\"],\n",
    "            class_metrics[\"recall_ci_high\"] - class_metrics[\"recall\"],\n",
    "        ]\n",
    "    )\n",
    "    ax.bar(\n",
    "        class_metrics[\"Stage\"],\n",
    "        class_metrics[\"recall\"],\n",
    "        color=colors,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.8,\n",
    "        yerr=yerr,\n",
    "        capsize=6,\n",
    "    )\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_ylabel(\"Sensibilidad / Recall\")\n",
    "    ax.set_title(\"Recall por clase con IC 95% (Wilson)\", fontweight=\"bold\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pares de confusión más frecuentes (errores sistemáticos)\n",
    "    conf_df = most_confused_pairs(cm, labels=STAGE_ORDER, top_n=5)\n",
    "    print(\"\\nPares de confusión más frecuentes:\")\n",
    "    if not conf_df.empty:\n",
    "        display(\n",
    "            conf_df.style.format(\n",
    "                {\"Conteo\": \"{:,.0f}\", \"Proporción_sobre_clase\": \"{:.2%}\"}\n",
    "            ).background_gradient(\n",
    "                cmap=\"Reds\", subset=[\"Conteo\", \"Proporción_sobre_clase\"]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"No se registran confusiones relevantes (fuera de la diagonal).\")\n",
    "else:\n",
    "    print(\"No hay matriz de confusión disponible en las métricas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar múltiples modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar métricas de todos los modelos disponibles\n",
    "all_metrics = {}\n",
    "for model_type, metrics_path in MODEL_PATHS.items():\n",
    "    # Si es un directorio, buscar el archivo JSON dentro\n",
    "    if metrics_path.is_dir():\n",
    "        json_files = list(metrics_path.glob(\"*_results.json\"))\n",
    "        if json_files:\n",
    "            metrics_path = json_files[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if metrics_path.exists():\n",
    "        all_metrics[model_type] = load_metrics(metrics_path)\n",
    "        print(f\"✓ Cargado: {model_type}\")\n",
    "\n",
    "print(f\"\\nTotal de modelos cargados: {len(all_metrics)}\")\n",
    "\n",
    "if len(all_metrics) > 1:\n",
    "    # Comparar métricas generales\n",
    "    comparison = []\n",
    "    for name, m in all_metrics.items():\n",
    "        if \"metrics\" in m:\n",
    "            comparison.append(\n",
    "                {\n",
    "                    \"Modelo\": name,\n",
    "                    \"Accuracy\": m[\"metrics\"].get(\"accuracy\", 0),\n",
    "                    \"Kappa\": m[\"metrics\"].get(\"kappa\", 0),\n",
    "                    \"F1-macro\": m[\"metrics\"].get(\"f1_macro\", 0),\n",
    "                    \"F1-weighted\": m[\"metrics\"].get(\"f1_weighted\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if comparison:\n",
    "        df_comp = pd.DataFrame(comparison)\n",
    "        display(\n",
    "            df_comp.style.format(\n",
    "                {\n",
    "                    \"Accuracy\": \"{:.4f}\",\n",
    "                    \"Kappa\": \"{:.4f}\",\n",
    "                    \"F1-macro\": \"{:.4f}\",\n",
    "                    \"F1-weighted\": \"{:.4f}\",\n",
    "                }\n",
    "            ).background_gradient(\n",
    "                cmap=\"RdYlGn\", subset=[\"Accuracy\", \"Kappa\", \"F1-macro\", \"F1-weighted\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Gráfico comparativo\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(df_comp))\n",
    "        width = 0.2\n",
    "\n",
    "        metrics_to_plot = [\"Accuracy\", \"Kappa\", \"F1-macro\", \"F1-weighted\"]\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax.bar(x + i * width, df_comp[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "        ax.set_ylabel(\"Score\", fontsize=12)\n",
    "        ax.set_title(\"Comparación de Modelos\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xticks(x + width * 1.5)\n",
    "        ax.set_xticklabels(df_comp[\"Modelo\"], rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Necesitas al menos 2 modelos para comparar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar matriz de confusión desde imagen (para modelos de Deep Learning)\n",
    "\n",
    "# Mapeo de modelos a sus imágenes de confusion matrix\n",
    "CONFUSION_MATRIX_IMAGES = {\n",
    "    \"cnn1d\": ARTIFACTS_DIR\n",
    "    / \"cnn1d_full_20251210_201502_artifacts\"\n",
    "    / \"cnn1d_full_20251210_201502_confusion_matrix.png\",\n",
    "    \"lstm_unidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_unidir_artifacts\"\n",
    "    / \"lstm_full_20251210_193039_confusion_matrix.png\",\n",
    "    \"lstm_bidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_artifacts\"\n",
    "    / \"lstm_full_20251211_031820_confusion_matrix.png\",\n",
    "    \"lstm_bidir_attention\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_attention_artifacts\"\n",
    "    / \"lstm_full_20251211_145034_confusion_matrix.png\",\n",
    "}\n",
    "\n",
    "model_to_show = \"cnn1d\"  # Cambiar según el modelo\n",
    "\n",
    "cm_path = CONFUSION_MATRIX_IMAGES.get(model_to_show)\n",
    "\n",
    "if cm_path and cm_path.exists():\n",
    "    print(f\"Matriz de Confusión - {model_to_show.upper()}\")\n",
    "    display(Image(filename=str(cm_path), width=700))\n",
    "else:\n",
    "    print(f\"No se encontró imagen de matriz de confusión para {model_to_show}\")\n",
    "    print(f\"Path buscado: {cm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar todas las matrices de confusión disponibles\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cm_images = {\n",
    "    \"CNN1D\": ARTIFACTS_DIR\n",
    "    / \"cnn1d_full_20251210_201502_artifacts\"\n",
    "    / \"cnn1d_full_20251210_201502_confusion_matrix.png\",\n",
    "    \"LSTM Unidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_unidir_artifacts\"\n",
    "    / \"lstm_full_20251210_193039_confusion_matrix.png\",\n",
    "    \"LSTM Bidir\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_artifacts\"\n",
    "    / \"lstm_full_20251211_031820_confusion_matrix.png\",\n",
    "    \"LSTM Bidir+Attn\": ARTIFACTS_DIR\n",
    "    / \"lstm_full_bidir_attention_artifacts\"\n",
    "    / \"lstm_full_20251211_145034_confusion_matrix.png\",\n",
    "}\n",
    "\n",
    "for idx, (name, path) in enumerate(cm_images.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    if path.exists():\n",
    "        img = mpimg.imread(str(path))\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(name, fontsize=14, fontweight=\"bold\")\n",
    "        axes[idx].axis(\"off\")\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, f\"No encontrado:\\n{name}\", ha=\"center\", va=\"center\")\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Matrices de Confusión - Modelos Deep Learning\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
