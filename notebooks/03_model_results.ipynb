{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de Resultados de Modelos\n",
    "\n",
    "Este notebook permite cargar y visualizar los resultados de entrenamiento de los modelos de clasificación de estadios de sueño.\n",
    "\n",
    "Carga métricas desde archivos JSON generados por `src/models.py` y genera visualizaciones interactivas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from src.models import load_metrics\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "STAGE_COLORS = {\n",
    "    \"W\": \"#fdae61\",\n",
    "    \"N1\": \"#fee090\",\n",
    "    \"N2\": \"#abd9e9\",\n",
    "    \"N3\": \"#2c7bb6\",\n",
    "    \"REM\": \"#d7191c\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar métricas de un modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar según el modelo que quieras visualizar\n",
    "model_type = \"random_forest\"  # o \"xgboost\"\n",
    "metrics_path = MODELS_DIR / f\"{model_type}_metrics.json\"\n",
    "\n",
    "if metrics_path.exists():\n",
    "    metrics = load_metrics(metrics_path)\n",
    "    print(f\"Métricas cargadas para {model_type}\")\n",
    "    print(f\"Timestamp: {metrics.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Parámetros del modelo: {metrics.get('model_params', {})}\")\n",
    "else:\n",
    "    print(f\"No se encontró {metrics_path}\")\n",
    "    print(\"Ejecuta primero: python src/models.py --model-type random_forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas generales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"metrics\" in metrics:\n",
    "    m = metrics[\"metrics\"]\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MÉTRICAS GENERALES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:        {m.get('accuracy', 0):.4f}\")\n",
    "    print(f\"Cohen's Kappa:   {m.get('kappa', 0):.4f}\")\n",
    "    print(f\"F1-score (macro): {m.get('f1_macro', 0):.4f}\")\n",
    "    print(f\"F1-score (weighted): {m.get('f1_weighted', 0):.4f}\")\n",
    "\n",
    "    if \"cv_results\" in metrics:\n",
    "        cv = metrics[\"cv_results\"]\n",
    "        print(\"\\nCross-Validation:\")\n",
    "        print(\n",
    "            f\"  Mean F1-macro: {cv.get('mean_score', 0):.4f} ± {cv.get('std_score', 0):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-score por estadio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"metrics\" in metrics and \"f1_per_class\" in metrics[\"metrics\"]:\n",
    "    f1_per_class = metrics[\"metrics\"][\"f1_per_class\"]\n",
    "\n",
    "    # Crear DataFrame para visualización\n",
    "    f1_df = pd.DataFrame(\n",
    "        [\n",
    "            {\"Stage\": stage, \"F1-score\": f1_per_class.get(stage, 0.0)}\n",
    "            for stage in STAGE_ORDER\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Gráfico de barras\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = [STAGE_COLORS.get(stage, \"gray\") for stage in STAGE_ORDER]\n",
    "    bars = ax.bar(\n",
    "        f1_df[\"Stage\"], f1_df[\"F1-score\"], color=colors, alpha=0.7, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Agregar valores en las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"F1-score\", fontsize=12)\n",
    "    ax.set_xlabel(\"Estadio de Sueño\", fontsize=12)\n",
    "    ax.set_title(\n",
    "        f\"F1-score por Estadio - {model_type.upper()}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar tabla\n",
    "    display(\n",
    "        f1_df.style.format({\"F1-score\": \"{:.4f}\"}).background_gradient(\n",
    "            cmap=\"YlGnBu\", subset=[\"F1-score\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"confusion_matrix\" in metrics:\n",
    "    cm = np.array(metrics[\"confusion_matrix\"])\n",
    "\n",
    "    # Normalizar por filas (recall por clase)\n",
    "    cm_normalized = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Matriz absoluta\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[0].set_title(\"Matriz de Confusión (Absoluta)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Verdadero\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Predicho\", fontsize=12)\n",
    "\n",
    "    # Matriz normalizada\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=STAGE_ORDER,\n",
    "        yticklabels=STAGE_ORDER,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={\"label\": \"Proporción\"},\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[1].set_title(\n",
    "        \"Matriz de Confusión (Normalizada)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Verdadero\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Predicho\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar múltiples modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar métricas de todos los modelos disponibles\n",
    "all_metrics = {}\n",
    "for model_type in [\"random_forest\", \"xgboost\"]:\n",
    "    metrics_path = MODELS_DIR / f\"{model_type}_metrics.json\"\n",
    "    if metrics_path.exists():\n",
    "        all_metrics[model_type] = load_metrics(metrics_path)\n",
    "\n",
    "if len(all_metrics) > 1:\n",
    "    # Comparar métricas generales\n",
    "    comparison = []\n",
    "    for name, m in all_metrics.items():\n",
    "        if \"metrics\" in m:\n",
    "            comparison.append(\n",
    "                {\n",
    "                    \"Modelo\": name,\n",
    "                    \"Accuracy\": m[\"metrics\"].get(\"accuracy\", 0),\n",
    "                    \"Kappa\": m[\"metrics\"].get(\"kappa\", 0),\n",
    "                    \"F1-macro\": m[\"metrics\"].get(\"f1_macro\", 0),\n",
    "                    \"F1-weighted\": m[\"metrics\"].get(\"f1_weighted\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if comparison:\n",
    "        df_comp = pd.DataFrame(comparison)\n",
    "        display(\n",
    "            df_comp.style.format(\n",
    "                {\n",
    "                    \"Accuracy\": \"{:.4f}\",\n",
    "                    \"Kappa\": \"{:.4f}\",\n",
    "                    \"F1-macro\": \"{:.4f}\",\n",
    "                    \"F1-weighted\": \"{:.4f}\",\n",
    "                }\n",
    "            ).background_gradient(\n",
    "                cmap=\"RdYlGn\", subset=[\"Accuracy\", \"Kappa\", \"F1-macro\", \"F1-weighted\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Gráfico comparativo\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(df_comp))\n",
    "        width = 0.2\n",
    "\n",
    "        metrics_to_plot = [\"Accuracy\", \"Kappa\", \"F1-macro\", \"F1-weighted\"]\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax.bar(x + i * width, df_comp[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "        ax.set_ylabel(\"Score\", fontsize=12)\n",
    "        ax.set_title(\"Comparación de Modelos\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xticks(x + width * 1.5)\n",
    "        ax.set_xticklabels(df_comp[\"Modelo\"])\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Necesitas al menos 2 modelos entrenados para comparar\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
